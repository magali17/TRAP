---
title: 'Aim 2: Annual Averages'
author: "Magali Blanco"
date: "12/19/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, 
                      cache=T, cache.comments = F, 
                      message = F, warning = F, 
                      tidy.opts=list(width.cutoff=60), tidy=TRUE,
                      fig.height = 8, fig.width = 8
                      )  

# Clear workspace of all objects and unload all extra (non-base) packages
rm(list = ls(all = TRUE))
if (!is.null(sessionInfo()$otherPkgs)) {
  res <- suppressWarnings(
    lapply(paste('package:', names(sessionInfo()$otherPkgs), sep=""),
      detach, character.only=TRUE, unload=TRUE, force=TRUE))
}

pacman::p_load(dplyr, tidyverse, knitr, kableExtra, Hmisc, EnvStats, lubridate, glmnet, ggpubr, VCA,
               #descriptive statistics
               qwraps2, #, magrittr
               )    
#Himsc: describe(); EnvStats: summaryFull(); ggpubr: ggarrange()

options(knitr.kable.NA = '')

source("A2.0.1_Var&Fns.R")

set.seed(1)
```


```{r}
#load data
mm <- readRDS(file.path("Data", "Aim 2", "Mobile Monitoring", "mm_191112.rda"))
mm.wide <- readRDS(file.path("Data", "Aim 2", "Mobile Monitoring", "mm.wide_191112.rda"))
mm_full <- readRDS(file.path("Data", "Aim 2", "Mobile Monitoring", "mm_full_191112.rda"))
geo.mm <- readRDS(file.path("Data", "Aim 2", "Geocovariates", "geo.mm.rda")) 



#includes means & medians, all data
ufp0 <- mm.wide %>% 
  select(
    site_id: site_lat, site_no, aqs_site:season,
    ptrak_ct_cm3_median, ptrak_ct_cm3_mean
  ) %>%
  #remove NAs
  drop_na(ptrak_ct_cm3_median, ptrak_ct_cm3_mean)  %>%
  
  mutate(
    site_id = factor(site_id),  
    # New Hour Binning
      tod5 = factor(ifelse(hour %in% seq(3,8), "3_8",
                                       ifelse(hour %in% seq(9,11), "9_11", 
                                              ifelse(hour %in% seq(12,15), "12_15",
                                                     ifelse(hour %in% seq(16,20), "16_20", "21_2")))),
                         levels= c("3_8", "9_11", "12_15", "16_20", "21_2")),
  
    tod3 = factor(ifelse(hour %in% seq(0,8), "0_8",
                                       ifelse(hour %in% seq(9,17), "9_17", "18_23")),
                         levels= c("0_8", "9_15", "16_23")),
    #selected this break b/c: 1) UFP values rise during warmer daytime hours; 2) mixing ht is higher during day; 3) "daytime" = typical business as usual mm campaigns
    tod2 = factor(ifelse(hour %in% seq(9, 17), "9_17", "18_08"), 
                         levels= c("9_17", "18_08"))
    ) %>%
  #drop winter for now
  filter(season != "winter")  

#reduce data: trim bottom and top 5% observations; only keep median
ufp <- ufp0 %>%
  group_by(site_id) %>%
  #trim high and low values
  filter(ptrak_ct_cm3_median <= quantile(ptrak_ct_cm3_median, (1-trim_quantile), na.rm = T),
         ptrak_ct_cm3_median >= quantile(ptrak_ct_cm3_median, (trim_quantile), na.rm = T)) %>%
  ungroup() %>%
  select(-ptrak_ct_cm3_mean) %>%
  rename(ptrak_ct_cm3 = ptrak_ct_cm3_median)

#no. months sampled
months.sampled <- month.abb[3:11]
n.months.sampled <- length(months.sampled)

```

### -->	Combine 2 stops if they are “close”? We stopped sampling MS0398 and switched to MS0601 which is close

```{r, ?delete?}
# geo <- read.csv(file.path("Data", "Aim 2", "Geocovariates", "dr0311_mobile_locations.txt")) %>%
#   select("native_id", "latitude", "longitude", "m_to_a1", "m_to_a2", "ll_a1_s01500", "m_to_coast", "m_to_l_port", "m_to_comm", "m_to_airp", "m_to_rr", "m_to_truck", "elev_elevation", "pop10_s15000", "ndvi_q50_a07500")
# #write.csv(geo, file.path("Data", "Aim 2", "Geocovariates", "geo_few.csv"))
# 
# #"We stopped sampling MS0398 and switched to MS0601 which is close"
# #t <- geo %>% filter(native_id %in% c("MS0398", "MS0601"))

```
 
# QAQC

Instruments Used

```{r}
#instrument used each day
mm %>% 
  filter(variable %in% c("ptrak_ct_cm3", "ufp_scan_20_421_nm_ct_cm3")) %>%
  #1 record per variable combination - to reduce file size
  select(date, instrument_id, variable) %>%  
  unique() %>%
  ggplot(aes(x=date, y=instrument_id, colour = instrument_id)) + 
  geom_point(alpha=0.5) + 
  theme(legend.position = "none") + 
  labs(title = "Instruments used") + 
  facet_wrap(~variable, scales = "free_y")

mm %>%
  filter(variable == "ptrak_ct_cm3") %>%
  group_by(instrument_id) %>%
  summarize(
    unique_sampling_days = length(unique(runname)),
    unique_site_readings = n()
            ) %>% 
  kable(caption = "Instruments used", 
        col.names = c("Instrument ID", "No. Sampling Days", "No. Site Readings"))

```

Compare collocated PTRAK readings 

```{r}
#need to compare PMPT_1 to PMPT_93 and PMPT_94, and PMPT_93 to PMPT_94
#using all collocatd observations 

mm %>%
    filter(instrument_id %in% c("PMPT_94", "PMPT_4")) %>%
  select(-mean_value) %>%           
  spread(instrument_id, median_value) %>%
  drop_na("PMPT_94", "PMPT_4") %>%
  #trim
  filter(PMPT_94 >= quantile(PMPT_94, trim_quantile, na.rm = T),
         PMPT_94 <= quantile(PMPT_94, (1-trim_quantile), na.rm = T),
         PMPT_4 >= quantile(PMPT_4, trim_quantile, na.rm = T),
         PMPT_4 <= quantile(PMPT_4, (1-trim_quantile), na.rm = T)) %>%
  colo.plot(data.wide = ., 
                      x.variable = "PMPT_94", x.label = "PTRAK ID 94",
                      y.variable = "PMPT_4",  y.label = "PTRAK ID 4",
                      mytitle = paste0("Comparison of collocated PTRAK instrument readings" , 
                                        "\ntop and bottom ", (trim_quantile)*100, "% of data trimmed"),
                      r2.digits = 3)


mm %>%
    filter(instrument_id %in% c("PMPT_93", "PMPT_4")) %>%
  select(-mean_value) %>%           
  spread(instrument_id, median_value) %>%
  drop_na("PMPT_93", "PMPT_4") %>%
  #trim
  filter(PMPT_93 >= quantile(PMPT_93, trim_quantile, na.rm = T),
         PMPT_93 <= quantile(PMPT_93, (1-trim_quantile), na.rm = T),
         PMPT_4 >= quantile(PMPT_4, trim_quantile, na.rm = T),
         PMPT_4 <= quantile(PMPT_4, (1-trim_quantile), na.rm = T)) %>%
  colo.plot(data.wide = ., 
                      x.variable = "PMPT_93", x.label = "PTRAK ID 93",
                      y.variable = "PMPT_4",  y.label = "PTRAK ID 94",
                      mytitle = paste0("Comparison of collocated PTRAK instrument readings" , 
                                        "\ntop and bottom ", (trim_quantile)*100, "% of data trimmed"),
                      r2.digits = 2)

```

[? delete] Compare NanoScan instruments.

```{r} 
mm %>%  
    #select only values from desired instruments
    filter(instrument_id %in% c("PMSCAN_5", "PMSCAN_3")) %>%
    #rename(value = value_mean_median) %>%
    select(-mean_value) %>%
    spread(instrument_id, median_value) %>%
  #trim
  filter(PMSCAN_5 >= quantile(PMSCAN_5, trim_quantile, na.rm = T),
         PMSCAN_5 <= quantile(PMSCAN_5, (1-trim_quantile), na.rm = T),
         PMSCAN_3 >= quantile(PMSCAN_3, trim_quantile, na.rm = T),
         PMSCAN_3 <= quantile(PMSCAN_3, (1-trim_quantile), na.rm = T)) %>%
  colo.plot(x.variable = "PMSCAN_5", x.label = "NanoScan ID 5",
          y.variable = "PMSCAN_3", y.label = "NanoScan ID 3",
          mytitle = paste0("Comparison of collocated NanoScan instrument readings" , 
                                        "\ntop and bottom ", (trim_quantile)*100, "% of data trimmed"),
          )

```

Compare NanoScan (20-421 nm +) to ptrak (20- 1000 nm) since NanoScan tends to be more steady. Assuming few particles > 421 nm, such that the NanoScan and PTRAK are comparable.


```{r}
# use wide (takes avg of identical collocatd instruments) since not comparing specific instrument IDs, but instrument Type   
 
mm.wide %>%
  drop_na(scan_20_421_nm_ct_cm3_median, ptrak_ct_cm3_median) %>%
  #trim
  filter(scan_20_421_nm_ct_cm3_median >= quantile(scan_20_421_nm_ct_cm3_median, trim_quantile, na.rm = T),
         scan_20_421_nm_ct_cm3_median <= quantile(scan_20_421_nm_ct_cm3_median, (1-trim_quantile), na.rm = T),
         ptrak_ct_cm3_median >= quantile(ptrak_ct_cm3_median, trim_quantile, na.rm = T),
         ptrak_ct_cm3_median <= quantile(ptrak_ct_cm3_median, (1-trim_quantile), na.rm = T)) %>%
  colo.plot(x.variable = "scan_20_421_nm_ct_cm3_median", x.label = "NanoScan (pt/cm3; 20-421 nm)",
                      y.variable = "ptrak_ct_cm3_median", y.label = "PTRAK (pt/cm3; 20-1000 nm)",
                      mytitle = paste0("Comparison of collocated PTRAK and NanoScan instruments" , 
                                        "\ntop and bottom ", (trim_quantile)*100, "% of data trimmed"),
                      r2.digits = 2)


```

### --> compare nanoscan readings to PTRAK when concentrations are low (< ~ 100 pt/cm3)
### --> could look at a time series  - if runs out of alcohol, readings could be bad 

```{r}

```

### --> ? apply a between method calibration (MOVUP report "Quality Control")

```{r}

```


# Central tendency of stop data 

Distribution
```{r}

mm_full %>%
  filter(variable %in% c("ptrak_ct_cm3")) %>%
  ggplot(aes(y=value, x=site_no, group=site_no)) +
  geom_boxplot(alpha=0.3) + 
  facet_wrap(~site_visit_no, scales= "free", labeller = "label_both") + 
  labs(title = "1-second UFP readings")

```

Metrics: median vs mean.    
Using all data prior to trimming.

Will focus on medians vs means b/c:
* more robust to outliers during short-two minute stops
* data are right skewed 

#### --> ? remove Very small 1 Hz readings (1 Hz) before calculating centratl tenendcy? 

```{r}
## Mean is only ~3% higher? 
print("Average mean/median ratio")
mean(ufp0$ptrak_ct_cm3_mean)/mean(ufp0$ptrak_ct_cm3_median)


#compare mean vs median. if no different, use one 
ufp0 %>%
  gather("method", "value", ptrak_ct_cm3_median, ptrak_ct_cm3_mean) %>%
  mutate(method = as.factor(substr(method, 14, nchar(method)))) %>%
  ggplot(aes(x=value, fill=method)) + 
  geom_density(alpha=0.3) + 
  scale_x_log10() + 
  labs(
    title = "Mean vs median stop readings",
    subtitle = "log x scale"
  )

#colocation plot
colo.plot(data.wide = ufp0, 
                    x.variable = "ptrak_ct_cm3_median", x.label = "stop median",
                    y.variable = "ptrak_ct_cm3_mean", y.label = "stop mean",
                    col.by = "season", 
                    mytitle = "Comparing diferent central tendency metrics\nfor stop concentrations"
                    )

```

## Trim data    

##--> make 1 df in long format?

```{r} 
untrimmed.plot <- ufp0 %>% 
  ggplot(aes(y=ptrak_ct_cm3_median)) + 
  geom_boxplot() + #scale_y_log10() +
  labs(title = "Untrimmed data",
       y= "UFP (pt/cm3)")


trimmed.plot <- ufp %>%
  ggplot(aes(y=ptrak_ct_cm3)) + 
  geom_boxplot() +
  labs(title = paste0("trimmed top and bottom ", trim_quantile*100, "% \nof data at each site" ),
       y= "UFP (pt/cm3)")
  
ggarrange(untrimmed.plot, trimmed.plot ) 

```

# Sample size 

```{r}
## table of stop counts/site overall & stratified
stop.counts.total <- ufp %>%
  group_by(site_id) %>%
  # no. samples/site
  dplyr::summarize(N = n()) %>%
  # distribution of no. samples
  distribution.table(var.string = "N") %>%
  mutate(Time = "overall") %>%
  select(Time, everything())

stop.counts.season <- ufp %>%
  group_by(site_id, season) %>%
  # no. samples/site
  dplyr::summarize(N = n()) %>%
  #group by time period of interest
  group_by(Time=season) %>%
  distribution.table(var.string = "N")  

stop.counts.tow <- ufp %>%
  group_by(site_id, time_of_week) %>%
  # no. samples/site
  dplyr::summarize(N = n()) %>%
  #group by time period of interest
  group_by(Time=time_of_week) %>%
  distribution.table(var.string = "N")  
  
stop.counts.tod <- ufp %>%
  group_by(site_id, time_of_day) %>%
  # no. samples/site
  dplyr::summarize(N = n()) %>%
  #group by time period of interest
  group_by(Time=time_of_day) %>%
  distribution.table(var.string = "N")  
  
stop.counts <- rbind(
  stop.counts.total,
  stop.counts.season,
  stop.counts.tow,
  stop.counts.tod
    ) %>%
  add_row(Time = "season", .before = 2) %>%
  add_row(Time = "time of week", .before = 6) %>%
  add_row(Time = "time of day", .before = 9) %>%
  kable(caption = "Number of stops per site (after trimming; N = No. sites sampled)") %>%
  add_indent(c(3:5, 7:8, 10:14))
  

## plot stratified by time 
ufp %>%
  group_by(site_no, season, time_of_week, time_of_day) %>%
  summarize(N = n()) %>%
  ggplot(aes(x=site_no, y=N, fill=time_of_day)) +
  geom_bar(stat = "identity", aes(group=site_no))+
  facet_wrap(~season+time_of_week, ncol = 2) +
  labs(title = "Number of samples by site and time (after trimming)",
    y="No. Samples",
       x= "Site No.",
       fill = "time of day")

#ggsave(filename = file.path("A2_Images", "samples_tow_tod.png"), 
#      height = 8, width = 8)


# hist of unique hours locations have sampled at 
ufp %>%
  ggplot(aes(x=hour, fill= time_of_day)) +
  geom_bar(aes( )) + 
  labs(title = "Unique hours sampled (after trimming)",
       y = "No. Times Sampled",
       fill = "Time of Day"
       )

```

# Stop-level concentrations

### # --> ? add percentile of mean to plot 

```{r}
# table of concentrations distribution 
# overall
ufp.dist.overall <- ufp %>% ungroup() %>%
  distribution.table(dt = ., var.string = "ptrak_ct_cm3") %>%
  mutate(time = "overall") %>%
  select(time, everything())

#by season
season.distrib <- ufp %>% group_by(time=season) %>%
  distribution.table(dt = ., var.string = "ptrak_ct_cm3")  

#by TOW
tow.distrib <- ufp %>% group_by(time= time_of_week) %>%
  distribution.table(dt = ., var.string = "ptrak_ct_cm3")  

#by TOD
tod.distrib <- ufp %>% group_by(time= time_of_day) %>%
  distribution.table(dt = ., var.string = "ptrak_ct_cm3")  

rbind(
  ufp.dist.overall,
  season.distrib,
  tow.distrib,
  tod.distrib
) %>%
  select(-round.int) %>%
  add_row(time = "Season", .before = 2) %>%
  add_row(time = "Time of Week", .before = 6) %>%
  add_row(time = "Time of Day", .before = 9) %>%
  kable(caption = "Distribution of median site UFP concentrations") %>%
  add_indent(c(3:5, 7:8, 10:11))
  
```

```{r}
#by time
p.hour <- ufp %>% 
  ggplot(aes(x=hour, y=ptrak_ct_cm3, group=hour)) + 
  geom_boxplot() +
  labs(x = "hour") + 
  labs(y="")
   
p.day <- ufp %>%
  ggplot(aes(x=day, y=ptrak_ct_cm3)) + 
  geom_boxplot() + 
  labs(y="")

p.season <- ufp %>%
  ggplot(aes(x=season, y=ptrak_ct_cm3)) + 
  geom_boxplot() + 
  labs(y="")

ggarrange(p.hour, p.day, p.season) %>%
  annotate_figure(left = "UFP (pt/cm3)", 
                  fig.lab = "UFP over time"
                  )

# hist of concentrations distribution: stratified by: major roadway proximity, airport proximity, population density, elevation  

```

```{r, delete?, include=F, eval=F}

# #counts at each location by quarter, DOW, hour bins - see how much missin data there are 
# #df w/ times that each stop Should have sampled
# loc <- unique(ufp$site_id)
# seasons <- unique(ufp$season)
# dow <- unique(ufp$time_of_week)
# hour <- unique(ufp$time_of_day)
# 
# loc.vec <- rep(loc, each = length(seasons)*length(dow)*length(hour))
# seasons.vec <- rep(seasons, each = length(dow)*length(hour), times=length(loc))
# dow.vec <- rep(dow, each = length(hour), times=length(loc)*length(seasons))
# hour.vec <- rep(hour, times=length(loc)*length(seasons)*length(dow))
#  
# df <- data.frame(
#   site_id = loc.vec,
#   season = seasons.vec,
#   time_of_week = dow.vec,
#   time_of_day = hour.vec
# )
# 
# #add unique stop variables
# df <- ufp %>%
#   select(site_id:site_no, aqs_site) %>%
#   #drop_na() %>%
#   unique()  %>%
#   left_join(df)
# 
# #df has more rows now b/c some locations were sampled multiple times during same season-week-hour combo
# avail_data <- left_join(df, ufp) %>%
#   mutate(
#     ufp_available = as.numeric(!is.na(ptrak_ct_cm3))
#   )
# 
# samples_per_stop <- avail_data %>% 
#   group_by(route, site_id, site_no, season, time_of_week, time_of_day) %>%
#   summarize(
#     N = sum(ufp_available)
#   )
# 
# samples_per_stop %>%
#   filter(season != "winter") %>%
#   ggplot(aes(x=site_no, y=N, fill=time_of_day)) + 
#   geom_bar(stat = "identity", aes(group=site_no))+
#   facet_wrap(~season+time_of_week, ncol = 2) + 
#   labs(title = "Number of Samples by Site and Time",
#     y="No. Samples",
#        x= "Site No.", 
#        fill = "time of day")
# #ggsave(filename = file.path("A2_Images", "samples_tow_tod.png"), 
# #      height = 8, width = 8)
# 
# 
# 
# samples_per_stop_season <- samples_per_stop %>%
#     #total stops per season
#     group_by(route, site_id, season) %>%
#   summarize(
#     N = sum(N)
#   ) 
# 
# #season
# samples_per_stop_season %>% 
#   ggplot(aes(x=N, fill = (N==0))) + 
#   geom_bar() + 
#   labs(title= paste("Histograms of No. Samples per", paste0(names(samples_per_stop)[1], collapse = "-"), ""),
#        x = paste0("No. Samples from stop locations (", length(unique(avail_data$site_id)), ")")
#        ) + 
#   facet_grid(~season) 
# 
# 
# #season-week
# samples_per_stop_season_wk <- samples_per_stop %>%
#     #total stops per season
#     group_by(route, site_id, season, time_of_week) %>%
#   summarize(
#     N = sum(N)
#   ) 




############# old #############
# t2 <- samples_per_stop_season_wk %>%  filter(
#   season == "fall",  
#   N == 0
#   ) %>% 
#   arrange(time_of_week, route) 
#t2 %>%  write.csv("fall missing stops.csv", row.names = F)


# samples_per_stop_season_wk %>% 
#   ggplot(aes(x=N, fill = (N==0))) + 
#   geom_bar() + 
#   labs(title= paste("Histograms of No. Samples per", paste0(names(samples_per_stop)[1], collapse = "-"), ""),
#        x = paste0("No. Samples from stop locations (", length(unique(ufp$site_id)), ")")) + 
#   facet_grid(time_of_week~season) 
#      
   
```


# Annual averages 

## Weighted Means   
   
a) calculate weighted annual avg (Mar - Oct) means using different methods to determine how sensitve estimates are to using various approaches: 
* unweighted
* weighted (season, tow, tod) 
* regression 
  + binning tow & tod differently 

###  Compare different averages

Site-specific data averages 

```{r}
#calc diff summary measures
## using within site information
ufp <- ufp %>%
  group_by(site_no) %>%
  mutate(mean_uw = mean(ptrak_ct_cm3, na.rm=T))

 #means by seaason-wk-tod5 (5 tods)
    #there are no sites w/ sampling during all 30/40 unique times (5 tod x 2 tow x 3 or 4 seasons)
 s_tow2_tod5 <- ufp %>%
   group_by(site_no, season, time_of_week, tod5) %>%
   dplyr::summarize(mean = mean(ptrak_ct_cm3)) %>%
   ungroup() %>%
   unite("swh", season:tod5, remove = F) %>%
   group_by(site_no) %>%
   mutate(unique_times = n_distinct(swh)) 
 rm(s_tow2_tod5)

 #means by seaason-wk-tod3 (3 tods)
    #there are no sites w/ sampling during all 18/24 unique times (3 tod x 2 tow x 3/4 seasons)
 s_tow2_tod3 <- ufp %>%
   group_by(site_no, season, time_of_week, tod3) %>%
   dplyr::summarize(mean = mean(ptrak_ct_cm3)) %>%
   ungroup() %>%
   unite("swh", season:tod3, remove = F) %>%
   group_by(site_no) %>%
   mutate(unique_times = n_distinct(swh)) 
 rm(s_tow2_tod3)

 #means by seaason-wk-tod2 (2 tods)
    #8 sites w/ sampling during all 12 unique times (2 tod x 2 tow x 3 seasons)
    #none when looking at all 4 seasons
 s_tow2_tod2 <- ufp %>%
   group_by(site_no, season, time_of_week, tod2) %>%
   dplyr::summarize(mean = mean(ptrak_ct_cm3)) %>%
   ungroup() %>%
   unite("swh", season:tod2, remove = F) %>%
   #calc number of unique sampling times 
   group_by(site_no) %>%
   mutate(unique_times = n_distinct(swh)) %>%
   #only incude locations with every season, TOW, TOD
   filter(unique_times ==12)

 #tod2_df %>% select(site_no) %>% n_distinct()

 ## using tod2, caclulate weighted mean for sites w/ obs during all time combinations
 s_tow2_tod2 <- s_tow2_tod2 %>%
   mutate(
     season.wt = 1/length(unique(season)),
     wk.wt = ifelse(time_of_week == "weekday", 5/7, 2/7),
     tod.wt = ifelse(tod2 == "9_17", 9/24, 15/24),
     wt = season.wt * wk.wt * tod.wt,
     wt_times_mean = wt*mean) %>% 

 # #check that weights for all sites = 1. #looks good
 # s_tow2_tod2 %>%
 #   group_by(site_no) %>%
 #   summarize(n = sum(wt))
   
   #calculate each site's weighted mean
   group_by(site_no) %>%
   dplyr::summarize(mean_wt = sum(wt_times_mean))
   

 #means by season-wk7 (2 tods)
    #no sites w/ sampling during all 42/56 unique times (2 tod x 7 tow x 3/4 seasons)
 s_tow7_tod2 <- ufp %>%
   group_by(site_no, season, day, tod2) %>%
   dplyr::summarize(mean = mean(ptrak_ct_cm3)) %>%
   ungroup() %>%
   unite("swh", season:tod2, remove = F) %>%
   #calc number of unique sampling times 
   group_by(site_no) %>%
   mutate(unique_times = n_distinct(swh))  
   #only incude locations with every season, TOW 
 rm(s_tow7_tod2)

 #no tod
    # no site w/ 21/28 unique times (3/4 seasons x 7 days)
 s_tow7 <- ufp %>%
   group_by(site_no, season, day) %>%
   dplyr::summarize(mean = mean(ptrak_ct_cm3)) %>%
   ungroup() %>%
   unite("swh", season:day, remove = F) %>%
   #calc number of unique sampling times 
   group_by(site_no) %>%
   mutate(unique_times = n_distinct(swh))
 rm(s_tow7)

 s_tow2 <- ufp %>%
   group_by(site_no, season, time_of_week) %>%
   dplyr::summarize(mean = mean(ptrak_ct_cm3)) %>%
   ungroup() %>%
   unite("swh", season:time_of_week, remove = F) %>%
   #calc number of unique sampling times 
   group_by(site_no) %>%
   mutate(unique_times = n_distinct(swh)) %>%
   #only incude locations with every season, TOW, TOD
   filter(unique_times ==6) %>%
 #  caclulate weighted mean for sites w/ obs during all time combinations
   mutate(
     season.wt = 1/length(unique(season)),
     wk.wt = ifelse(time_of_week == "weekday", 5/7, 2/7),
     wt = season.wt * wk.wt,
     wt_times_mean = wt*mean) %>%
   #calculate each site's weighted mean
   group_by(site_no) %>%
   dplyr::summarize(mean_wt = sum(wt_times_mean))

 #season
    # 305 sites w/ sampling during all 3 seasons
 s <- ufp %>%
   group_by(site_no, season) %>%
   dplyr::summarize(mean = mean(ptrak_ct_cm3)) %>%
   #calc number of unique sampling times 
   group_by(site_no) %>%
   mutate(unique_times = n_distinct(season)) %>%
   #only incude locations with every season, TOW, TOD
   filter(unique_times ==3) %>%
 #no_tod_df %>% select(site_no) %>% n_distinct()
 #  caclulate weighted mean for sites w/ obs during all time combinations
   mutate(
     season.wt = 1/3,
     wt_times_mean = season.wt*mean) %>%
   #calculate each site's weighted mean
   group_by(site_no) %>%
   dplyr::summarize(mean_wt = sum(wt_times_mean))

 #month-weighted avg
 #locations w/ samples Mar - Nov

 m <- ufp %>%
   group_by(site_no, month) %>%
   dplyr::summarize(mean = mean(ptrak_ct_cm3)) %>%
   group_by(site_no) %>%
   filter(n_distinct(month) == n.months.sampled) %>%
 #  caclulate weighted mean for sites w/ obs during all time combinations
   mutate(
     wt_times_mean = mean * 1/n.months.sampled) %>%
   #calculate each site's weighted mean
   group_by(site_no) %>%
   dplyr::summarize(mean_wt = sum(wt_times_mean))

#105 locations w/ samples each month #nrow(m)

``` 

### --> use spline for some? 

Regression to calculate annual (9-month for now) averages.

```{r}
#df for predictions - all unique variable combinations
preds.uw <- data.frame(site_no = unique(ufp$site_no))

preds.s <- expand.grid(site_no = unique(ufp$site_no), 
                            season = unique(ufp$season))

preds.s.tow <- expand.grid(site_no = unique(ufp$site_no), 
                            season = unique(ufp$season),
                            time_of_week = unique(ufp$time_of_week))

preds.s.tow.tod2 <- expand.grid(site_no = unique(ufp$site_no), 
                            season = unique(ufp$season), 
                            time_of_week = unique(ufp$time_of_week), 
                            tod2 = unique(ufp$tod2))

preds.m.day.hr <- expand.grid(site_no = unique(ufp$site_no), 
                            month = unique(ufp$month), 
                            day = unique(ufp$day), 
                            hour = unique(ufp$hour))


#different fits
fit.uw <- lm(ptrak_ct_cm3 ~ factor(site_no) , data = ufp)
fit.s <-  update(fit.uw, ~. + season)
fit.s.tow <- update(fit.s, ~. + time_of_week)
fit.s.tow.tod2 <- update(fit.s.tow, ~. + tod2)
fit.m.day.hr <- lm(ptrak_ct_cm3 ~ factor(month, ordered = F) + factor(day, ordered = F) + factor(hour) + site_no , data = ufp)
#summary(fit.m.day.hr)

#predictions
preds.uw$yhat_uw <- predict(fit.uw, preds.uw)
preds.s$yhat <- predict(fit.s, preds.s) 
preds.s.tow$yhat <- predict(fit.s.tow, preds.s.tow) 
preds.s.tow.tod2$yhat <- predict(fit.s.tow.tod2, preds.s.tow.tod2) 
preds.m.day.hr$yhat <- predict(fit.m.day.hr, preds.m.day.hr) 

#calculate weighted avgs based on predictions

## --> CHANGE season wt later to 1/4
season.wt <- 1/3

preds.s <- preds.s %>%
  mutate(
    season_wt = season.wt,
    yhat = yhat*season_wt
  ) %>%
  group_by(site_no) %>%
  dplyr::summarize(yhat_s = sum(yhat))

preds.s.tow <- preds.s.tow %>%
  mutate(
    season_wt = season.wt,
    tow_wt = ifelse(time_of_week == "weekday", 5/7, 2/7),
    yhat = yhat*season_wt*tow_wt
  ) %>%
  group_by(site_no) %>%
  dplyr::summarize(yhat_s_tow2 = sum(yhat))
 
preds.s.tow.tod2 <- preds.s.tow.tod2 %>%
  mutate(
    season_wt = season.wt,
    tow_wt = ifelse(time_of_week == "weekday", 5/7, 2/7),
    tod_wt = ifelse(tod2 == "9_17", 9/24, (1- 9/24)),
    yhat = yhat*season_wt*tow_wt*tod_wt
  ) %>%
  group_by(site_no) %>%
  dplyr::summarize(yhat_s_tow2_tod2 = sum(yhat))
  

# --> update weights if end up sampling btwn 1-4
preds.m.day.hr <- preds.m.day.hr %>%
  mutate(
    #double the weight of the 4 hrs neighboring the 4 night hrs w/o sampling (1-4 AM)
    hr_wt = ifelse(hour %in% c(7:22), 1/24, 2/24),
    yhat = yhat * (1/n.months.sampled)*1/7*hr_wt
    ) %>%
  #check that hr weights add up to 1. looks good
  # preds.m.day.hr %>% select(hour, hr_wt) %>% unique() %>%
  #   dplyr::summarize(wt_sum = sum(hr_wt))

  group_by(site_no) %>%
  dplyr::summarize(yhat_m_day_hr = sum(yhat))

#merge all predictions to 1 df
yhats <- left_join(preds.uw, preds.s) %>%
  left_join(preds.s.tow) %>%
  left_join(preds.s.tow.tod2) %>%
  left_join(preds.m.day.hr)

```

Save annual averages from all methods with geocovariates.

```{r, include=T}
#join all estimates
annual <- ufp %>% 
  ungroup() %>%
  select(site_no, site_id, mean_uw) %>%
  #rename(mean_uw = mean_uw) %>%
  unique() %>%
  left_join(s_tow2_tod2) %>%
  rename(mean_s_tow2_tod2 = mean_wt) %>%
  left_join(s_tow2) %>%
  rename(mean_s_tow2 = mean_wt) %>%
  left_join(s) %>%
  rename(mean_s = mean_wt) %>%
  left_join(yhats) %>%
  left_join(geo.mm)

```

Plots of UFP vs method

```{r}
annual %>%
    gather(key = "method_weight", value = "ufp", c(mean_uw:yhat_m_day_hr)) %>%
  #gather(key = "method_weight", value = "ufp", -site_no, -site_id) %>%
  drop_na() %>%
  mutate(method = ifelse(grepl(method_weight , pattern = "yhat"), "regression", "site mean")) %>% 
  separate(method_weight, into=c("method", "weight"), 
                     sep = "_" , remove = F, extra = "merge") %>%
 
  #plot site samples if use diff avg methods. 
#annual.l %>%
  ggplot(aes(x=method_weight)) + 
  geom_bar() + 
  labs(title = "no. sites with estimates using various methods")


```


```{r, fig.height = 8}
#estimates for locations with all estimation methods
annual %>% 
  ## --> ? drop m_day_hr since can't compare? 
  #select(-yhat_m_day_hr) %>%
  ufp_by_method()


```

## --> issue splotting  ?? too many geocovariates
## --> ? why are m_day_hr predictions so different? see ls coefficients?
## --> change boxplots to horiz line for mean/median

```{r}
#S_TOW2
annual %>% 
  # select(-mean_s_tow2_tod2,
  #        -yhat_m_day_hr
  #        ) %>%
  ufp_by_method() #+ facet_wrap(~method, ncol = 1)
 

```

 
### Characterize sites w/ smallerst & largest inter-method differences 

- sites w/ high inter-method variabiitly have high extreme values, while those w/ low inter-method variability have either: a) no or b) both high and low extreme values

```{r}
# TOD2
tod2 <- annual %>% drop_na()

# not including yhat_m_day_hr b/c only available for regression method
tod2$min <- apply(tod2[,c(3:10)], 1, function(x)  range(x)[1])  
tod2$max <- apply(tod2[,c(3:10)], 1, function(x)  range(x)[2])  
tod2$diff <- apply(tod2[,c(3:10)], 1, function(x)  diff(range(x)))  

#tod2$m_to_a1_a2 <- apply(tod2[c("m_to_a1", "m_to_a2")], 1, min)

```

```{r}
# TOW2
tow2 <- annual %>% select(-mean_s_tow2_tod2) %>%
  drop_na()  

tow2$min <- apply(tow2[,c(3:9)], 1, function(x)  range(x)[1])  
tow2$max <- apply(tow2[,c(3:9)], 1, function(x)  range(x)[2])  
tow2$diff <- apply(tow2[,c(3:9)], 1, function(x)  diff(range(x)))  

#tow2$m_to_a1_a2 <- apply(tow2[c("m_to_a1", "m_to_a2")], 1, min)

```

```{r, include=T}
#plots looking at which sites have largest diff btwn estimates
extremes <- 0.25

#sites w/ most/least variability in estimates
tow.2.high <- tow2 %>%
  filter(diff > quantile(tow2$diff, (1-extremes)))
tow.2.low <- tow2 %>%
  filter(diff < quantile(tow2$diff, extremes))

#RAW values for high/low values - are there any extreme values?
#high range of values
h.tow2 <- ufp %>%
  filter(site_id %in% unique(tow.2.high$site_id)) %>%
  mutate(diff_lvl = "high")
l.tow2 <- ufp %>%
  filter(site_id %in% unique(tow.2.low$site_id)) %>%
  mutate(diff_lvl = "low")
h.l.tow2 <- rbind(h.tow2, l.tow2)

#density/box plots for UFP  
h.l.tow2 %>%
  ggplot(aes(x=ptrak_ct_cm3)) +
  geom_density(aes(fill = diff_lvl), alpha=0.5) +
  labs(subtitle = paste("Sea_TOW2, n =", length(unique(h.l.tow2$site_id))))

```

```{r, fig.height=8}
h.l.tow2 %>%
  ggplot(aes(x=site_no, y= ptrak_ct_cm3, fill=diff_lvl, group=site_no)) + 
  geom_boxplot() +
  #scale_x_continuous(breaks=h.l.tow2$site_no, labels=h.l.tow2$site_id) + 
  theme(axis.text.x = element_text(angle = 90)) +
  facet_wrap(~diff_lvl, scales="free_x") +
  labs(title = "Sites with 'high' and 'low' inter-method variability. \nX-asis is arranged by stop order.",
       subtitle = paste("Sea_TOW2, n =", length(unique(h.l.tow2$site_id))))

```

Density plots by geocovariates

more varibility in UFP estimates at places w/ more direct sources? (Bossche 2015)

```{r}
#tow
tow.2.high$diff_lvl <- "high"
tow.2.low$diff_lvl <- "low"
tow.2.high.low <- rbind(tow.2.high, tow.2.low) 

tow.2.high.low %>%
  gather("geocov", "value", c("ll_a1_s01500", "m_to_coast", "m_to_airp", "m_to_l_port",  "m_to_rr", "m_to_truck", "elev_elevation", "pop10_s15000", "ndvi_q50_a07500", "imp_a01000", "lu_resi_p15000")) %>%
  ggplot(aes(x = value, fill = factor(diff_lvl))) +
  geom_density(alpha=0.5) +
  facet_wrap(~geocov, scales="free") +
  labs(subtitle = paste("Sea_TOW2, n =", length(unique(tow.2.high.low$site_id))))

```

### Effect sizes: method, site, TOW, TOD

- most variation comes from site, tiny (but significant) amount from estimation method
- mean_mo and mean_sea estimates significantly lower than two2 and tow2_tod2

```{r, include=T}
#### Sea_TOW2_TOD2
 
# #ANOVA for TOD2
# tod2.anova <- tod2 %>%
#   select(site_id:mean_sea) %>%
#   gather("method", "value", -site_id, -route)  
# 
# anovaVCA(value ~ method + site_id, Data=as.data.frame(tod2.anova))
# 
# tod2.lm1a <- tod2.anova %>%                               #mean_sea_tow2_tod2
#   mutate(method = relevel(factor(method, ordered = F), ref = "mean_sea_tow2_tod2")) %>% 
#   lm(value ~ method + site_id, data = .)
# 
# anova(tod2.lm1a) %>% kable()
# summary(tod2.lm1a)
# #CI
# "confidence intervals"
# confint(tod2.lm1a)[c(1:4),]


```

#### Compare averaging method effects (at the annual avg level). 

#### --> use fixed effects anova? 
### --> use > 2 categories for TOW, TOD

```{r, eval=T}
#ANOVA for TOW2
tow2.anova <- tow2 %>%
  select(site_id:yhat_s_tow2_tod2) %>%
  gather("method", "value", -site_id) %>%
  ungroup() %>%
  mutate(
    method = as.factor(method),
    site_id = as.factor(site_id)) %>%
  as.data.frame()

anovaVCA(value ~ method + site_id, Data = tow2.anova)
 

tow2.lm1a <- tow2.anova %>%  
  mutate(method = relevel(factor(method, ordered = F), ref = "mean_s_tow2")) %>% 
  lm(value ~ method + site_id, data = .)

anova(tow2.lm1a) %>% kable()
summary(tow2.lm1a)
#CI
confint(tow2.lm1a)[c(1:7),]

```

#### Compare temporal effects (at the stop-level avg level).    
Y ~ season + tow + tod + site_id 

How do the seasonal/TOW/TOD effects impact annual mean UFP estimates relative to the method effects (~100-200 pt/cm3)

### --> use fixed effects anova for these analyses (assume indipendent features) 

Using sites w/ season-TOW2-TOD2 estimates.

```{r, include=T}
tod.sites <- tod2$site_id

tod2.stops <- ufp %>%
  filter(site_id %in% tod.sites) %>%
  as.data.frame()


anovaVCA(ptrak_ct_cm3 ~ season + time_of_week + tod2 + site_id,  Data = tod2.stops)

tod2_lm1 <- lm(ptrak_ct_cm3 ~ season + time_of_week + tod2 + site_id, data = tod2.stops)

anova(tod2_lm1) %>% kable()

#don't show site values, which we know are mostly significant & higher
summary(tod2_lm1) #$coef %>%
  # as.data.frame() %>%
  # rownames_to_column() %>%
  # filter(!grepl(rowname, pattern = "site_id"))
   
```

Using all sites. 

```{r}
#using all data 
anovaVCA(ptrak_ct_cm3 ~ season + time_of_week + tod2 + site_id,  Data = as.data.frame(ufp))

tow2_lm1 <- lm(ptrak_ct_cm3 ~ season + time_of_week + tod2 + site_id, data = ufp)

anova(tow2_lm1) %>% kable()

summary(tow2_lm1) #$coef #[c(1:6),]


```


#### Compare impacts on LUR    

Fitting LUR using different averaging methods for UFP; plotting observed averages vs model predictions.

Correlations. Using season-TOW2 estimates (n = `r nrow(tow2)`).


```{r}
#find highest correlations
### --> check 28:ncol(mm) - correct?
ufp.geo <- mm %>% 
  left_join(geo.mm) %>%
  select(site_id, 
         28:ncol(.)) %>%
  unique() 

ufp.geo <- ufp.geo %>%
  right_join(annual)

geo.cor <- ufp.geo %>%
  select_if(is.numeric) %>%
  cor(use = "complete.obs") %>%
  as.data.frame() %>%
  rownames_to_column() %>%
  filter(rowname %in% c("mean_uw", "mean_s", "mean_s_tow2", "mean_s_tow2_tod2",
                         "yhat_uw", "yhat_s", "yhat_s_tow2", "yhat_s_tow2_tod2"
                         ))

geo.cor <- geo.cor %>% column_to_rownames() 

geo.cor <- t(geo.cor) %>%
  as.data.frame() %>%
  rownames_to_column() %>%
  filter(!rowname %in% c("mean_uw", "mean_s", "mean_s_tow2", "mean_s_tow2_tod2",
                         "yhat_uw", "yhat_s", "yhat_s_tow2", "yhat_s_tow2_tod2"
                         ))

#only keep valriables w/ high corr
geo.cor %>% filter(abs(mean_uw) > 0.85) %>% 
  arrange(desc(mean_uw)) %>%
  kable(caption = "Geocovariates most correlated (R) with site annual averages")


```

Evaluate LUR predictions using different methods.

note: regression estimates are more similar than site avg methods 

## --> why are some site underpredicted by LUR? where are these? e.g. could look at distance from 1-1 line
## --> ? fit interction term & then plot to see if predictions are closer to the 1-1 line?

```{r}
#save lm predictions for dataset
tow2.pred <- tow2 %>%
  save.pred.fn(y_string = "mean_s_tow2") %>%
  save.pred.fn(y_string = "mean_s") %>%
  save.pred.fn(y_string = "mean_uw") %>% 
  save.pred.fn(y_string = "yhat_s_tow2") %>%
  save.pred.fn(y_string = "yhat_s") %>%
  save.pred.fn(y_string = "yhat_uw")  

 
#compare predictsion vs observations
LUR_mean_s_tow2_plot <- colo.plot(data.wide = tow2.pred, 
                    x.variable = "mean_s_tow2", 
                    y.variable = "LUR_mean_s_tow2")

LUR_mean_s_plot <- colo.plot(data.wide = tow2.pred, 
                    x.variable = "mean_s", 
                    y.variable = "LUR_mean_s")

LUR_mean_uw_plot <- colo.plot(data.wide = tow2.pred, 
                    x.variable = "mean_uw", 
                    y.variable = "LUR_mean_uw")

LUR_yhat_s_tow2_plot <- colo.plot(data.wide = tow2.pred, 
                    x.variable = "yhat_s_tow2", 
                    y.variable = "LUR_yhat_s_tow2")

LUR_yhat_s_plot <- colo.plot(data.wide = tow2.pred, 
                    x.variable = "yhat_s", 
                    y.variable = "LUR_yhat_s")

LUR_yhat_uw_plot <- colo.plot(data.wide = tow2.pred, 
                    x.variable = "yhat_uw", 
                    y.variable = "LUR_yhat_uw")
 
ggarrange(LUR_mean_s_tow2_plot, 
          LUR_mean_s_plot ,
          LUR_mean_uw_plot, 
          LUR_yhat_s_tow2_plot,
          LUR_yhat_s_plot,
          LUR_yhat_uw_plot,
           common.legend = T, legend = "bottom"
           #nrow = 2
          ) %>%
  annotate_figure(fig.lab = "Site UFP: Method estimates vs LUR predictions (in-sample) \nLUR fit: 9-month avg UFP ~ ll_a1_s01500 + m_to_airp  + elev_elevation + pop10_s15000")

```

#### ---> ? could subtract a constant from regression estimates if they are constantly higher

```{r}

```


Select an estimate to use
```{r}


#saveRDS(annual, file.path("Data", "Aim 2", "Mobile Monitoring", "annual.rda"))

```
