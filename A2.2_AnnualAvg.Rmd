---
title: 'Aim 2: Annual Averages'
author: "Magali Blanco"
date: "12/19/2019"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, 
                      cache=T, cache.comments = F, 
                      message = F, warning = F, 
                      tidy.opts=list(width.cutoff=60), tidy=TRUE,
                      fig.height = 8, fig.width = 8
                      )  

# Clear workspace of all objects and unload all extra (non-base) packages
rm(list = ls(all = TRUE))
if (!is.null(sessionInfo()$otherPkgs)) {
  res <- suppressWarnings(
    lapply(paste('package:', names(sessionInfo()$otherPkgs), sep=""),
      detach, character.only=TRUE, unload=TRUE, force=TRUE))
}

pacman::p_load(dplyr, tidyverse, knitr, kableExtra, Hmisc, EnvStats, lubridate, glmnet, ggpubr, VCA,
               #descriptive statistics
               qwraps2, #, magrittr
               glmnet #lasso
               )    
#Himsc: describe(); EnvStats: summaryFull(); ggpubr: ggarrange()

options(knitr.kable.NA = '')
set.seed(1)
source("A2.0.1_Var&Fns.R")

#load data
mm <- readRDS(file.path("Data", "Aim 2", "Mobile Monitoring", "mm_2019-12-22.rda"))
mm.wide <- readRDS(file.path("Data", "Aim 2", "Mobile Monitoring", "mm.w_2019-12-22.rda"))
geo.mm <- readRDS(file.path("Data", "Aim 2", "Geocovariates", "geo.mm.rda")) %>%
  select(-contains("lambert"), -latitude, -longitude)

ufp0 <- mm.wide %>% 
  select(date:site_no,
         ptrak_pt_cm3_median,
         ptrak_pt_cm3_mean
         ) %>%
  mutate(
    site_id = factor(site_id),  
    # New Hour Binning
      tod5 = factor(ifelse(hour %in% seq(3,8), "3_8",
                                       ifelse(hour %in% seq(9,11), "9_11", 
                                              ifelse(hour %in% seq(12,15), "12_15",
                                                     ifelse(hour %in% seq(16,20), "16_20", "21_2")))),
                         levels= c("3_8", "9_11", "12_15", "16_20", "21_2")),
  
    tod3 = factor(ifelse(hour %in% seq(0,8), "0_8",
                                       ifelse(hour %in% seq(9,17), "9_17", "18_23")),
                         levels= c("0_8", "9_15", "16_23")),
    #selected this break b/c: 1) UFP values rise during warmer daytime hours; 2) mixing ht is higher during day; 3) "daytime" = typical business as usual mm campaigns
    tod2 = factor(ifelse(hour %in% seq(9, 17), "9_17", "18_08"), 
                         levels= c("9_17", "18_08"))
    ) %>%
  #drop winter for now
  filter(season != "winter")  

#dataset for main analyses w/ median UFP stop concentrations &  geocovariates
ufp0 <- ufp0 %>% select(-ptrak_pt_cm3_mean) %>%
  rename(ptrak = ptrak_pt_cm3_median) %>%
  #remove NAs
  drop_na(ptrak) %>%
  left_join(geo.mm)

#trim data 
ufp <- ufp0 %>%
  group_by(site_id) %>%
  filter(ptrak <= quantile(ptrak, (1-trim_quantile), na.rm = T),
         ptrak >= quantile(ptrak, (trim_quantile), na.rm = T)) %>%
  ungroup()  

#no. months sampled
months.sampled <- month.abb[3:12]
n.months.sampled <- length(months.sampled)

n.seasons.sampled <- length(unique(ufp$season))

```

```{r} 
untrimmed.plot <- ufp0 %>% 
  ggplot(aes(y=ptrak)) + 
  geom_boxplot() + #scale_y_log10() +
  labs(title = "Untrimmed data",
       y= "UFP (pt/cm3)")

trimmed.plot <- ufp %>%
  ggplot(aes(y=ptrak)) + 
  geom_boxplot() +
  labs(title = paste0("Trimmed top and bottom ", trim_quantile*100, "% \nof each site's stops" ),
       y= "UFP (pt/cm3)")
  
ggarrange(untrimmed.plot, trimmed.plot ) 

```

# Sample size 

```{r}
## table of stop counts/site overall & stratified
stop.counts.total <- ufp %>%
  dplyr::group_by(site_id) %>%
  # no. samples/site
  dplyr::summarize(N = n()) %>%
  # distribution of no. samples
  distribution.table(var.string = "N") %>%
  mutate(Time = "Overall") %>%
  select(Time, everything())

stop.counts.season <- ufp %>%
  dplyr::group_by(site_id, season) %>%
  # no. samples/site
  dplyr::summarize(N = n()) %>%
  #group by time period of interest
  group_by(Time=season) %>%
  distribution.table(var.string = "N")  

stop.counts.tow <- ufp %>%
  group_by(site_id, time_of_week) %>%
  # no. samples/site
  dplyr::summarize(N = n()) %>%
  #group by time period of interest
  group_by(Time=time_of_week) %>%
  distribution.table(var.string = "N")  
  
stop.counts.tod <- ufp %>%
  group_by(site_id, time_of_day) %>%
  # no. samples/site
  dplyr::summarize(N = n()) %>%
  #group by time period of interest
  group_by(Time=time_of_day) %>%
  distribution.table(var.string = "N")  
  
rbind(
  stop.counts.total,
  stop.counts.season,
  stop.counts.tow,
  stop.counts.tod
    ) %>%
  add_row(Time = "season", .before = 2) %>%
  add_row(Time = "time of week", .before = 6) %>%
  add_row(Time = "time of day", .before = 9) %>%
  kable(caption = "Number of stop samples per site (after trimming; N = No. sites sampled)", 
        col.names = c("Time", "N", "mean (SD)", "median (IQR)", "Min", "Max")
        ) %>%
  add_indent(c(3:5, 7:8, 10:14)) %>%
  kable_styling()

## plot stratified by time 
ufp %>%
  group_by(site_no, season, time_of_week, time_of_day) %>%
  dplyr::summarize(N = n()) %>%
  ggplot(aes(x=site_no, y=N, fill=time_of_day)) +
  geom_bar(stat = "identity", aes(group=site_no))+
  facet_wrap(~season+time_of_week, ncol = 2) +
  labs(title = "Number of samples by site and time (after trimming)",
    y="No. Samples",
       x= "Site No.",
       fill = "time of day")

#ggsave(filename = file.path("A2_Images", "samples_tow_tod.png"), 
#      height = 8, width = 8)


# hist of unique hours locations have sampled at 
ufp %>%
  ggplot(aes(x=hour, fill= time_of_day)) +
  geom_bar(aes( )) + 
  labs(title = "Unique hours sampled (after trimming)",
       y = "No. Times Sampled",
       fill = "Time of Day"
       )

```

# Concentrations at different times

Table

```{r}
# table of concentrations distribution 
# overall
ufp.dist.overall <- ufp %>% ungroup() %>%
  distribution.table(dt = ., var.string = "ptrak") %>%
  mutate(time = "Overall") %>%
  select(time, everything())

#by season
season.distrib <- ufp %>% group_by(time=season) %>%
  distribution.table(dt = ., var.string = "ptrak")  

#by TOW
tow.distrib <- ufp %>% group_by(time= time_of_week) %>%
  distribution.table(dt = ., var.string = "ptrak")  

#by TOD
tod.distrib <- ufp %>% group_by(time= time_of_day) %>%
  distribution.table(dt = ., var.string = "ptrak")  

rbind(
  ufp.dist.overall,
  season.distrib,
  tow.distrib,
  tod.distrib
) %>%
  add_row(time = "Season", .before = 2) %>%
  add_row(time = "Time of Week", .before = 6) %>%
  add_row(time = "Time of Day", .before = 9) %>%
  kable(caption = "Distribution of median stop concentrations over time") %>%
  add_indent(c(3:5, 7:8, 10:14)) %>%
  kable_styling()
  
```

Plots

```{r}
#by time
p.hour <- ufp %>% 
  ggplot(aes(x=hour, y=ptrak, group=hour)) + 
  geom_boxplot() +
  labs(x = "hour") + 
  labs(y="")
   
p.day <- ufp %>%
  ggplot(aes(x=day, y=ptrak)) + 
  geom_boxplot() + 
  labs(y="")

p.season <- ufp %>%
  ggplot(aes(x=season, y=ptrak)) + 
  geom_boxplot() + 
  labs(y="")

ggarrange(p.hour, p.day, p.season) %>%
  annotate_figure(left = "UFP Concentration (pt/cm3)", 
                  fig.lab = "UFP concentrations observed at stops"
                  )

# hist of concentrations distribution: stratified by: major roadway proximity, airport proximity, population density, elevation  

```

# Concentrations by correlated geocovariates

```{r}
select.cov <- 10
geo.cor <- ufp %>%
  #select_if(is.numeric) %>%
  select(ptrak,
         m_to_a1:tl_s15000) %>%
  cor(use = "complete.obs") %>%
  as.data.frame() %>%
  select(ptrak) %>%
  rownames_to_column() %>%
  arrange(desc(ptrak)) %>%
  filter(row_number() %in% c(1:select.cov, (n()-select.cov):n()),
         rowname != "ptrak"
         )  %>%
  rename(geocovariate = rowname) 
 
geo.cor %>% kable() %>%
   kable_styling()

cov.list <- c("lu_transport_p05000", 
         "ll_a1_s05000",
         "rlu_dev_hi_p05000", "rlu_evergreen_p03000",
         "imp_a05000",
         "intersect_a1_a3_s03000",
         "m_to_l_airp",
         "ndvi_summer_a01000")

#plot UFPs based on strongest variable correlations

ufp %>%
  select(ptrak, 
         cov.list) %>%
  gather("geo", "value", -ptrak) %>%
  ggplot(aes(x=value, y=ptrak)) + 
  geom_point(alpha=0.1) + 
  geom_smooth() +
  facet_wrap(~geo, scales = "free_x")
  
```

# Annual averages 

## Weighted Means   
   
a) calculate weighted annual avg (Mar - Oct) means using different methods to determine how sensitve estimates are to using various approaches: 
* unweighted
* weighted (season, tow, tod) 
* regression 
  + binning tow & tod differently 

###  Compare different averages

Site-specific data averages 

```{r}
#calc diff summary measures
## using within site information
ufp <- ufp %>%
  group_by(site_no) %>%
  mutate(mean_uw = mean(ptrak, na.rm=T))

 #means by seaason-wk-tod5 (5 tods)
min.t <- 5*2*n.seasons.sampled
s_tow2_tod5 <- ufp %>%
   group_by(site_no, season, time_of_week, tod5) %>%
   dplyr::summarize(mean = mean(ptrak)) %>%
   ungroup() %>%
   unite("swh", season:tod5, remove = F) %>%
   group_by(site_no) %>%
   mutate(unique_times = n_distinct(swh)) #%>% filter(unique_times >= min.t)
  #there are no sites w/ sampling during all 30/40 unique times (5 tod x 2 tow x 3 or 4 seasons)
 if(!max(s_tow2_tod5$unique_times) >= min.t){ rm(s_tow2_tod5) }
 
 #means by seaason-wk-tod3 (3 tods)
min.t <- 3*2*n.seasons.sampled
s_tow2_tod3 <- ufp %>%
   group_by(site_no, season, time_of_week, tod3) %>%
   dplyr::summarize(mean = mean(ptrak)) %>%
   ungroup() %>%
   unite("swh", season:tod3, remove = F) %>%
   group_by(site_no) %>%
   mutate(unique_times = n_distinct(swh)) #%>% filter(unique_times >=min.t)
 #there are no sites w/ sampling during all 18/24 unique times (3 tod x 2 tow x 3/4 seasons)
if(!max(s_tow2_tod3$unique_times) >= min.t){rm(s_tow2_tod3)}
 
 #means by seaason-wk-tod2 (2 tods)
    #8 sites w/ sampling during all 12 unique times (2 tod x 2 tow x 3 seasons)
    #none when looking at all 4 seasons
 min.t <- 2*2*n.seasons.sampled
 s_tow2_tod2 <- ufp %>%
   group_by(site_no, season, time_of_week, tod2) %>%
   dplyr::summarize(mean = mean(ptrak)) %>%
   ungroup() %>%
   unite("swh", season:tod2, remove = F) %>%
   group_by(site_no) %>%
   mutate(unique_times = n_distinct(swh)) %>% 
   filter(unique_times >= min.t)
if(!max(s_tow2_tod2$unique_times) >= min.t){rm(s_tow2_tod2)}

 ## using tod2, caclulate weighted mean for sites w/ obs during all time combinations
 s_tow2_tod2 <- s_tow2_tod2 %>%
   mutate(
     season.wt = 1/n.seasons.sampled,
     wk.wt = ifelse(time_of_week == "weekday", 5/7, 2/7),
     tod.wt = ifelse(tod2 == "9_17", 9/24, 15/24),
     wt = season.wt * wk.wt * tod.wt,
     wt_times_mean = wt*mean) %>% 

 # #check that weights for all sites = 1. #looks good
 # s_tow2_tod2 %>%
 #   group_by(site_no) %>%
 #   summarize(n = sum(wt))
   
   #calculate each site's weighted mean
   group_by(site_no) %>%
   dplyr::summarize(mean_wt = sum(wt_times_mean))
   
 #means by season-wk7 (2 tods)
    #no sites w/ sampling during all 42/56 unique times (2 tod x 7 tow x 3/4 seasons)
min.t <- 2*7*n.seasons.sampled
s_tow7_tod2 <- ufp %>%
   group_by(site_no, season, day, tod2) %>%
   dplyr::summarize(mean = mean(ptrak)) %>%
   ungroup() %>%
   unite("swh", season:tod2, remove = F) %>%
   group_by(site_no) %>%
   mutate(unique_times = n_distinct(swh)) #%>% filter(unique_times >=min.t)
if(!max(s_tow7_tod2$unique_times) >= min.t){rm(s_tow7_tod2)} 

 #no tod
    # no site w/ 21/28 unique times (3/4 seasons x 7 days)
min.t <- 7*n.seasons.sampled
s_tow7 <- ufp %>%
   group_by(site_no, season, day) %>%
   dplyr::summarize(mean = mean(ptrak)) %>%
   ungroup() %>%
   unite("swh", season:day, remove = F) %>%
   group_by(site_no) %>%
   mutate(unique_times = n_distinct(swh)) #%>% filter(unique_times >=min.t)
if(!max(s_tow7$unique_times) >= min.t){rm(s_tow7)} 

min.t <- 2*n.seasons.sampled
s_tow2 <- ufp %>%
   group_by(site_no, season, time_of_week) %>%
   dplyr::summarize(mean = mean(ptrak)) %>%
   ungroup() %>%
   unite("swh", season:time_of_week, remove = F) %>%
   group_by(site_no) %>%
   mutate(unique_times = n_distinct(swh)) %>%
   filter(unique_times >=min.t) %>%
 #  caclulate weighted mean for sites w/ obs during all time combinations
   mutate(
     season.wt = 1/n.seasons.sampled,
     wk.wt = ifelse(time_of_week == "weekday", 5/7, 2/7),
     wt = season.wt * wk.wt,
     wt_times_mean = wt*mean) %>%
   #calculate each site's weighted mean
   group_by(site_no) %>%
   dplyr::summarize(mean_wt = sum(wt_times_mean))

 #season
    # 305 sites w/ sampling during all 3 seasons
 
min.t <- n.seasons.sampled
s <- ufp %>%
   group_by(site_no, season) %>%
   dplyr::summarize(mean = mean(ptrak)) %>%
   group_by(site_no) %>%
   mutate(unique_times = n_distinct(season)) %>%
   filter(unique_times == min.t) %>%
 #no_tod_df %>% select(site_no) %>% n_distinct()
 #  caclulate weighted mean for sites w/ obs during all time combinations
   mutate(
     season.wt = 1/n.seasons.sampled,
     wt_times_mean = season.wt*mean) %>%
   #calculate each site's weighted mean
   group_by(site_no) %>%
   dplyr::summarize(mean_wt = sum(wt_times_mean))

 #month-weighted avg
 #locations w/ samples Mar - Nov

 m <- ufp %>%
   group_by(site_no, month) %>%
   dplyr::summarize(mean = mean(ptrak)) %>%
   group_by(site_no) %>%
   filter(n_distinct(month) == n.months.sampled) %>%
 #  caclulate weighted mean for sites w/ obs during all time combinations
   mutate(
     wt_times_mean = mean * 1/n.months.sampled) %>%
   #calculate each site's weighted mean
   group_by(site_no) %>%
   dplyr::summarize(mean_wt = sum(wt_times_mean))

#105 locations w/ samples each month #nrow(m)

``` 

Regression to calculate annual (for now, 3 season) averages.

```{r}
#df for predictions - all unique variable combinations
preds.uw <- data.frame(site_no = unique(ufp$site_no))

preds.s <- expand.grid(site_no = unique(ufp$site_no), 
                            season = unique(ufp$season))

preds.s.tow <- expand.grid(site_no = unique(ufp$site_no), 
                            season = unique(ufp$season),
                            time_of_week = unique(ufp$time_of_week))

preds.s.tow.tod2 <- expand.grid(site_no = unique(ufp$site_no), 
                            season = unique(ufp$season), 
                            time_of_week = unique(ufp$time_of_week), 
                            tod2 = unique(ufp$tod2))

preds.m.day.hr <- expand.grid(site_no = unique(ufp$site_no), 
                            month = unique(ufp$month), 
                            day = unique(ufp$day), 
                            hour = unique(ufp$hour))


#different fits
fit.uw <- lm(ptrak ~ factor(site_no) , data = ufp)
fit.s <-  update(fit.uw, ~. + season)
fit.s.tow <- update(fit.s, ~. + time_of_week)
fit.s.tow.tod2 <- update(fit.s.tow, ~. + tod2)
fit.m.day.hr <- lm(ptrak ~ factor(month, ordered = F) + factor(day, ordered = F) + factor(hour) + site_no , data = ufp)
#summary(fit.m.day.hr)

#predictions
preds.uw$yhat_uw <- predict(fit.uw, preds.uw)
preds.s$yhat <- predict(fit.s, preds.s) 
preds.s.tow$yhat <- predict(fit.s.tow, preds.s.tow) 
preds.s.tow.tod2$yhat <- predict(fit.s.tow.tod2, preds.s.tow.tod2) 
preds.m.day.hr$yhat <- predict(fit.m.day.hr, preds.m.day.hr) 

#calculate weighted avgs based on predictions

season.wt <- 1/n.seasons.sampled

preds.s <- preds.s %>%
  mutate(
    season_wt = season.wt,
    yhat = yhat*season_wt
  ) %>%
  group_by(site_no) %>%
  dplyr::summarize(yhat_s = sum(yhat))

preds.s.tow <- preds.s.tow %>%
  mutate(
    season_wt = season.wt,
    tow_wt = ifelse(time_of_week == "weekday", 5/7, 2/7),
    yhat = yhat*season_wt*tow_wt
  ) %>%
  group_by(site_no) %>%
  dplyr::summarize(yhat_s_tow2 = sum(yhat))
 
preds.s.tow.tod2 <- preds.s.tow.tod2 %>%
  mutate(
    season_wt = season.wt,
    tow_wt = ifelse(time_of_week == "weekday", 5/7, 2/7),
    tod_wt = ifelse(tod2 == "9_17", 9/24, (1- 9/24)),
    yhat = yhat*season_wt*tow_wt*tod_wt
  ) %>%
  group_by(site_no) %>%
  dplyr::summarize(yhat_s_tow2_tod2 = sum(yhat))
  

#calculate weights based on samling hours available 
#sort(unique(ufp$hour))
hrs.sampled <- sort(unique(ufp$hour))
hrs.wt1 <- c(6:22)
non.wt1.hrs <- (0:23)[!(0:23) %in% hrs.wt1]
#other hrs sampled w/o a wt=1
other.hrs.sampled <- hrs.sampled[!hrs.sampled %in% hrs.wt1]
other.hrs.wt <- length(non.wt1.hrs)/length(other.hrs.sampled)
 
#preds.m.day.hr2 <-preds.m.day.hr

preds.m.day.hr <- preds.m.day.hr %>%
  mutate(
    #double the weight of the 4 hrs neighboring the 4 night hrs w/o sampling (1-4 AM)
    hr_wt = ifelse(hour %in% hrs.wt1, 1/24, other.hrs.wt/24),
    month_wt = 1/n.months.sampled,
    day_wt = 1/7,
    yhat = yhat*hr_wt*month_wt*day_wt
    ) %>%
  #check that hr weights add up to 1. looks good
  # t <-preds.m.day.hr %>% group_by(site_no) %>%
  #   dplyr::summarize(sum_wt = sum(hr_wt*month_wt*day_wt)) 

  group_by(site_no) %>%
  dplyr::summarize(yhat_m_day_hr = sum(yhat))

#merge all predictions to 1 df
yhats <- left_join(preds.uw, preds.s) %>%
  left_join(preds.s.tow) %>%
  left_join(preds.s.tow.tod2) %>%
  left_join(preds.m.day.hr)


```

Save annual averages from all methods with geocovariates.

```{r, include=T}
#join all estimates
annual <- ufp %>% 
  ungroup() %>%
  select(site_no, site_id, 
         m_to_a1:tl_s15000,
         mean_uw,
         ) %>%
  unique() %>%
  left_join(s_tow2_tod2) %>%
  rename(mean_s_tow2_tod2 = mean_wt) %>%
  left_join(s_tow2) %>%
  rename(mean_s_tow2 = mean_wt) %>%
  left_join(s) %>%
  rename(mean_s = mean_wt) %>%
  left_join(yhats) %>%
  select(site_no, site_id, 
         mean_uw:yhat_m_day_hr, everything())

annual.l <- annual %>%
  gather(key = "method_weight", value = "ufp", c(mean_uw:yhat_m_day_hr)) %>%
  drop_na() %>%
  separate(method_weight, into=c("method", "weight"), 
                     sep = "_" , remove = F, extra = "merge")  

```

Disribution of annual UFP predictions.

```{r}
#table 
annual.l %>%
  group_by(method, weight) %>%
  distribution.table(var.string = "ufp") %>%
  kable(caption = "Annual average site estimates using various methods and weights") %>%
  kable_styling()

#histograms
annual.l %>%
  ggplot(aes(x=ufp)) + 
  geom_histogram() + 
  facet_grid(rows = vars(method), cols = vars(weight)) + 
  labs(title = "Annual average site estimates using various methods and weights",
       x = "Annual UFP concentration estimate (pt/cm3)"
       )

```

```{r, fig.height = 8}
#TOD2
annual.l.tod2 <- annual.l %>%
  filter(site_no %in% s_tow2_tod2$site_no,
         !weight %in% c("m_day_hr")
         ) 
## ranges
annual.l.tod2.rage <- annual.l.tod2 %>%
  group_by(site_no, method) %>%
  dplyr::summarize(
    min = min(ufp),
    max = max(ufp),
    mean = mean(ufp))

ufp_by_method (dt = annual.l.tod2, 
               dt.range = annual.l.tod2.rage)

#TOW2
annual.l.tow2 <- annual.l %>%
  filter(site_no %in% s_tow2$site_no,
         !weight %in% c("m_day_hr",
                        "s_tow2_tod2"
                        )) %>%
  mutate(group = cut(site_no, breaks = 12, labels = F))

## ranges
annual.l.tow2.rage <- annual.l.tow2 %>%
  group_by(group, site_no, method) %>%
  dplyr::summarize(
    min = min(ufp),
    max = max(ufp),
    mean = mean(ufp))

ufp_by_method (dt = annual.l.tow2, 
               dt.range = annual.l.tow2.rage) + 
  facet_wrap(~group, scales = "free")

```
 
### Characterize sites w/ smallest & largest inter-method differences 

- sites w/ high inter-method variabiitly have high extreme values, while those w/ low inter-method variability have either: a) no or b) both high and low extreme values

## --> update column names of fn 

```{r}
# TOD2
tod2 <- annual %>% drop_na()

# not including yhat_m_day_hr b/c only available for regression method
tod2$min <- apply(tod2[,c(3:10)], 1, function(x)  range(x)[1])  
tod2$max <- apply(tod2[,c(3:10)], 1, function(x)  range(x)[2])  
tod2$diff <- tod2$max - tod2$min #apply(tod2[,c(3:10)], 1, function(x)  diff(range(x)))  

```

```{r}
# TOW2
tow2 <- annual %>% select(-mean_s_tow2_tod2) %>%
  drop_na()  

tow2$min <- apply(tow2[,c(3:8)], 1, function(x)  range(x)[1])  
tow2$max <- apply(tow2[,c(3:8)], 1, function(x)  range(x)[2])  
tow2$diff <- tow2$max - tow2$min  #apply(tow2[,c(3:8)], 1, function(x)  diff(range(x)))  

```

```{r, include=T}
#plots looking at which sites have largest diff btwn estimates
extremes <- 0.25

#sites w/ most/least variability in estimates
tow.2.high <- tow2 %>%
  filter(diff > quantile(tow2$diff, (1-extremes)))
tow.2.low <- tow2 %>%
  filter(diff < quantile(tow2$diff, extremes))

#RAW values for high/low values - are there any extreme values?
#high range of values
h.tow2 <- ufp %>%
  filter(site_id %in% unique(tow.2.high$site_id)) %>%
  mutate(diff_lvl = "high")
l.tow2 <- ufp %>%
  filter(site_id %in% unique(tow.2.low$site_id)) %>%
  mutate(diff_lvl = "low")
h.l.tow2 <- rbind(h.tow2, l.tow2)

#density/box plots for UFP  
h.l.tow2 %>%
  ggplot(aes(x=ptrak)) +
  geom_density(aes(fill = diff_lvl), alpha=0.5) +
  labs(subtitle = paste("Sea_TOW2, n =", length(unique(h.l.tow2$site_id))))

```

```{r, fig.height=8}
h.l.tow2 %>%
  ggplot(aes(x=site_no, y= ptrak, fill=diff_lvl, group=site_no)) + 
  geom_boxplot() +
  #scale_x_continuous(breaks=h.l.tow2$site_no, labels=h.l.tow2$site_id) + 
  theme(axis.text.x = element_text(angle = 90)) +
  facet_wrap(~diff_lvl, scales="free_x") +
  labs(title = "Sites with 'high' and 'low' inter-method variability. \nX-asis is arranged by stop order.",
       subtitle = paste("Sea_TOW2, n =", length(unique(h.l.tow2$site_id))))

```

Density plots by geocovariates

more varibility in UFP estimates at places w/ more direct sources? (Bossche 2015)

```{r}
#tow
tow.2.high$diff_lvl <- "high"
tow.2.low$diff_lvl <- "low"
tow.2.high.low <- rbind(tow.2.high, tow.2.low) 

cov.list2 <- c("ll_a1_s01500", "m_to_coast", "m_to_airp", "m_to_l_port",  "m_to_rr", "m_to_truck", "elev_elevation", "pop10_s15000", "ndvi_q50_a07500", "imp_a01000", "lu_resi_p15000")

tow.2.high.low %>%
  gather("geocov", "value", c(cov.list2)) %>%
  ggplot(aes(x = value, fill = factor(diff_lvl))) +
  geom_density(alpha=0.5) +
  facet_wrap(~geocov, scales="free") +
  labs(subtitle = paste("Sea_TOW2, n =", length(unique(tow.2.high.low$site_id))))

```

### Effect sizes: method, site, TOW, TOD

- most variation comes from site, tiny (but significant) amount from estimation method
- mean_mo and mean_sea estimates significantly lower than two2 and tow2_tod2

```{r, include=T}
#### Sea_TOW2_TOD2
 
# #ANOVA for TOD2
# tod2.anova <- tod2 %>%
#   select(site_id:mean_sea) %>%
#   gather("method", "value", -site_id, -route)  
# 
# anovaVCA(value ~ method + site_id, Data=as.data.frame(tod2.anova))
#  
# tod2.lm1a <- tod2.anova %>%                               #mean_sea_tow2_tod2
#   mutate(method = relevel(factor(method, ordered = F), ref = "mean_sea_tow2_tod2")) %>% 
#   lm(value ~ method + site_id, data = .) 
# 
# anova(tod2.lm1a) %>% kable()
# summary(tod2.lm1a)
# #CI
# "confidence intervals"
# confint(tod2.lm1a)[c(1:4),]


```

#### Compare averaging method effects (at the annual avg level). 

```{r, eval=T}
#ANOVA for TOW2
tow2.anova <- tow2 %>%
  select(site_id:yhat_s_tow2_tod2) %>%
  gather("method", "value", -site_id) %>%
  ungroup() %>%
  mutate(
    method = as.factor(method),
    site_id = as.factor(site_id)) %>%
  as.data.frame()

anovaVCA(value ~ method + site_id, Data = tow2.anova)
 

tow2.lm1a <- tow2.anova %>%  
  mutate(method = relevel(factor(method, ordered = F), ref = "mean_s_tow2")) %>% 
  lm(value ~ method + site_id, data = .)

anova(tow2.lm1a) #%>% kable()
summary(tow2.lm1a)
#CI
#confint(tow2.lm1a)[c(1:7),]

```

#### Compare temporal effects (at the stop-level avg level).    
Y ~ season + tow + tod + site_id 

How do the seasonal/TOW/TOD effects impact annual mean UFP estimates relative to the method effects (~100-200 pt/cm3)

Using sites w/ season-TOW2-TOD2 estimates.

```{r, include=T}
tod.sites <- tod2$site_id

tod2.stops <- ufp %>%
  filter(site_id %in% tod.sites) %>%
  as.data.frame()

anovaVCA(ptrak~ season + time_of_week + tod2 + site_id,  Data = tod2.stops)
tod2_lm1 <- lm(ptrak ~ season + time_of_week + tod2 + site_id, data = tod2.stops)
anova(tod2_lm1) #%>% kable()
summary(tod2_lm1) 
  
anovaVCA(ptrak ~ season + day + hour + site_id,  Data = tod2.stops)
tod2_lm2 <- lm(ptrak ~ season + factor(day, ordered = F) + factor(hour) + site_id, data = tod2.stops)
anova(tod2_lm2) #%>% kable()

 
```

Using all sites. 

```{r}
#using all data 
anovaVCA(ptrak ~ season + time_of_week + tod2 + site_id,  Data = as.data.frame(ufp))
tow2_lm1 <- lm(ptrak ~ season + time_of_week + tod2 + site_id, data = ufp)
anova(tow2_lm1) %>% kable()
summary(tow2_lm1) #$coef #[c(1:6),]

anovaVCA(ptrak ~ season + day + hour + site_id,  Data = as.data.frame(ufp))
tow2_lm2 <- lm(ptrak ~ season + factor(day, ordered = F) + factor(hour) + site_id, data = ufp)
anova(tow2_lm2) #%>% kable()
summary(tow2_lm2) #$coef #[c(1:6),]

```

#### Compare impacts on LUR    

Fitting LUR using different averaging methods for UFP; plotting observed averages vs model predictions.

Correlations. Using season-TOW2 estimates (n = `r nrow(tow2)`).


```{r}
# #find highest correlations 
# ### --> check 28:ncol(mm) - correct?
# ufp.geo <- mm %>% 
#   left_join(geo.mm) %>%
#   select(site_id, 
#          28:ncol(.)) %>%
#   unique() 
# 
# ufp.geo <- ufp.geo %>%
#   right_join(annual)
# 
# geo.cor <- ufp.geo %>%
#   select_if(is.numeric) %>%
#   cor(use = "complete.obs") %>%
#   as.data.frame() %>%
#   rownames_to_column() %>%
#   filter(rowname %in% c("mean_uw", "mean_s", "mean_s_tow2", "mean_s_tow2_tod2",
#                          "yhat_uw", "yhat_s", "yhat_s_tow2", "yhat_s_tow2_tod2"
#                          ))
# 
# geo.cor <- geo.cor %>% column_to_rownames() 
# 
# geo.cor <- t(geo.cor) %>%
#   as.data.frame() %>%
#   rownames_to_column() %>%
#   filter(!rowname %in% c("mean_uw", "mean_s", "mean_s_tow2", "mean_s_tow2_tod2",
#                          "yhat_uw", "yhat_s", "yhat_s_tow2", "yhat_s_tow2_tod2"
#                          ))
# 
# #only keep valriables w/ high corr
# geo.cor %>% filter(abs(mean_uw) > 0.85) %>% 
#   arrange(desc(mean_uw)) %>%
#   kable(caption = "Geocovariates most correlated (R) with site annual averages")


```

Evaluate LUR predictions using different methods.

note: regression estimates are more similar than site avg methods 

```{r}
#-sites w/ TOW2

#save lm predictions for dataset
tow2 <- tow2 %>%
  #?? fit separate models for high vs low values of ll_a1_s01500 / rlu_evergreen_p01000
  # rename(var = ll_a1_s01500) %>%
  # filter(var < quantile(var, 0.6)) %>%

  save.pred.fn(y_string = "mean_s_tow2") %>%
  save.pred.fn(y_string = "mean_s") %>%
  save.pred.fn(y_string = "mean_uw") %>%
  save.pred.fn(y_string = "yhat_s_tow2") %>%
  save.pred.fn(y_string = "yhat_s") %>%
  save.pred.fn(y_string = "yhat_uw")

 # # quick plot view
 # colo.plot(data.wide = tow2,
 #                    x.variable = "mean_uw",
 #                    y.variable = "LUR_mean_uw")

#compare predictsion vs observations
LUR_mean_s_tow2_plot <- colo.plot(data.wide = tow2,
                    x.variable = "mean_s_tow2",
                    y.variable = "LUR_mean_s_tow2")

LUR_mean_s_plot <- colo.plot(data.wide = tow2,
                    x.variable = "mean_s",
                    y.variable = "LUR_mean_s")

LUR_mean_uw_plot <- colo.plot(data.wide = tow2,
                    x.variable = "mean_uw",
                    y.variable = "LUR_mean_uw")

LUR_yhat_s_tow2_plot <- colo.plot(data.wide = tow2,
                    x.variable = "yhat_s_tow2",
                    y.variable = "LUR_yhat_s_tow2")

LUR_yhat_s_plot <- colo.plot(data.wide = tow2,
                    x.variable = "yhat_s",
                    y.variable = "LUR_yhat_s")

LUR_yhat_uw_plot <- colo.plot(data.wide = tow2,
                    x.variable = "yhat_uw",
                    y.variable = "LUR_yhat_uw")

ggarrange(LUR_mean_s_tow2_plot,
          LUR_mean_s_plot ,
          LUR_mean_uw_plot,
          LUR_yhat_s_tow2_plot,
          LUR_yhat_s_plot,
          LUR_yhat_uw_plot,
           common.legend = T, legend = "bottom"
          ) %>%
  annotate_figure(fig.lab = "Site UFP: Method estimates vs LUR predictions (in-sample) \nLUR fit: 10-month avg UFP ~ ll_a1_s01500 + m_to_airp  + elev_elevation + pop10_s15000")

```

```{r, delete1?}
 
# # cov.list <- c("ll_a1_s01500", "m_to_l_airp", "elev_elevation", "pop10_s15000")
# 
# ###################################
# # Lasso: logit(residual <0) ~ geocovariates
# library(glmnet)
# set.seed(1)
# #create categorical dummy variables
# df <- tow2 %>%
#   select(resid_mean_uw,
#          m_to_a1:tl_s15000)  
# 
# x <- model.matrix(resid_mean_uw~., df)[,-1]
# y <- as.numeric(tow2$resid_mean_uw <0)
# 
# ### --> Error: x and y not same length
# #select lambda through CV
# cv.out <- cv.glmnet(x = x, 
#                     y = y, 
#                     alpha=1, 
#                     #family= "gaussian"
#                     family= "binomial"
#                     )
# 
# bestlam <- cv.out$lambda.min
# 
# lasso.m <- glmnet(x = x, 
#                   y = y, 
#                  alpha = 1, 
#                  #family= "gaussian",
#                  family= "binomial"
#                  )
# 
# lasso.coef <- predict(lasso.m, 
#                       type= "coefficients", 
#                       s= bestlam)[1:(ncol(x)+1),]
# 
# #coefficients chosen by Lasso w/ bestlam that are not 0
# lasso.vars <- lasso.coef[lasso.coef != 0] %>%
#   as.data.frame() %>%
#   rownames_to_column()
# 
# names(lasso.vars) <- c("cov", "coef")
# 
# lasso.vars <- lasso.vars %>%
#   filter(#coef < 0,
#          cov != "(Intercept)"
#          ) %>%
#   arrange(desc(coef))
# 
# #intersect_a1_a1_s00500, lu_reservior_p00400, rlu_evergreen_p03000, rlu_woody_wetland_p01000, rlu_dev_med_p00750
#   
# ############################################
# #variables predictive of residuals <0
# lasso.results <- tow2 %>%
#   #scale geocovariate predictors
#   mutate_at(vars(m_to_a1:tl_s15000), ~scale(.)) %>%
#   lm(formula(paste("resid_mean_uw<0", "~", paste(lasso.vars$cov, collapse = " + " ))), 
#    data = .) %>% 
#   summary() %>%
#   broom::tidy() %>% 
#   filter(p.value < 0.05,
#          term != "(Intercept)"
#          ) 
# 
# lasso.results %>%
#   kable(caption = "Lasso-selected covariates (centered and scaled) predictive of residuals < 0 (below 1-1 line)\nonly showing p < 0.05") %>%
#   kable_styling()
# 
# #ll_a1_s01500
#  
# ############################################
# #plot most predictive covariates
# 
# #? m_to_truck, m_to_l_airp
# # ? pop10_s15000
# 
# covs <- lasso.results$term
# 
# t <- tow2 %>%
#   select(mean_uw, LUR_mean_uw, #resid_mean_uw,
#          covs
#          ) %>%
#   mutate_at(vars(covs), ~scale(.)) %>%
#   gather("cov", "value", covs)  
# 
# t %>%
#   #drop some extremely high covariate values
#   filter(value < quantile(value, 0.99)) %>%
#   ggplot(aes(x=mean_uw, y= LUR_mean_uw)) + 
#   geom_point(aes(col = value)) +
#   scale_color_distiller("centered & scaled value", palette = "Spectral") +
#   geom_abline(intercept = 0, slope = 1) +
#   geom_smooth(method = "lm", aes(fill="LS"))  +
#   facet_wrap(~cov)  
 
```

```{r, delete2?}
# #ll_a1_s01500
# tow2 <- tow2 %>%
#   mutate(weird_mean_uw = mean_uw >13000) %>%
#   select(weird_mean_uw, everything())
# 
#  tow2 %>%
#    mutate_at(vars(covs), ~scale(.)) %>%
#   gather("cov", "value", covs) %>%
#    ggplot(aes(x =value, fill = weird_mean_uw)) + 
#      geom_density(position="dodge", alpha=0.4) + 
#      facet_wrap(~cov)
     
```

Identify sites that are underpredicted by LUR. 
 -all sites

```{r}
#annual.copy <- annual
#annual <- annual.copy

high.quantile <- 0.95

#save lm predictions & residuals for dataset
annual <- annual %>%
  select(-c(mean_s, mean_s_tow2, mean_s_tow2_tod2,
            yhat_s, yhat_s_tow2, yhat_s_tow2_tod2, 
            yhat_m_day_hr)) %>%
  drop_na() %>%
  save.pred.fn(y_string = "mean_uw") %>%
  mutate(
    residual_lvl = ifelse(resid_mean_uw >quantile(resid_mean_uw, high.quantile), "high",
                          ifelse(resid_mean_uw < quantile(resid_mean_uw, (1-high.quantile)), "low", "normal")
                          )
  )
  
 #compare predictsion vs observations
colo.plot(data.wide = annual,
                    x.variable = "mean_uw",
                    y.variable = "LUR_mean_uw", 
          mytitle = "Site UFP: Method estimates vs LUR predictions (in-sample) \nLUR fit: 10-month avg UFP ~ ll_a1_s01500 + m_to_airp  + elev_elevation + pop10_s15000", col.by = "residual_lvl")

```

 
```{r}
# Lasso 
 
set.seed(1)
#create categorical dummy variables
df <- annual %>%
  select(resid_mean_uw,
         m_to_a1:tl_s15000)  

x <- model.matrix(resid_mean_uw~., df)[,-1]
y <- as.numeric(annual$resid_mean_uw)# <0)

### --> Error: x and y not same length
#select lambda through CV
cv.out <- cv.glmnet(x = x, 
                    y = y, 
                    alpha=1, 
                    family= "gaussian"
                    #family= "binomial"
                    )

bestlam <- cv.out$lambda.min

lasso.m <- glmnet(x = x, 
                  y = y, 
                 alpha = 1, 
                 family= "gaussian",
                 #family= "binomial"
                 )

lasso.coef <- predict(lasso.m, 
                      type= "coefficients", 
                      s= bestlam)[1:(ncol(x)+1),]

#coefficients chosen by Lasso w/ bestlam that are not 0
lasso.vars <- lasso.coef[lasso.coef != 0] %>%
  as.data.frame() %>%
  rownames_to_column()

names(lasso.vars) <- c("cov", "coef")

lasso.vars <- lasso.vars %>%
  filter(cov != "(Intercept)") %>%
  arrange(desc(coef))

#intersect_a1_a1_s00500, lu_reservior_p00400, rlu_evergreen_p03000, rlu_woody_wetland_p01000, rlu_dev_med_p00750
  
############################################
#variables predictive of residuals <0
lasso.results <- annual %>%
  #scale geocovariate predictors
  mutate_at(vars(m_to_a1:tl_s15000), ~scale(.)) %>%
  lm(formula(paste("resid_mean_uw<0", "~", paste(lasso.vars$cov, collapse = " + " ))), 
   data = .) %>% 
  summary() %>%
  broom::tidy() %>% 
  filter(p.value < 0.05,
         term != "(Intercept)"
         ) 

lasso.results %>%
  kable(caption = "Lasso-selected covariates (centered and scaled) predictive of residuals \nonly showing p < 0.05") %>%
  kable_styling()

#ll_a1_s01500
 
############################################
#plot most predictive covariates

#? m_to_truck, m_to_l_airp
# ? pop10_s15000

covs <- lasso.results$term

annual.scaled <- annual %>%
  select(site_id, mean_uw, LUR_mean_uw, resid_mean_uw, residual_lvl,
         covs) %>%
  mutate_at(vars(covs), ~scale(.)) %>%
  as.data.frame()

# #? delete
# annual.scaled %>%
#   ggplot(aes(x=resid_mean_uw, fill= residual_lvl)) + 
#   geom_histogram() + 
#   labs(title = paste0("LUR Residuals, \ncolored by low/high quantile: ", 1-high.quantile),
#        x = "residual")

p <- annual.scaled %>%
  ggplot(aes(x=mean_uw, 
             fill=residual_lvl)) + 
  geom_histogram() + 
  labs(title = "10-month site average (mean UW), colored by LUR residual level",
       x = "Mean UW 10-month site concentration (pt/cm3) "
       )
    
p

#log x scale - annual avgs are log-normally distributed
p + scale_x_log10() + 
  labs(subtitle = "log x scale")

annual.scaled %>%
  gather("cov", "value", covs) %>%
  ggplot(aes(x=value, fill=residual_lvl)) +
  geom_density(alpha=0.3) + 
  facet_wrap(~cov, scales = "free")

annual.scaled %>%
  ggplot(aes(x=mean_uw, y= LUR_mean_uw)) + 
  geom_point(aes(
    col = residual_lvl
    # col = resid_mean_uw, size = resid_mean_uw>quantile(resid_mean_uw, high.quantile) 
    #              | resid_mean_uw < quantile(resid_mean_uw, (1-high.quantile))
                )) +
  #scale_color_distiller("residual", palette = "Spectral") +
  geom_abline(intercept = 0, slope = 1) +
  geom_smooth(method = "lm", aes(fill="LS")) + 
    theme(legend.position = "bottom") + 
  labs(size = paste0("high residual (", 1-high.quantile, ")"),
       title = "Comparison of 10-month site averages: 'UW mean' vs LUR prediction"
       )  

```

LUR residuals mapped

```{r}
#Export csv w/ site lat/long
# site_averages <- left_join(annual.scaled, unique(ufp[c("site_id", "site_lat", "site_long")])) %>%
#   select(contains("site"), everything()) %>%
#   rename(latitude = site_lat,
#          longitude = site_long)
# 
# write.csv(site_averages, 
#           file =  file.path("Data", "Aim 2", "Mobile Monitoring", "site_averages.csv"),
#           row.names = F)

```

![UFP concentrations. darker red = higher concentration](../Write Up/1. Proposal/Aim 2. 2019 UFP/Images/site averages 191223.png)


```{r}
stops <- ufp %>%
  left_join(annual.scaled[c("site_id", "residual_lvl", "mean_uw")]) %>%
  drop_na(residual_lvl)

#boxplots of stop readings
stops %>%
  ggplot(aes(x=site_id, y=ptrak)) + 
  geom_boxplot(aes()) +
  geom_point(aes(y=mean_uw, col="mean uw")) +
  scale_y_log10() +
  facet_wrap(~residual_lvl, labeller = "label_both", 
             scales="free_x",
             nrow=1) + 
  labs(y = "UFP Stop Conc (pt/cm3)",
       col = "")


 
```

### --> ? ANOVA?

## --> ? how to fit interction term & then plot to see if predictions are closer to the 1-1 line?

```{r}

```
















Select an estimate to use

```{r}


#saveRDS(annual, file.path("Data", "Aim 2", "Mobile Monitoring", "annual.rda"))

```


notes:
* ? could subtract a constant from regression estimates if they are constantly higher
