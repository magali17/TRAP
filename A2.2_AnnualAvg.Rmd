---
title: 'Aim 2: Annual Averages'
author: "Magali Blanco"
date: "12/19/2019"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r, incldue=F}
#notes
# refs:
## Sampson 2013 National UK Model using PLS for PM2.5

```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, 
                      cache=T, cache.comments = F, 
                      message = F, warning = F, 
                      tidy.opts=list(width.cutoff=60), tidy=TRUE,
                      fig.height = 8, fig.width = 8
                      )  

# Clear workspace of all objects and unload all extra (non-base) packages
rm(list = ls(all = TRUE))
if (!is.null(sessionInfo()$otherPkgs)) {
  res <- suppressWarnings(
    lapply(paste('package:', names(sessionInfo()$otherPkgs), sep=""),
      detach, character.only=TRUE, unload=TRUE, force=TRUE))
}

pacman::p_load(dplyr, tidyverse, knitr, kableExtra, Hmisc, EnvStats, lubridate, glmnet, ggpubr, VCA,
               #descriptive statistics
               qwraps2, #, magrittr
               glmnet #lasso
               )    
#Himsc: describe(); EnvStats: summaryFull(); ggpubr: ggarrange()

options(knitr.kable.NA = '')
set.seed(1)
source("A2.0.1_Var&Fns.R")

#load data
mm <- readRDS(file.path("Data", "Aim 2", "Mobile Monitoring", "mm_2019-12-22.rda"))
mm.wide <- readRDS(file.path("Data", "Aim 2", "Mobile Monitoring", "mm.w_2019-12-22.rda"))

cov_mm <- readRDS(file.path("Data", "Aim 2", "Geocovariates", "cov_mm_preprocessed.rda")) %>%
  select(#-contains("lambert"), -latitude, -longitude,
         -contains("pop_"), -contains("pop90_")
         )
site_locations <- cov_mm %>% 
  select(
    site_id, 
    contains("lambert"), latitude, longitude)

cov_mm <- cov_mm %>% select(-contains("lambert"), -latitude, -longitude)
cov_names <- names(cov_mm)[!names(cov_mm) %in% c("site_id")]


ufp0 <- mm.wide %>% 
  select(date:site_no,
         ptrak_pt_cm3_median,
         ptrak_pt_cm3_mean
         ) %>%
  mutate(
    site_id = factor(site_id),  
    # New Hour Binning
      tod5 = factor(ifelse(hour %in% seq(3,8), "3_8",
                                       ifelse(hour %in% seq(9,11), "9_11", 
                                              ifelse(hour %in% seq(12,15), "12_15",
                                                     ifelse(hour %in% seq(16,20), "16_20", "21_2")))),
                         levels= c("3_8", "9_11", "12_15", "16_20", "21_2")),
  
    tod3 = factor(ifelse(hour %in% seq(0,8), "0_8",
                                       ifelse(hour %in% seq(9,17), "9_17", "18_23")),
                         levels= c("0_8", "9_15", "16_23")),
    #selected this break b/c: 1) UFP values rise during warmer daytime hours; 2) mixing ht is higher during day; 3) "daytime" = typical business as usual mm campaigns
    tod2 = factor(ifelse(hour %in% seq(9, 17), "9_17", "18_08"), 
                         levels= c("9_17", "18_08"))
    ) %>%
  #drop winter for now
  filter(season != "winter")  

#dataset for main analyses w/ median UFP stop concentrations &  geocovariates
ufp0 <- ufp0 %>% select(-ptrak_pt_cm3_mean) %>%
  rename(ptrak = ptrak_pt_cm3_median) %>%
  #remove NAs
  drop_na(ptrak) %>%
  left_join(cov_mm)

#trim data 
ufp <- ufp0 %>%
  group_by(site_id) %>%
  filter(ptrak <= quantile(ptrak, (1-trim_quantile), na.rm = T),
         ptrak >= quantile(ptrak, (trim_quantile), na.rm = T)) %>%
  ungroup()  

#no. months sampled
months.sampled <- month.abb[3:12]
n.months.sampled <- length(months.sampled)

n.seasons.sampled <- length(unique(ufp$season))

```

```{r} 
untrimmed.plot <- ufp0 %>% 
  ggplot(aes(y=ptrak)) + 
  geom_boxplot() + #scale_y_log10() +
  labs(title = "Untrimmed data",
       y= "UFP (pt/cm3)")

trimmed.plot <- ufp %>%
  ggplot(aes(y=ptrak)) + 
  geom_boxplot() +
  labs(title = paste0("Trimmed top and bottom ", trim_quantile*100, "% \nof each site's stops" ),
       y= "UFP (pt/cm3)")
  
ggarrange(untrimmed.plot, trimmed.plot ) 

```

# Sample size 

```{r}
## table of stop counts/site overall & stratified
stop.counts.total <- ufp %>%
  dplyr::group_by(site_id) %>%
  # no. samples/site
  dplyr::summarize(N = n()) %>%
  # distribution of no. samples
  distribution.table(var.string = "N") %>%
  mutate(Time = "Overall") %>%
  select(Time, everything())

stop.counts.season <- ufp %>%
  dplyr::group_by(site_id, season) %>%
  # no. samples/site
  dplyr::summarize(N = n()) %>%
  #group by time period of interest
  group_by(Time=season) %>%
  distribution.table(var.string = "N")  

stop.counts.tow <- ufp %>%
  group_by(site_id, time_of_week) %>%
  # no. samples/site
  dplyr::summarize(N = n()) %>%
  #group by time period of interest
  group_by(Time=time_of_week) %>%
  distribution.table(var.string = "N")  
  
stop.counts.tod <- ufp %>%
  group_by(site_id, time_of_day) %>%
  # no. samples/site
  dplyr::summarize(N = n()) %>%
  #group by time period of interest
  group_by(Time=time_of_day) %>%
  distribution.table(var.string = "N")  
  
rbind(
  stop.counts.total,
  stop.counts.season,
  stop.counts.tow,
  stop.counts.tod
    ) %>%
  add_row(Time = "season", .before = 2) %>%
  add_row(Time = "time of week", .before = 6) %>%
  add_row(Time = "time of day", .before = 9) %>%
  kable(caption = "Number of stop samples per site (after trimming; N = No. sites sampled)", 
        col.names = c("Time", "N", "mean (SD)", "median (IQR)", "Min", "Max")
        ) %>%
  add_indent(c(3:5, 7:8, 10:14)) %>%
  kable_styling()

## plot stratified by time 
ufp %>%
  group_by(site_no, season, time_of_week, time_of_day) %>%
  dplyr::summarize(N = n()) %>%
  ggplot(aes(x=site_no, y=N, fill=time_of_day)) +
  geom_bar(stat = "identity", aes(group=site_no))+
  facet_wrap(~season+time_of_week, ncol = 2) +
  labs(title = "Number of samples by site and time (after trimming)",
    y="No. Samples",
       x= "Site No.",
       fill = "time of day")

#ggsave(filename = file.path("A2_Images", "samples_tow_tod.png"), 
#      height = 8, width = 8)


# hist of unique hours locations have sampled at 
ufp %>%
  ggplot(aes(x=hour, fill= time_of_day)) +
  geom_bar(aes( )) + 
  labs(title = "Unique hours sampled (after trimming)",
       y = "No. Times Sampled",
       fill = "Time of Day"
       )

```

# Concentrations at different times

Table

```{r}
# table of concentrations distribution 
# overall
ufp.dist.overall <- ufp %>% ungroup() %>%
  distribution.table(dt = ., var.string = "ptrak") %>%
  mutate(time = "Overall") %>%
  select(time, everything())

#by season
season.distrib <- ufp %>% group_by(time=season) %>%
  distribution.table(dt = ., var.string = "ptrak")  

#by TOW
tow.distrib <- ufp %>% group_by(time= time_of_week) %>%
  distribution.table(dt = ., var.string = "ptrak")  

#by TOD
tod.distrib <- ufp %>% group_by(time= time_of_day) %>%
  distribution.table(dt = ., var.string = "ptrak")  

rbind(
  ufp.dist.overall,
  season.distrib,
  tow.distrib,
  tod.distrib
) %>%
  add_row(time = "Season", .before = 2) %>%
  add_row(time = "Time of Week", .before = 6) %>%
  add_row(time = "Time of Day", .before = 9) %>%
  kable(caption = "Distribution of median stop concentrations over time") %>%
  add_indent(c(3:5, 7:8, 10:14)) %>%
  kable_styling()
  
```

Plots

```{r}
#by time
p.hour <- ufp %>% 
  ggplot(aes(x=hour, y=ptrak, group=hour)) + 
  geom_boxplot() +
  labs(x = "hour") + 
  labs(y="")
   
p.day <- ufp %>%
  ggplot(aes(x=day, y=ptrak)) + 
  geom_boxplot() + 
  labs(y="")

p.season <- ufp %>%
  ggplot(aes(x=season, y=ptrak)) + 
  geom_boxplot() + 
  labs(y="")

ggarrange(p.hour, p.day, p.season) %>%
  annotate_figure(left = "UFP Concentration (pt/cm3)", 
                  fig.lab = "UFP concentrations observed at stops"
                  )

# hist of concentrations distribution: stratified by: major roadway proximity, airport proximity, population density, elevation  

```

# Concentrations by correlated geocovariates

### --> use PLS plot here 

```{r}
#select.cov <- #20

geo.cor <- ufp %>%
  #select_if(is.numeric) %>%
  select(ptrak,
         cov_names) %>%
  cor(use = "complete.obs") %>%
  as.data.frame() %>%
  select(ptrak) %>%
  rownames_to_column() %>%
  arrange(desc(abs(ptrak))) %>%
  filter(#row_number() %in% c(1:select.cov, (n()-select.cov):n()),
         rowname != "ptrak"
         )  %>%
  rename(geocovariate = rowname,
         cor_with_ptrak = ptrak) 
 
# geocov vs correlation
geo.cor %>%
  ggplot(aes(x=cor_with_ptrak, y=geocovariate)) + 
  geom_point() + 
  geom_vline(xintercept = 0, alpha=0.5, linetype = "dashed") + 
  labs(x = "Correlation (R) with PTRAK stop readings",
       title = "Correlation (R) between stop covariates and PTRAK readings"
       )

# DELETE? 
# #select covariates most correlated w/ UFP stop readings
# cov.list <- geo.cor$geocovariate[1:40]
# #plot UFPs based on strongest variable correlations
# ufp %>%
#   select(ptrak, 
#          cov.list) %>%
#   gather("geo", "value", -ptrak) %>%
#   ggplot(aes(x=value, y=ptrak)) + 
#   geom_point(alpha=0.1) + 
#   geom_smooth(se = T) +
#   facet_wrap(~geo, scales = "free")
  
```

# Annual averages 

## Weighted Means   
   
a) calculate weighted annual avg (Mar - Oct) means using different methods to determine how sensitve estimates are to using various approaches: 
* unweighted
* weighted (season, tow, tod) 
* regression 
  + binning tow & tod differently 

###  Compare different averages

Site-specific data averages 

```{r}
#calc diff summary measures
## using within site information
ufp <- ufp %>%
  group_by(site_no) %>%
  mutate(mean_uw = mean(ptrak, na.rm=T))

 #means by seaason-wk-tod5 (5 tods)
min.t <- 5*2*n.seasons.sampled
s_tow2_tod5 <- ufp %>%
   group_by(site_no, season, time_of_week, tod5) %>%
   dplyr::summarize(mean = mean(ptrak)) %>%
   ungroup() %>%
   unite("swh", season:tod5, remove = F) %>%
   group_by(site_no) %>%
   mutate(unique_times = n_distinct(swh)) #%>% filter(unique_times >= min.t)
  #there are no sites w/ sampling during all 30/40 unique times (5 tod x 2 tow x 3 or 4 seasons)
 if(!max(s_tow2_tod5$unique_times) >= min.t){ rm(s_tow2_tod5) }
 
 #means by seaason-wk-tod3 (3 tods)
min.t <- 3*2*n.seasons.sampled
s_tow2_tod3 <- ufp %>%
   group_by(site_no, season, time_of_week, tod3) %>%
   dplyr::summarize(mean = mean(ptrak)) %>%
   ungroup() %>%
   unite("swh", season:tod3, remove = F) %>%
   group_by(site_no) %>%
   mutate(unique_times = n_distinct(swh)) #%>% filter(unique_times >=min.t)
 #there are no sites w/ sampling during all 18/24 unique times (3 tod x 2 tow x 3/4 seasons)
if(!max(s_tow2_tod3$unique_times) >= min.t){rm(s_tow2_tod3)}
 
 #means by seaason-wk-tod2 (2 tods)
    #8 sites w/ sampling during all 12 unique times (2 tod x 2 tow x 3 seasons)
    #none when looking at all 4 seasons
 min.t <- 2*2*n.seasons.sampled
 s_tow2_tod2 <- ufp %>%
   group_by(site_no, season, time_of_week, tod2) %>%
   dplyr::summarize(mean = mean(ptrak)) %>%
   ungroup() %>%
   unite("swh", season:tod2, remove = F) %>%
   group_by(site_no) %>%
   mutate(unique_times = n_distinct(swh)) %>% 
   filter(unique_times >= min.t)
if(!max(s_tow2_tod2$unique_times) >= min.t){rm(s_tow2_tod2)}

 ## using tod2, caclulate weighted mean for sites w/ obs during all time combinations
 s_tow2_tod2 <- s_tow2_tod2 %>%
   mutate(
     season.wt = 1/n.seasons.sampled,
     wk.wt = ifelse(time_of_week == "weekday", 5/7, 2/7),
     tod.wt = ifelse(tod2 == "9_17", 9/24, 15/24),
     wt = season.wt * wk.wt * tod.wt,
     wt_times_mean = wt*mean) %>% 

 # #check that weights for all sites = 1. #looks good
 # s_tow2_tod2 %>%
 #   group_by(site_no) %>%
 #   summarize(n = sum(wt))
   
   #calculate each site's weighted mean
   group_by(site_no) %>%
   dplyr::summarize(mean_wt = sum(wt_times_mean))
   
 #means by season-wk7 (2 tods)
    #no sites w/ sampling during all 42/56 unique times (2 tod x 7 tow x 3/4 seasons)
min.t <- 2*7*n.seasons.sampled
s_tow7_tod2 <- ufp %>%
   group_by(site_no, season, day, tod2) %>%
   dplyr::summarize(mean = mean(ptrak)) %>%
   ungroup() %>%
   unite("swh", season:tod2, remove = F) %>%
   group_by(site_no) %>%
   mutate(unique_times = n_distinct(swh)) #%>% filter(unique_times >=min.t)
if(!max(s_tow7_tod2$unique_times) >= min.t){rm(s_tow7_tod2)} 

 #no tod
    # no site w/ 21/28 unique times (3/4 seasons x 7 days)
min.t <- 7*n.seasons.sampled
s_tow7 <- ufp %>%
   group_by(site_no, season, day) %>%
   dplyr::summarize(mean = mean(ptrak)) %>%
   ungroup() %>%
   unite("swh", season:day, remove = F) %>%
   group_by(site_no) %>%
   mutate(unique_times = n_distinct(swh)) #%>% filter(unique_times >=min.t)
if(!max(s_tow7$unique_times) >= min.t){rm(s_tow7)} 

min.t <- 2*n.seasons.sampled
s_tow2 <- ufp %>%
   group_by(site_no, season, time_of_week) %>%
   dplyr::summarize(mean = mean(ptrak)) %>%
   ungroup() %>%
   unite("swh", season:time_of_week, remove = F) %>%
   group_by(site_no) %>%
   mutate(unique_times = n_distinct(swh)) %>%
   filter(unique_times >=min.t) %>%
 #  caclulate weighted mean for sites w/ obs during all time combinations
   mutate(
     season.wt = 1/n.seasons.sampled,
     wk.wt = ifelse(time_of_week == "weekday", 5/7, 2/7),
     wt = season.wt * wk.wt,
     wt_times_mean = wt*mean) %>%
   #calculate each site's weighted mean
   group_by(site_no) %>%
   dplyr::summarize(mean_wt = sum(wt_times_mean))

 #season
    # 305 sites w/ sampling during all 3 seasons
 
min.t <- n.seasons.sampled
s <- ufp %>%
   group_by(site_no, season) %>%
   dplyr::summarize(mean = mean(ptrak)) %>%
   group_by(site_no) %>%
   mutate(unique_times = n_distinct(season)) %>%
   filter(unique_times == min.t) %>%
 #no_tod_df %>% select(site_no) %>% n_distinct()
 #  caclulate weighted mean for sites w/ obs during all time combinations
   mutate(
     season.wt = 1/n.seasons.sampled,
     wt_times_mean = season.wt*mean) %>%
   #calculate each site's weighted mean
   group_by(site_no) %>%
   dplyr::summarize(mean_wt = sum(wt_times_mean))

 #month-weighted avg
 #locations w/ samples Mar - Nov

 m <- ufp %>%
   group_by(site_no, month) %>%
   dplyr::summarize(mean = mean(ptrak)) %>%
   group_by(site_no) %>%
   filter(n_distinct(month) == n.months.sampled) %>%
 #  caclulate weighted mean for sites w/ obs during all time combinations
   mutate(
     wt_times_mean = mean * 1/n.months.sampled) %>%
   #calculate each site's weighted mean
   group_by(site_no) %>%
   dplyr::summarize(mean_wt = sum(wt_times_mean))

#105 locations w/ samples each month #nrow(m)

``` 

Regression to calculate annual (for now, 3 season) averages.

```{r}
#df for predictions - all unique variable combinations
preds.uw <- data.frame(site_no = unique(ufp$site_no))

preds.s <- expand.grid(site_no = unique(ufp$site_no), 
                            season = unique(ufp$season))

preds.s.tow <- expand.grid(site_no = unique(ufp$site_no), 
                            season = unique(ufp$season),
                            time_of_week = unique(ufp$time_of_week))

preds.s.tow.tod2 <- expand.grid(site_no = unique(ufp$site_no), 
                            season = unique(ufp$season), 
                            time_of_week = unique(ufp$time_of_week), 
                            tod2 = unique(ufp$tod2))

preds.m.day.hr <- expand.grid(site_no = unique(ufp$site_no), 
                            month = unique(ufp$month), 
                            day = unique(ufp$day), 
                            hour = unique(ufp$hour))


#different fits
fit.uw <- lm(ptrak ~ factor(site_no) , data = ufp)
fit.s <-  update(fit.uw, ~. + season)
fit.s.tow <- update(fit.s, ~. + time_of_week)
fit.s.tow.tod2 <- update(fit.s.tow, ~. + tod2)
fit.m.day.hr <- lm(ptrak ~ factor(month, ordered = F) + factor(day, ordered = F) + factor(hour) + site_no , data = ufp)
#summary(fit.m.day.hr)

#predictions
preds.uw$yhat_uw <- predict(fit.uw, preds.uw)
preds.s$yhat <- predict(fit.s, preds.s) 
preds.s.tow$yhat <- predict(fit.s.tow, preds.s.tow) 
preds.s.tow.tod2$yhat <- predict(fit.s.tow.tod2, preds.s.tow.tod2) 
preds.m.day.hr$yhat <- predict(fit.m.day.hr, preds.m.day.hr) 

#calculate weighted avgs based on predictions

season.wt <- 1/n.seasons.sampled

preds.s <- preds.s %>%
  mutate(
    season_wt = season.wt,
    yhat = yhat*season_wt
  ) %>%
  group_by(site_no) %>%
  dplyr::summarize(yhat_s = sum(yhat))

preds.s.tow <- preds.s.tow %>%
  mutate(
    season_wt = season.wt,
    tow_wt = ifelse(time_of_week == "weekday", 5/7, 2/7),
    yhat = yhat*season_wt*tow_wt
  ) %>%
  group_by(site_no) %>%
  dplyr::summarize(yhat_s_tow2 = sum(yhat))
 
preds.s.tow.tod2 <- preds.s.tow.tod2 %>%
  mutate(
    season_wt = season.wt,
    tow_wt = ifelse(time_of_week == "weekday", 5/7, 2/7),
    tod_wt = ifelse(tod2 == "9_17", 9/24, (1- 9/24)),
    yhat = yhat*season_wt*tow_wt*tod_wt
  ) %>%
  group_by(site_no) %>%
  dplyr::summarize(yhat_s_tow2_tod2 = sum(yhat))
  

#calculate weights based on samling hours available 
#sort(unique(ufp$hour))
hrs.sampled <- sort(unique(ufp$hour))
hrs.wt1 <- c(6:22)
non.wt1.hrs <- (0:23)[!(0:23) %in% hrs.wt1]
#other hrs sampled w/o a wt=1
other.hrs.sampled <- hrs.sampled[!hrs.sampled %in% hrs.wt1]
other.hrs.wt <- length(non.wt1.hrs)/length(other.hrs.sampled)
 
#preds.m.day.hr2 <-preds.m.day.hr

preds.m.day.hr <- preds.m.day.hr %>%
  mutate(
    #double the weight of the 4 hrs neighboring the 4 night hrs w/o sampling (1-4 AM)
    hr_wt = ifelse(hour %in% hrs.wt1, 1/24, other.hrs.wt/24),
    month_wt = 1/n.months.sampled,
    day_wt = 1/7,
    yhat = yhat*hr_wt*month_wt*day_wt
    ) %>%
  #check that hr weights add up to 1. looks good
  # t <-preds.m.day.hr %>% group_by(site_no) %>%
  #   dplyr::summarize(sum_wt = sum(hr_wt*month_wt*day_wt)) 

  group_by(site_no) %>%
  dplyr::summarize(yhat_m_day_hr = sum(yhat))

#merge all predictions to 1 df
yhats <- left_join(preds.uw, preds.s) %>%
  left_join(preds.s.tow) %>%
  left_join(preds.s.tow.tod2) %>%
  left_join(preds.m.day.hr)


```

Save annual averages from all methods with geocovariates.

```{r, include=T}
#join all estimates
annual <- ufp %>% 
  ungroup() %>%
  select(site_no, site_id, 
         cov_names,
         mean_uw,
         ) %>%
  unique() %>%
  left_join(s_tow2_tod2) %>%
  rename(mean_s_tow2_tod2 = mean_wt) %>%
  left_join(s_tow2) %>%
  rename(mean_s_tow2 = mean_wt) %>%
  left_join(s) %>%
  rename(mean_s = mean_wt) %>%
  left_join(yhats) %>%
  select(site_no, site_id, 
         mean_uw:yhat_m_day_hr, everything())

method_names_all <- annual %>%
  select(contains("mean_"), contains("yhat_")) %>%
  names()

#drop estimate/s that can only be calculated w/ regression
method_names <- setdiff(method_names_all, c("yhat_m_day_hr"))


annual.l <- annual %>%
  gather(key = "method_weight", value = "ufp", method_names_all) %>%
  drop_na() %>%
  separate(method_weight, into=c("method", "weight"), 
                     sep = "_" , remove = F, extra = "merge")  

```

Disribution of annual UFP predictions.

```{r}
#table 
annual.l %>%
  group_by(method, weight) %>%
  distribution.table(var.string = "ufp") %>%
  kable(caption = "Annual average site estimates using various methods and weights") %>%
  kable_styling()

#histograms
annual.l %>%
  ggplot(aes(x=ufp)) + 
  geom_histogram() + 
  facet_grid(rows = vars(method), cols = vars(weight)) + 
  labs(title = "Annual average site estimates using various methods and weights",
       x = "Annual UFP concentration estimate (pt/cm3)"
       )

```

### --> add log UFP to dataset; adjust rest of code to ?only include log-transformed variables

```{r, fig.height = 8}
#TOD2
annual.l.tod2 <- annual.l %>%
  filter(site_no %in% s_tow2_tod2$site_no,
         !weight %in% c("m_day_hr")
         ) 
## ranges
annual.l.tod2.rage <- annual.l.tod2 %>%
  group_by(site_no, method) %>%
  dplyr::summarize(
    min = min(ufp),
    max = max(ufp),
    mean = mean(ufp))

ufp_by_method (dt = annual.l.tod2, 
               dt.range = annual.l.tod2.rage) 

#TOW2
annual.l.tow2 <- annual.l %>%
  filter(site_no %in% s_tow2$site_no,
         !weight %in% c("m_day_hr",
                        "s_tow2_tod2"
                        )) %>%
  mutate(group = cut(site_no, breaks = 12, labels = F))

## ranges
annual.l.tow2.rage <- annual.l.tow2 %>%
  group_by(group, site_no, method) %>%
  dplyr::summarize(
    min = min(ufp),
    max = max(ufp),
    mean = mean(ufp))

ufp_by_method (dt = annual.l.tow2, 
               dt.range = annual.l.tow2.rage) + 
  facet_wrap(~group, scales = "free")

```
 
### Characterize sites w/ smallest & largest inter-method differences 

- sites w/ high inter-method variabiitly have high extreme values, while those w/ low inter-method variability have either: a) no or b) both high and low extreme values

## --> update column names of fn 

```{r}
# TOD2
tod2 <- annual %>% drop_na()

# not including yhat_m_day_hr b/c only available for regression method
tod2$min <- apply(tod2[,method_names], 1, function(x)  range(x)[1])  
tod2$max <- apply(tod2[,method_names], 1, function(x)  range(x)[2])  

tod2$diff <- tod2$max - tod2$min  

```

```{r}
# TOW2
tow2 <- annual %>% 
  select(-mean_s_tow2_tod2,
         -yhat_s_tow2_tod2,
         -yhat_m_day_hr
         ) %>%
  drop_na()  

method_names_tow2 <- method_names[!grepl("s_tow2_tod2",method_names)]

tow2$min <- apply(tow2[,method_names_tow2], 1, function(x)  range(x)[1])  
tow2$max <- apply(tow2[,method_names_tow2], 1, function(x)  range(x)[2])  
tow2$diff <- tow2$max - tow2$min 

```

```{r, include=T}
#plots looking at which sites have largest diff btwn estimates
extremes <- 0.25

#sites w/ most/least variability in estimates
tow.2.high <- tow2 %>%
  filter(diff > quantile(tow2$diff, (1-extremes)))
tow.2.low <- tow2 %>%
  filter(diff < quantile(tow2$diff, extremes))

#RAW values for high/low values - are there any extreme values?
#high range of values
h.tow2 <- ufp %>%
  filter(site_id %in% unique(tow.2.high$site_id)) %>%
  mutate(diff_lvl = "high")
l.tow2 <- ufp %>%
  filter(site_id %in% unique(tow.2.low$site_id)) %>%
  mutate(diff_lvl = "low")
h.l.tow2 <- rbind(h.tow2, l.tow2)

#density/box plots for UFP  
h.l.tow2 %>%
  ggplot(aes(x=ptrak)) +
  geom_density(aes(fill = diff_lvl), alpha=0.5) +
  labs(subtitle = paste("Sea_TOW2, n =", length(unique(h.l.tow2$site_id))))

```

```{r, fig.height=8}
h.l.tow2 %>%
  ggplot(aes(x=site_no, y= ptrak, fill=diff_lvl, group=site_no)) + 
  geom_boxplot() +
  #scale_x_continuous(breaks=h.l.tow2$site_no, labels=h.l.tow2$site_id) + 
  theme(axis.text.x = element_text(angle = 90)) +
  facet_wrap(~diff_lvl, scales="free_x") +
  labs(title = "Sites with 'high' and 'low' inter-method variability. \nX-asis is arranged by stop order.",
       subtitle = paste("Sea_TOW2, n =", length(unique(h.l.tow2$site_id))))

```

Density plots by geocovariates

more varibility in UFP estimates at places w/ more direct sources? (Bossche 2015)

```{r}
#tow
tow.2.high$diff_lvl <- "high"
tow.2.low$diff_lvl <- "low"
tow.2.high.low <- rbind(tow.2.high, tow.2.low) 

# cov.list2 <- c("ll_a1_s01500", "m_to_coast", "m_to_airp", "m_to_l_port",  "m_to_rr", "m_to_truck", "elev_elevation", "pop10_s15000", "ndvi_q50_a07500", "imp_a01000", "lu_resi_p15000")

#plot 20 most correlated covariates
tow.2.high.low %>%
  gather("geocov", "value", c(geo.cor$geocovariate[1:20])) %>%
  ggplot(aes(x = value, fill = factor(diff_lvl))) +
  geom_density(alpha=0.5) +
  facet_wrap(~geocov, scales="free") +
  labs(subtitle = paste("Sea_TOW2, n =", length(unique(tow.2.high.low$site_id))))

```

### Effect sizes: method, site, TOW, TOD

- most variation comes from site, tiny (but significant) amount from estimation method
- mean_mo and mean_sea estimates significantly lower than two2 and tow2_tod2

```{r, include=T}
#### Sea_TOW2_TOD2
 
# #ANOVA for TOD2
# tod2.anova <- tod2 %>%
#   select(site_id:mean_sea) %>%
#   gather("method", "value", -site_id, -route)  
# 
# anovaVCA(value ~ method + site_id, Data=as.data.frame(tod2.anova))
#  
# tod2.lm1a <- tod2.anova %>%                               #mean_sea_tow2_tod2
#   mutate(method = relevel(factor(method, ordered = F), ref = "mean_sea_tow2_tod2")) %>% 
#   lm(value ~ method + site_id, data = .) 
# 
# anova(tod2.lm1a) %>% kable()
# summary(tod2.lm1a)
# #CI
# "confidence intervals"
# confint(tod2.lm1a)[c(1:4),]


```

#### Compare averaging method effects (at the annual avg level). 

```{r, eval=T}
#ANOVA for TOW2
 tow2.anova <- tow2 %>%
  select(site_id, method_names_tow2) %>%
  gather("method", "value", -site_id) %>%
  ungroup() %>%
  mutate(
    method = as.factor(method),
    site_id = as.factor(site_id)) %>%
  as.data.frame()

anovaVCA(value ~ method + site_id, Data = tow2.anova)
 

tow2.lm1a <- tow2.anova %>%  
  mutate(method = relevel(factor(method, ordered = F), ref = "mean_s_tow2")) %>% 
  lm(value ~ method + site_id, data = .)

anova(tow2.lm1a) #%>% kable()
summary(tow2.lm1a)
#CI
#confint(tow2.lm1a)[c(1:7),]

```

#### Compare temporal effects (at the stop-level avg level).    
Y ~ season + tow + tod + site_id 

How do the seasonal/TOW/TOD effects impact annual mean UFP estimates relative to the method effects (~100-200 pt/cm3)

Using sites w/ season-TOW2-TOD2 estimates.

### --> ? make time_of_week --> day of week? tod2...?

```{r, include=T}
tod.sites <- tod2$site_id

tod2.stops <- ufp %>%
  filter(site_id %in% tod.sites) %>%
  as.data.frame()

anovaVCA(ptrak~ season + time_of_week + tod2 + site_id,  Data = tod2.stops)
tod2_lm1 <- lm(ptrak ~ season + time_of_week + tod2 + site_id, data = tod2.stops)
anova(tod2_lm1) #%>% kable()
summary(tod2_lm1) 
  
anovaVCA(ptrak ~ season + day + hour + site_id,  Data = tod2.stops)
tod2_lm2 <- lm(ptrak ~ season + factor(day, ordered = F) + factor(hour) + site_id, data = tod2.stops)
anova(tod2_lm2) #%>% kable()

 
```

Using all sites. 

```{r}
#using all data 
anovaVCA(ptrak ~ season + time_of_week + tod2 + site_id,  Data = as.data.frame(ufp))
tow2_lm1 <- lm(ptrak ~ season + time_of_week + tod2 + site_id, data = ufp)
anova(tow2_lm1) %>% kable()
summary(tow2_lm1) #$coef #[c(1:6),]

anovaVCA(ptrak ~ season + day + hour + site_id,  Data = as.data.frame(ufp))
tow2_lm2 <- lm(ptrak ~ season + factor(day, ordered = F) + factor(hour) + site_id, data = ufp)
anova(tow2_lm2) %>% 
  kable() %>% kable_styling()
summary(tow2_lm2) #$coef #[c(1:6),]

```

#### Compare impacts on LUR (log-transformed values)   

Fitting LUR using different averaging methods for UFP; plotting observed averages vs model predictions.

Correlations. Using season-TOW2 estimates (n = `r nrow(tow2)`).

Evaluate LUR predictions using different methods.

note: regression estimates are more similar than site avg methods 

```{r}
#-sites w/ TOW2
tow2.copy <- tow2
#tow2 <-  tow2.copy

reg_predictors <- paste0(geo.cor$geocovariate[1:5], collapse = " + ")

#save lm predictions for dataset
tow2 <- tow2 %>%
  # log transform estimates
  mutate_at(vars(contains("mean_"), contains("yhat_")), ~log(.)) %>%
  save.pred.fn(y_string = "mean_s_tow2", x_string = reg_predictors) %>%
  save.pred.fn(y_string = "mean_s", x_string = reg_predictors) %>%
  save.pred.fn(y_string = "mean_uw", x_string = reg_predictors) %>%
  save.pred.fn(y_string = "yhat_s_tow2", x_string = reg_predictors) %>%
  save.pred.fn(y_string = "yhat_s", x_string = reg_predictors) %>%
  save.pred.fn(y_string = "yhat_uw", x_string = reg_predictors) 
  # transform back to native scale
  #mutate_at(vars(contains("mean_"), contains("yhat_")), ~exp(.)) 

rmse.digits. <- 0
#compare predictsion vs observations
LUR_mean_s_tow2_plot <- colo.plot(data.wide = tow2,
                    x.variable = "mean_s_tow2",
                    y.variable = "LUR_mean_s_tow2", rmse.digits = rmse.digits.)

LUR_mean_s_plot <- colo.plot(data.wide = tow2,
                    x.variable = "mean_s",
                    y.variable = "LUR_mean_s", rmse.digits = rmse.digits.)

LUR_mean_uw_plot <- colo.plot(data.wide = tow2,
                    x.variable = "mean_uw",
                    y.variable = "LUR_mean_uw", rmse.digits = rmse.digits.)

LUR_yhat_s_tow2_plot <- colo.plot(data.wide = tow2,
                    x.variable = "yhat_s_tow2",
                    y.variable = "LUR_yhat_s_tow2", rmse.digits = rmse.digits.)

LUR_yhat_s_plot <- colo.plot(data.wide = tow2,
                    x.variable = "yhat_s",
                    y.variable = "LUR_yhat_s", rmse.digits = rmse.digits.)

LUR_yhat_uw_plot <- colo.plot(data.wide = tow2,
                    x.variable = "yhat_uw",
                    y.variable = "LUR_yhat_uw", rmse.digits = rmse.digits.)

ggarrange(LUR_mean_s_tow2_plot,
          LUR_mean_s_plot ,
          LUR_mean_uw_plot,
          LUR_yhat_s_tow2_plot,
          LUR_yhat_s_plot,
          LUR_yhat_uw_plot,
           common.legend = T, legend = "bottom"
          ) %>%
  annotate_figure(fig.lab = paste0("Site UFP: Method estimates vs LUR predictions (in-sample)",
                                   "\nlog 10-month avg UFP ~", reg_predictors)
                  )

```

Identify sites that are underpredicted by LUR. 
 -all sites
  - using yhat_s_tow2

### --> edit & make annual_final for export 

```{r log-transform fn exp}
# log_transform_var <- function(all.data, removeOrig=FALSE)
# {
#   em.vars <- grep("^em_", colnames(all.data))
#   new.varnames <- c()
#   for (i in em.vars)
#   {
#     newcol.index <- 1 + length(colnames(all.data))
#     all.data[, newcol.index] <- log(all.data[, i] + 0.1)
#     colnames(all.data)[newcol.index] <- paste('log_', colnames(all.data)[i], sep='')
#   }
#   if (removeOrig) all.data <- all.data[, -em.vars]
#   return (all.data)
# }

```

```{r}
annual.copy <- annual
#annual <- annual.copy

high.quantile <- 0.95

#save lm predictions & residuals for dataset
annual <- annual %>%
  # log transform estimates
  mutate_at(vars(contains("mean_"), contains("yhat_")), ~log(.)) %>%
  #select(-c(mean_s, mean_s_tow2, mean_s_tow2_tod2,
            # yhat_s, yhat_s_tow2_tod2, 
            # yhat_m_day_hr
            #)
         #) %>%
  #select(-mean_s_tow2_tod2) %>%
  #drop stop_id 309, which has no geocovariates, using elev_elevation as example
  drop_na(elev_elevation) %>%
  save.pred.fn(y_string = "yhat_s_tow2", x_string = reg_predictors) %>%
  mutate(
    residual_lvl = ifelse(resid_yhat_s_tow2 >quantile(resid_yhat_s_tow2, high.quantile), "high",
                          ifelse(resid_yhat_s_tow2 < quantile(resid_yhat_s_tow2, (1-high.quantile)), "low", "normal")
                          )
    ) # %>%
  ## transform back to native scale
  #mutate_at(vars(contains("mean_"), contains("yhat_")), ~exp(.)) 
  
 #compare predictsion vs observations
colo.plot(data.wide = annual,
          x.variable = "yhat_s_tow2",
          y.variable = "LUR_yhat_s_tow2", 
          rmse.digits = 2,
          mytitle =paste0("Site UFP: Method estimates vs LUR predictions (in-sample)",
                                   "\n10-month avg log UFP ~", reg_predictors),
          col.by = "residual_lvl")

```

 
```{r}
# Lasso 
 
set.seed(1)
#create categorical dummy variables
df <- annual %>%
  select(resid_yhat_s_tow2,
         cov_names)  

x <- model.matrix(resid_yhat_s_tow2~., df)[,-1]
y <- as.numeric(annual$resid_yhat_s_tow2)# <0)

### --> Error: x and y not same length
#select lambda through CV
cv.out <- cv.glmnet(x = x, 
                    y = y, 
                    alpha=1, 
                    family= "gaussian"
                    #family= "binomial"
                    )

bestlam <- cv.out$lambda.min

lasso.m <- glmnet(x = x, 
                  y = y, 
                 alpha = 1, 
                 family= "gaussian",
                 #family= "binomial"
                 )

lasso.coef <- predict(lasso.m, 
                      type= "coefficients", 
                      s= bestlam)[1:(ncol(x)+1),]

#coefficients chosen by Lasso w/ bestlam that are not 0
lasso.vars <- lasso.coef[lasso.coef != 0] %>%
  as.data.frame() %>%
  rownames_to_column()

names(lasso.vars) <- c("cov", "coef")

lasso.vars <- lasso.vars %>%
  filter(cov != "(Intercept)") %>%
  arrange(desc(coef))

#intersect_a1_a1_s00500, lu_reservior_p00400, rlu_evergreen_p03000, rlu_woody_wetland_p01000, rlu_dev_med_p00750
  
############################################
#variables predictive of residuals <0
lasso.results <- annual %>%
  #scale geocovariate predictors
  mutate_at(vars(cov_names), ~scale(.)) %>%
  lm(formula(paste("resid_yhat_s_tow2", "~", paste(lasso.vars$cov, collapse = " + " ))), 
   data = .) %>% 
  summary() %>%
  broom::tidy() %>% 
  filter(p.value < 0.05,
         term != "(Intercept)"
         ) 

############################################
#plot most predictive covariates

#? m_to_truck, m_to_l_airp
# ? pop10_s15000

covs <- lasso.results$term

annual.scaled <- annual %>%
  select(site_id, yhat_s_tow2, LUR_yhat_s_tow2, resid_yhat_s_tow2, residual_lvl,
         covs) %>%
  mutate_at(vars(covs), ~scale(.)) %>%
  as.data.frame()

# #? delete
# annual.scaled %>%
#   ggplot(aes(x=resid_mean_uw, fill= residual_lvl)) + 
#   geom_histogram() + 
#   labs(title = paste0("LUR Residuals, \ncolored by low/high quantile: ", 1-high.quantile),
#        x = "residual")

p <- annual.scaled %>%
  ggplot(aes(x=yhat_s_tow2, 
             fill=residual_lvl)) + 
  geom_histogram() + 
  labs(title = "10-month site average (yhat_s_tow2), colored by LUR residual level",
       x = "Mean UW 10-month site concentration (pt/cm3) "
       )
    
p
 
#table of lasso-selected variables
lasso.results %>%
  kable(caption = "Lasso-selected covariates (centered and scaled) predictive of residuals \nonly showing p < 0.05") %>%
  kable_styling()

lasso.results %>%
  ggplot(aes(x=estimate, y = term)) + 
  geom_point() + 
  geom_vline(xintercept = 0, alpha=0.5, linetype= "dashed") + 
  labs(
    x = "coefficient estimate",
    y = "geocovariate",
    title = "Lasso-selected covariates (centered and scaled) predictive of residuals \nonly showing p < 0.05"
         
         )


#distribution of UFP values by lasso variables
annual.scaled %>%
  gather("cov", "value", covs) %>%
  ggplot(aes(x=value, fill=residual_lvl)) +
  geom_density(alpha=0.3) + 
  facet_wrap(~cov, scales = "free")

# ? delete? already have this up above
# annual.scaled %>%
#   ggplot(aes(x=mean_uw, y= LUR_mean_uw)) + 
#   geom_point(aes(
#     col = residual_lvl
#     # col = resid_mean_uw, size = resid_mean_uw>quantile(resid_mean_uw, high.quantile) 
#     #              | resid_mean_uw < quantile(resid_mean_uw, (1-high.quantile))
#                 )) +
#   #scale_color_distiller("residual", palette = "Spectral") +
#   geom_abline(intercept = 0, slope = 1) +
#   geom_smooth(method = "lm", aes(fill="LS")) + 
#     theme(legend.position = "bottom") + 
#   labs(size = paste0("high residual (", 1-high.quantile, ")"),
#        title = "Comparison of 10-month site averages: 'UW mean' vs LUR prediction"
#        )  

```

10-month yhat_s_tow2 estimates mapped

```{r}
# # #Export csv w/ site lat/long
# site_averages <- annual.scaled %>% 
#   left_join(unique(ufp[c("site_id", "site_lat", "site_long")])) %>%
#   select(contains("site"), everything()) %>%
#   rename(latitude = site_lat,
#          longitude = site_long)
# 
# write.csv(site_averages,
#           file =  file.path("Data", "Aim 2", "Mobile Monitoring", "site_averages.csv"),
#           row.names = F)

```

![UFP concentrations. darker red = higher concentration](../Write Up/1. Proposal/Aim 2. 2019 UFP/Images/yhat_s_tow2 site averages 191223.png)


```{r}
#boxplots of stop readings
ufp %>%
  left_join(annual.scaled[c("site_id", "residual_lvl", "yhat_s_tow2")]) %>%
  drop_na(residual_lvl) %>%
  ggplot(aes(x=site_id, y=ptrak)) + 
  geom_boxplot(aes()) +
  geom_point(aes(y=mean_uw, col="mean uw")) +
  scale_y_log10() +
  facet_wrap(~residual_lvl, labeller = "label_both", 
             scales="free_x",
             nrow=1) + 
  labs(y = "UFP Stop Conc (pt/cm3)",
       col = "")


 
```

## --> ? how to fit interction term & then plot to see if predictions are closer to the 1-1 line?

```{r}

```
















Select an estimate to use.

#### --> upate w/ selected annual estimate method

```{r}
annual_final <- site_locations %>%
  right_join(annual) %>%
  select(
    #site_id, lat/long
    names(site_locations), 
    # --> select method & add log-transformed
    yhat_s_tow2, #log_yhat_s_tow2,
    cov_names
    
  )

#saveRDS(annual_final, file.path("Data", "Aim 2", "Mobile Monitoring", "annual_ufp_and_geocov.rda"))

```

Boxplots of annual averages: 
* overall
* by geographic covariate

```{r}

```



notes:
* ? could subtract a constant from regression estimates if they are constantly higher
