---
title: "Aim 2"
author: "Magali Blanco"
date: ' `r Sys.Date()` '
output:
  html_document:
    number_sections: yes
    df_print: paged
    toc: yes
    toc_depth: '3'
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: 3
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, cache=T, cache.comments = F, message = F, warning = F, tidy.opts=list(width.cutoff=60), tidy=TRUE, fig.height = 8)  

# Clear workspace of all objects and unload all extra (non-base) packages
rm(list = ls(all = TRUE))
if (!is.null(sessionInfo()$otherPkgs)) {
  res <- suppressWarnings(
    lapply(paste('package:', names(sessionInfo()$otherPkgs), sep=""),
      detach, character.only=TRUE, unload=TRUE, force=TRUE))
}

pacman::p_load(dplyr, tidyverse, chron, knitr)  #chronn: is.holiday, is.weekend

#set plot theme
theme_set(theme_linedraw() + theme(legend.position="bottom")) 


```

```{r}

#load stop average data 
mm <- readRDS(file.path("Data", "MobileMonitoring", "mm_190806.rda"))
mm.wide <- readRDS(file.path("Data", "MobileMonitoring", "mm.wide_190806.rda"))

#fake, temp data from Amanda
temp <- read.csv(file.path( "Data", "Aim 2", "Temp Data", "dr0311_mobile_locations.txt"))
  
```

```{r simple avg}
# #estimate site-specific median pollutant concentrations up until now
# median.conc <- mm %>% 
#   #filter(instrument_id =="PMDISC_8") %>% 
#   group_by(site_id, site_lat, site_long, instrument_id) %>% 
#   rename(lat = site_lat, long = site_long) %>%
#   summarize(
#     median_conc = median(value)
#   )
# 
# ##estimates for specific pollutants from primary instruments
# ufp.conc <- median.conc %>%
#   filter(instrument_id == "PMDISC_8") %>%
#   rename(median_conc_ct_m3 = median_conc )
# 
# no2.conc <- median.conc %>%
#   filter(instrument_id == "NO2_2") %>%
#   rename(median_conc_ppb = median_conc )
# 
# # write.csv(ufp.conc, "ufp.conc.csv", row.names = F)
# # write.csv(no2.conc, "no2.conc.csv", row.names = F)

```

# --> trim data 
LS: "I think we trim before weighting and then do the weighting based on the data we have."

# --> Calculate Weighted Means 
LS: "Ultimately we want each DOW to have a weight of 1/7, each quarter to have a weight of Â¼, and each time of day to have the weight according to how we ultimately bin or otherwise model time of day"

```{r}
#season-weekend-day-time_of_day




```


```{r}
#table of obsevations over time before/after weighing (e.g., % samples during day before/after weighting)

```

 
