---
title: "Aim 2"
author: "Magali Blanco"
date: ' `r Sys.Date()` '
output:
  html_document:
    number_sections: yes
    df_print: paged
    toc: yes
    toc_depth: '3'
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: 3
editor_options:
  chunk_output_type: console
---

### --> in CleanUp code: drop particles < 100 pt/cm3? see distribution
### ---> ? drop BC lvls > 27k ng/m3 (1% of data) - MOVUP report

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, 
                      cache=T, cache.comments = F, 
                      message = F, warning = F, 
                      tidy.opts=list(width.cutoff=60), tidy=TRUE,
                      fig.height = 8, fig.width = 8
                      )  

# Clear workspace of all objects and unload all extra (non-base) packages
rm(list = ls(all = TRUE))
if (!is.null(sessionInfo()$otherPkgs)) {
  res <- suppressWarnings(
    lapply(paste('package:', names(sessionInfo()$otherPkgs), sep=""),
      detach, character.only=TRUE, unload=TRUE, force=TRUE))
}

pacman::p_load(dplyr, tidyverse, 
               knitr, kableExtra, 
               Hmisc, EnvStats, 
               #descriptive statistics
               qwraps2,  
               ggpubr, VCA,
               pls,
               geoR #UK
               )    
#Himsc: describe(); EnvStats: summaryFull(); ggpubr: ggarrange()

options(knitr.kable.NA = '')
set.seed(1)
source("A2.0.1_Var&Fns.R")

#read in data
annual0 <- readRDS(file.path("Data", "Aim 2", "Mobile Monitoring", "annual.rda")) %>%
  drop_na(m_to_a1)
geo.mm <- readRDS(file.path("Data", "Aim 2", "Geocovariates", "geo.mm.rda")) %>%
  select(site_id, contains("lambert"), latitude, longitude)

```


 
# UK

PLS to create summary features of geocovariates 

#### --> use geocovariate code to update geocovariates, then see if this is still an issue:
errors when variables aren't scaled:...In FUN(X[[i]], ...) : Scaling with (near) zero standard deviation

#### --> see code to ID if there is scaling 
# --> centering/scaling PLS: see “Regionalized national UK model using PLS regression sampson kaufman, 2013” for details 

        
```{r}
set.seed(1)

# 1. fit PLS using training set to ID # features to use
# 2. calculate RMSE using test set w/ selected # features
# 3. fit final model using all data

# loadings(), coef(pls.fit), scores(), ? loading.weights()

# 1. fit PLS using training set to ID # features to use
annual <- annual0 %>%
  select(#site_id,
         y = yhat_s_tow2, 
         m_to_a1:ncol(annual0)) %>%
  mutate(
    #log-transform UFP
    y = log(y)
    #train = sample(c(TRUE, FALSE), size = nrow(annual), replace = T)
    ) %>%
  drop_na()

# --> ? issue: PLS only works when variables aren't scaled
 pls1 <- plsr(y~., 
             data=annual,
             #scale=T,
             validation = "CV")

#summary(pls1)

```

```{r}
## plot to select no. of PLS features based on CV RMSE

cv.rmse <- RMSEP(pls1, estimate = "CV")
cv.df <- data.frame(PLS_Components = cv.rmse$comps,
                    CV_RMSE = as.numeric(cv.rmse$val))  

min.cv.comp <- cv.df$PLS_Components[cv.df$CV_RMSE == min(cv.df$CV_RMSE)]
min.cv.rmse <- cv.df$CV_RMSE[cv.df$CV_RMSE == min(cv.df$CV_RMSE)]

cv.df %>%
  ggplot(aes(x = PLS_Components, y = CV_RMSE)) + 
  geom_line() + 
  geom_point(aes(x= min.cv.comp, y = min.cv.rmse, col = "Min RMSE")) +
  labs(col = "") + 
  labs(
    title = "CV RMSE for PLS components",
    #subtitle = "using training data",
    x = "PLS Components",
    y = "CV RMSE"
    ) +
  annotate("text", -Inf, Inf, label = paste0("Min RMSE\n-Components: ", min.cv.comp, 
                                             "\n-RMSE: ", round(min.cv.rmse)), 
           hjust = 0, vjust = 1)

```

Extract features/scores

```{r}
# --> ? issue: PLS only works when variables aren't scaled

pls1 <- plsr(y~., data = annual, 
             ncomp = min.cv.comp
             #scale=T,
             )
#summary(pls1)

# extract scores for UK
pls1.scores <- scores(pls1) [,c(1:min.cv.comp)] %>%
  as.data.frame()

# take out spaces

# error 
#pls1.scores %>% rename(gsub(" ", "", .) )

names(pls1.scores) <- gsub(" ", "", names(pls1.scores))

```

Predicted vs Measured

```{r}
# # comparing predictions for the training data (in-sample)
plot(pls1, ncomp=min.cv.comp, asp=1, line=T, 
     main = paste0("PLS using", min.cv.comp, " components") )
 
```

Plot the loadings. 

### --> ? why does loadings.weights() and loadings() produce diff results?

### --> only keep absolute elevation? 
### --> relabel covariates in plot

```{r}
pls.loadings <- pls1$loadings[] %>%
  as.data.frame() %>%
  rownames_to_column(var = "cov")

# rename variables if buffers
pls.loadings <- pls.loadings %>%
  mutate(
    buffer = substr(cov, nchar(cov)-4, nchar(cov)),
    buffer = as.numeric(ifelse(!is.na(as.integer(buffer)), buffer, NA)),
    cov2 = ifelse(is.na(buffer), cov, substr(cov, 1, nchar(cov)-5) )
    ) %>%
  select(contains("cov"), buffer, everything())

my.alpha=0.3
smallest.buffer <- min(pls.loadings$buffer, na.rm = T)
largest.buffer <- max(pls.loadings$buffer, na.rm = T)

pls.loadings.l  <- pls.loadings %>%
  #make long format for faceting 
  gather(key = "Component", value = "Loading", contains("Comp")) %>%
  mutate(Component = as.numeric(substr(Component, 6, nchar(Component)))
         ) %>%
  #only show first 3 components for now
  filter(Component <=3)   

pls.loadings.l  %>%
  #drop these in first points
  drop_na(buffer) %>%
  ggplot(aes(x = Loading, y = cov2)) + 
  geom_point(aes(size=buffer),
             shape=1,
             alpha=my.alpha) +  
  scale_size(breaks = c(smallest.buffer,  largest.buffer)) + #500, 5000, 10000,
  
  geom_point(data = pls.loadings.l[is.na(pls.loadings.l$buffer),], 
           alpha=my.alpha,
           aes(shape="")) + 
  geom_vline(xintercept=0, 
             linetype="solid", 
             alpha=my.alpha) +
    facet_wrap(~Component, scales = "free",
               labeller = "label_both") +
  labs(#x = paste0("Loading"),
       y = "Geocovariate", 
       shape= "non-buffer", #"proximity,\nelevation",
       title = "PLS Geocovariate Component Loadings")
 

```

Create a geodataset.

```{r}
pls.df <- cbind(site_id = annual0$site_id,
                y=annual$y, 
                pls1.scores) %>%
  left_join(geo.mm) #%>%

pls_geo <- as.geodata(pls.df, 
                      coords.col = 15:16, 
                      data.col = 2, 
                      covar.col = 3:(3+min.cv.comp-1), covar.names = names(pls.df)[3:(3+min.cv.comp-1)]
                      )

geo.s <- summary(pls_geo)
max.dist <- geo.s$distances.summary[["max"]]
 
plot(pls_geo)
#points(pls_geo, pt.divide = "quintile")

```

Model the error term   
* check that exponential function models geostatistical structure appropriately

### --> ? why does wls_ests=variofit(...model="exp") produce diff parameter estimates than likfit(pls_geo, ini=wls_ests)? what is likfit() doing??

-Empirical Variograms

```{r}
#trend
cov.trend <-  as.formula(paste0("~ ", paste0(names(pls1.scores), collapse = " + " )))

# cloud1 <- variog(pls_geo, max.dist=max.dist/2,
#                  option="cloud")
          
              #uvec = seq(0, max.dist/2, length=20))
#common practice - set max distance to half max dist
bin_variog <- variog(pls_geo, max.dist=max.dist*1/2,
                     #UK
                     trend =  cov.trend
                     )

plot(bin_variog)
 

# cloud1[c("u", "v")] %>% as.data.frame() %>%
#   ggplot(aes(x=u, y=v)) + 
#   geom_point(alpha=0.1) + 
#   geom_smooth() +
#   labs(x = "distance (m)",
#        y = "semivariance",
#        title = "Empirical variogram: cloud"
#        )
# 
# bin_variog[c("u", "v")] %>% as.data.frame() %>%
#   ggplot(aes(x=u, y=v)) + 
#   geom_point() + 
#   geom_line() +
#   labs(x = "distance (m)",
#        y = "semivariance",
#        title = "Empirical variogram: bin"
#        )

```

# --> ? error: flat semivariance 
# --> ? variofit() vs likefit() ?
# --> ? for other parametric fits, use wls_estimates for ini ??

estimating geostatistical model parameters using various estimation methods. 

```{r}
#The two basic options are likfit which uses maximum likelihood to do the estimation (with options for ML vs REML), and variofit which uses a parametric model fit using either ordinary or weighted least squares. 

#partial sill: sigma sq; range: phi, nugget: tau sq

# uses weighted least squares and an exponential variogram
wls_ests <- variofit(bin_variog, cov.model = "exp")
 
# try various estimatio methods
ml <- likfit(pls_geo, ini=wls_ests, cov.model = "exp",
             trend = cov.trend,
             lik.method = "ML") 
reml <- likfit(pls_geo, ini=wls_ests, cov.model = "exp",
               trend = cov.trend,
               lik.method="REML")
ols <- variofit(bin_variog, ini=wls_ests, cov.model = "exp", 
                weights="equal")
wls <- variofit(bin_variog, ini=wls_ests, cov.model = "exp",
                weights = "npairs")

# summary(ml)
# summary(reml)
# summary(ols)
# summary(wls)

#plot potential exponential fits
par(mfrow = c(1, 1))
plot(bin_variog, main = "Binned empirical (dots) and modeled (lines) variograms", )
lines(bin_variog, lty=1)
lines(ml, lty=2, col=2) 
lines(reml, lty=3, col=3) 
lines(ols, lty=4, col=4)
lines(wls, lty=5, col=5)
legend("bottomright", 
       legend = c("ML","REML","OLS","WLS"), 
       lty=c(2:5), col = c(2:5)
       )


```

use LOOCV to select variogram model

## --> ? why is worse looking model have better RMSE, R2?
## --> ? what R2 estimate is this? data-based...?

```{r}
xv.ml <- xvalid(pls_geo, model = ml)
xv.reml <- xvalid(pls_geo, model = reml)
xv.ols <- xvalid(pls_geo, model = ols)
xv.wls <- xvalid(pls_geo, model = wls)

print("ML LOOCV")
par(mfrow = c(5, 2))
plot(xv.ml) #caption = "ML LOOCV")

print("REML LOOCV")
plot(xv.reml)

print("OLS LOOCV")
plot(xv.ols)

print("WLS LOOCV")
plot(xv.wls)


#calculate MSE and R2
mse.ml <- mean((xv.ml$data - xv.ml$predicted)^2)
mse.reml <- mean((xv.reml$data - xv.reml$predicted)^2)
mse.ols <- mean((xv.ols$data - xv.ols$predicted)^2)
mse.wls <- mean((xv.wls$data - xv.wls$predicted)^2)

r2.ml <- 1 - mse.ml/mean((xv.ml$data - mean(xv.ml$data))^2)
r2.reml <- 1 - mse.reml/mean((xv.reml$data - mean(xv.reml$data))^2)
r2.ols <- 1 - mse.ols/mean((xv.ols$data - mean(xv.ols$data))^2)
r2.wls <- 1 - mse.wls/mean((xv.wls$data - mean(xv.wls$data))^2)

loocv.results <- data.frame(
  method = c("ML", "REML", "OLS", "WLS"),
  RMSE = sqrt(c(mse.ml, mse.reml, mse.ols, mse.wls)),
  R2 = c(r2.ml, r2.reml, r2.ols, r2.wls)
  )

loocv.results %>% 
  kable(caption = "LOOCV RMSE and R2 for various model parameter estimation methods",
        digits = 4) %>%
  kable_styling()
  
  
```


UK: UFP ~ PLS + E
* ? use native or log-transformed UFP values? see distribution

### --> use subset=Train

```{r}
 


```

? CV RMSE, R2 ?

```{r}

```


Predict at participant homes

```{r}
 
```

prediction boxplots overall and by important TRAP indicators

```{r}

```


Cross-validation   
* exclude mobile monitoring observations at AQS sites

Out-of sample/CV Test error (RMSE):
* units are in pt/cm3

### --> why is tehre an NA in error estimate? 

```{r}
# pls.pred <- predict(pls1, annual[annual$train==FALSE,], ncomp = min.cv.comp)
# 
# (pls.pred - annual$y[annual$train==FALSE])^2 %>%
#   mean(na.rm=T) %>% sqrt() %>% round()

```

RMSE
```{r}

```

CV R2  -- see 556 notes

```{r}

```

plot of predicted vs measured

```{r}

```


[add GIS map of predictions]

```{r}

```


Amanda: "Compare distribution of PLS scores in monitoring vs cohort. See if combination of variables is unusual. Check predicted PLS values are not super large – suggests extrapolation. If have strange points, can look to see where they are on a map (e.g., a point is in middle of lake; or strange geographic area)."

```{r}

```


# Sensitivity Analyses

## Averaging of 1 sec data at each stop

??? Use averages

```{r}

```

## Annual averages 

Trim 10% observations

```{r}

```

Windosrize extreme stop readings 

```{r}
# #windosrized alternative 
# ufp <- ufp %>%
#   group_by(site_id) %>%
#   mutate(
#     ufp_wind = ifelse(ptrak_ct_cm3 > quantile(ptrak_ct_cm3, (1-trim_quantile), na.rm = T), quantile(ptrak_ct_cm3, (1-trim_quantile), na.rm = T), 
#                       ifelse(ptrak_ct_cm3 < quantile(ptrak_ct_cm3, (trim_quantile), na.rm = T), quantile(ptrak_ct_cm3, (trim_quantile), na.rm = T), 
#                              ptrak_ct_cm3)))


```

## UK Model

Change TRAP indicator variable transformations (native to log-transformed or vise versa)

```{r}


```

Use different annual averages

```{r}

```

? [time permitting] Normalize to the Roosevelt garage 
```{r}

```

