---
title: "Aim 2"
author: "Magali Blanco"
date: ' `r Sys.Date()` '
output:
  html_document:
    number_sections: yes
    df_print: paged
    toc: yes
    toc_depth: '3'
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: 3
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, 
                      cache=T, cache.comments = F, 
                      message = F, warning = F, 
                      tidy.opts=list(width.cutoff=60), tidy=TRUE,
                      fig.height = 8, fig.width = 8
                      )  

# Clear workspace of all objects and unload all extra (non-base) packages
rm(list = ls(all = TRUE))
if (!is.null(sessionInfo()$otherPkgs)) {
  res <- suppressWarnings(
    lapply(paste('package:', names(sessionInfo()$otherPkgs), sep=""),
      detach, character.only=TRUE, unload=TRUE, force=TRUE))
}

pacman::p_load(dplyr, tidyverse, 
               knitr, kableExtra, 
               Hmisc, EnvStats, 
               #descriptive statistics
               qwraps2,  
               ggpubr, VCA,
               pls,
               geoR, #UK
               #gstat #alternative for UK
               )    
#Himsc: describe(); EnvStats: summaryFull(); ggpubr: ggarrange()

options(knitr.kable.NA = '')
set.seed(1)
source("A2.0.1_Var&Fns.R")

#read in data
annual0 <- readRDS(file.path("Data", "Aim 2", "Mobile Monitoring", "annual.rda")) %>%
  drop_na(m_to_a1)
geo.mm <- readRDS(file.path("Data", "Aim 2", "Geocovariates", "geo.mm.rda")) %>%
  select(site_id, contains("lambert"), latitude, longitude)

```

# PLS 

Create summary features of geocovariates 

#### --> use geocovariate code to update geocovariates, then see if this is still an issue:
errors when variables aren't scaled:...In FUN(X[[i]], ...) : Scaling with (near) zero standard deviation

```{r}
set.seed(1)

# 1. fit PLS using training set to ID # features to use
# 2. calculate RMSE using test set w/ selected # features
# 3. fit final model using all data

# loadings(), coef(pls.fit), scores(), ? loading.weights()

# 1. fit PLS using training set to ID # features to use
annual <- annual0 %>%
  select(site_id,
         ufp = yhat_s_tow2, 
         m_to_a1:ncol(annual0)) %>%
  mutate(
    #log-transform UFP
    log_ufp = log(ufp)
    ) %>%
  drop_na()

train.index <- sample(c(TRUE, FALSE), size = nrow(annual), replace = T)
sites.train <- annual$site_id[train.index]
sites.test <- annual$site_id[!train.index]

annual.train <- annual %>% filter(train.index) %>%
  select(-site_id)
annual.test <- annual %>% filter(!train.index) %>%
  select(-site_id)

# --> ? issue: PLS only works when variables aren't scaled
 pls.train <- plsr(log_ufp~., 
             data=annual.train,
             #scale=T,
             validation = "CV")

#summary(pls.train)

```

```{r, d??}
# ## plot to select no. of PLS features based on CV RMSE
# RMSEP(pls.train, estimate = "CV",  )
# 
# cv.rmse <- RMSEP(pls.train, estimate = "CV")
# cv.df <- data.frame(PLS_Components = cv.rmse$comps,
#                     CV_RMSE = as.numeric(cv.rmse$val))  
# 
# min.cv.comp <- cv.df$PLS_Components[cv.df$CV_RMSE == min(cv.df$CV_RMSE)]
# min.cv.rmse <- cv.df$CV_RMSE[cv.df$CV_RMSE == min(cv.df$CV_RMSE)]
# 
# cv.df %>%
#   ggplot(aes(x = PLS_Components, y = CV_RMSE)) + 
#   geom_line() + 
#   geom_point(aes(x= min.cv.comp, y = min.cv.rmse, 
#                  col = "Min RMSE")) +
#   labs(col = "") + 
#   labs(
#     title = "CV RMSE for PLS components",
#     #subtitle = "using training data",
#     x = "PLS Components",
#     y = "CV RMSE"
#     ) +
#   annotate("text", -Inf, Inf, label = paste0("Min RMSE\n-Components: ", min.cv.comp, 
#                                              "\n-RMSE: ", round(min.cv.rmse, 2)), 
#            hjust = 0, vjust = 1)

```

Manually calculate test RMSE and R2 for various components.

```{r}
# --> ? issue: PLS only works when variables aren't scaled
n.comp <- 30

pls.comp.fit <- data.frame(
  pls_comp=seq(1, n.comp),
  RMSE = NA, 
  R2 = NA)

for (i in 1:n.comp) {
  pls.train <- plsr(log_ufp ~., 
             data = annual.train, 
             ncomp = i,
             #scale=T
             )

  #predict test set
  pls.pred <- predict(pls.train, annual.test, ncomp = i)
  
  pls.mse <- mean((pls.pred - annual.test$log_ufp)^2, na.rm=T)  
  pls.r2 <- 1 - pls.mse/mean((annual.test$log_ufp - mean(annual.test$log_ufp))^2)
  
  pls.comp.fit$RMSE[i] <- sqrt(pls.mse)
  pls.comp.fit$R2[i] <- pls.r2
  
  }

min.rmse <- min(pls.comp.fit$RMSE)  
min.comp <- pls.comp.fit$pls_comp[pls.comp.fit$RMSE == min.rmse]
r2.at.min.rmse <- pls.comp.fit$R2[pls.comp.fit$RMSE == min.rmse]  

fit.info <- paste0("Min RMSE: ", round(min.rmse, 3),
                   "\nComponents: ", min.comp, 
                   "\nR2: ", round(r2.at.min.rmse, 2)
                                             )

pls.comp.fit %>%
  gather("eval", "value", RMSE:R2) %>%
  ggplot(aes(x=pls_comp, y = value, col = eval)) + 
  geom_point() + 
  geom_line() +
  # annotate("text", -Inf, Inf, label = fit.info, 
  #          hjust = 0, vjust = 1) + 
  geom_vline(xintercept = min.comp, 
             linetype = "dashed",
             alpha=0.5) +
  facet_wrap(~eval, scales = "free_y",
             ncol=1) +
  labs(x = "PLS components",
       title = "Test RMSE and R2 using various PLS Components",
       subtitle = fit.info,
       col = ""
       ) + 
  theme(legend.position = "none")
  
```

```{r}
pls.plots.list <- list()

#plot
# # comparing predictions for the training data (in-sample)
p <- plot(pls.train, ncomp = i, asp=1, line=T, 
     main = paste0("PLS using ", i, " components") )

```


Fit PLS using entire dataset.

```{r}
pls1 <- plsr(log_ufp ~., 
             data = annual, 
             ncomp = min.cv.comp,
             #scale=T
             )
```

Extract composite scores

```{r}
# extract scores for UK
pls1.scores <- scores(pls1) [,c(1:min.cv.comp)] %>%
  as.data.frame()

# take out spaces in names
names(pls1.scores) <- gsub(" ", "", names(pls1.scores))

```

Predicted vs Measured

```{r}
# # comparing predictions for the training data (in-sample)
plot(pls1, ncomp = min.cv.comp, asp=1, line=T, 
     main = paste0("PLS using ", min.cv.comp, " components") )
 
```

Plot the loadings. 

### --> ? why does loadings.weights() and loadings() produce diff results?

### --> only keep absolute elevation? 
### --> relabel covariates in plot

```{r}
pls.loadings <- pls1$loadings[] %>%
  as.data.frame() %>%
  rownames_to_column(var = "cov")

# rename variables if buffers
pls.loadings <- pls.loadings %>%
  mutate(
    buffer = substr(cov, nchar(cov)-4, nchar(cov)),
    buffer = as.numeric(ifelse(!is.na(as.integer(buffer)), buffer, NA)),
    cov2 = ifelse(is.na(buffer), cov, substr(cov, 1, nchar(cov)-5) )
    ) %>%
  select(contains("cov"), buffer, everything())

my.alpha=0.3

pls.loadings.l  <- pls.loadings %>%
  #make long format for faceting 
  gather(key = "Component", value = "Loading", contains("Comp")) %>%
  mutate(Component = as.numeric(substr(Component, 6, nchar(Component)))
         ) %>%
  #only show first few components for now
  filter(Component <= 3)   

pls.loadings.l  %>%
  #drop these in first points
  drop_na(buffer) %>%
  ggplot(aes(x = Loading, y = cov2)) + 
  geom_point(aes(size=buffer),
             shape=1,
             alpha=my.alpha) +  
  scale_size(breaks = c(min(pls.loadings$buffer, na.rm = T),  
                        max(pls.loadings$buffer, na.rm = T)
                        )
             ) + #500, 5000, 10000,
  
  geom_point(data = pls.loadings.l[is.na(pls.loadings.l$buffer),], 
           alpha=my.alpha,
           aes(shape="")) + 
  geom_vline(xintercept=0, 
             linetype="solid", 
             alpha=my.alpha) +
    facet_wrap(~Component,
               labeller = "label_both",
               ) +
  labs(#x = paste0("Loading"),
       y = "Geocovariate", 
       shape= "non-buffer", #"proximity,\nelevation",
       title = "PLS Geocovariate Component Loadings") + 
  theme(legend.position = "bottom")
 

```

Create geodatasets.  

```{r}
pls.df.all <- cbind(site_id = annual$site_id,
                log_ufp=annual$log_ufp, 
                pls1.scores
                ) %>%
  left_join(geo.mm) 

pls.df <- pls.df.all %>%
  #drop AQS sites from primary dataset
  filter(!grepl("MC", site_id))

pls.df.aqs <- pls.df.all %>%
  #drop AQS sites (use for out-of-sample validation later)
  filter(grepl("MC", site_id))


pls.df.train <- pls.df %>% 
  filter(train.index)
pls.df.test <- pls.df %>% 
  filter(!train.index)

#training data (? need??)
pls_geo_train <- as.geodata(pls.df.train, 
                      coords.col = c("lambert_x", "lambert_y"), 
                      data.col = "log_ufp", 
                      covar.col = names(pls1.scores))
#stops 
pls_geo <- as.geodata(pls.df, 
                      coords.col = c("lambert_x", "lambert_y"), 
                      data.col = "log_ufp", 
                      covar.col = names(pls1.scores))
#AQS sites
aqs_geo <- as.geodata(pls.df.aqs, 
                      coords.col = c("lambert_x", "lambert_y"), 
                      data.col = "log_ufp", 
                      covar.col = names(pls1.scores))

geo.s <- summary(pls_geo_train)
max.dist <- geo.s$distances.summary[["max"]]
 
plot(pls_geo_train)
#points(pls_geo_train, pt.divide = "quintile")

```

Model the error term   
* check that exponential function models geostatistical structure appropriately

### --> variogram reaches sill (flattens out) very quickly

```{r}
#Empirical Variogram

#trend
cov.trend <-  as.formula(paste0("~ ", paste0(names(pls1.scores), collapse = " + " )))
max.plot.dist <- max.dist*.1

variog_train <- variog(pls_geo_train, 
                     #common practice - set max distance to half max dist
                     max.dist=max.plot.dist,
                     #UK
                     trend =  cov.trend)

#plot(variog_train)


# cloud1 <- variog(pls_geo_train, max.dist=max.plot.dist,
#                  option="cloud")
              #uvec = seq(0, max.dist/2, length=20))
#plot(cloud1)

```

estimating geostatistical model parameters using various estimation methods. 

```{r, results='hide'}
# partial sill: sigma sq
# range: phi, 
# nugget: tau sq

#estimate intitial range & sill values - use the variofit function with no initial values and let geoR try to estimate these. usin weighted least squares and an exponential fit
wls_ests_train <- variofit(variog_train, cov.model = "exp")
 
# try various estimatio methods. The two basic options are likfit which uses maximum likelihood to do the estimation (with options for ML vs REML), and variofit which uses a parametric model fit using either ordinary or weighted least squares. 

ml_train <- likfit(pls_geo_train, ini=wls_ests_train, cov.model = "exp",
             trend = cov.trend,
             lik.method = "ML") 
reml_train <- likfit(pls_geo_train, ini=wls_ests_train, cov.model = "exp",
               trend = cov.trend,
               lik.method="REML")
ols_train <- variofit(variog_train, ini=wls_ests_train, cov.model = "exp", 
                weights="equal")
wls_train <- variofit(variog_train, ini=wls_ests_train, cov.model = "exp",
                weights = "npairs")

# summary(ml_train)
# summary(reml_train)
# summary(ols_train)
# summary(wls_train)

```

```{r}
#plot potential exponential fits
par(mfrow = c(1, 1))
plot(variog_train, 
     main = "Binned empirical and modeled variograms using training data", 
     xlab = "Distance (m)" )
lines(variog_train, lty=1)
lines(ml_train, lty=2, col=2) 
lines(reml_train, lty=3, col=3) 
lines(ols_train, lty=4, col=4)
lines(wls_train, lty=5, col=5)
legend("bottomright", 
       legend = c("Binned Empirical", "ML Model","REML Model","OLS Model","WLS Model"), 
       lty=c(1:5), col = c(1:5),
       cex = 0.7
       )

```

use LOOCV to select variogram model

```{r, results='hide'}
xv.ml <- xvalid(pls_geo_train, model = ml_train)
xv.reml <- xvalid(pls_geo_train, model = reml_train)
xv.ols <- xvalid(pls_geo_train, model = ols_train)
xv.wls <- xvalid(pls_geo_train, model = wls_train)

```

### --> Knitr error - plotting. "plot.window(xlim, ylim, ...) need finite "xlim" values...

```{r, eval=F}
# ????
# max.dist = max.dist

print("ML LOOCV")
par(mfrow = c(5, 2))
plot(xv.ml) 

print("REML LOOCV")
#par(mfrow = c(3, 4))
plot(xv.reml)

print("OLS LOOCV")
#par(mfrow = c(3, 4))
plot(xv.ols)

print("WLS LOOCV")
#par(mfrow = c(3, 4))
plot(xv.wls)

```

## --> ? why is worse looking model (REML) have y slighbetter RMSE, R2 than better looking model (OLS)?

```{r}
#calculate MSE and R2 (R2 = 1 – MSE / Var(Y))
mse.ml <- mean((xv.ml$data - xv.ml$predicted)^2)
mse.reml <- mean((xv.reml$data - xv.reml$predicted)^2)
mse.ols <- mean((xv.ols$data - xv.ols$predicted)^2)
mse.wls <- mean((xv.wls$data - xv.wls$predicted)^2)

r2.ml <- 1 - mse.ml/mean((xv.ml$data - mean(xv.ml$data))^2)
r2.reml <- 1 - mse.reml/mean((xv.reml$data - mean(xv.reml$data))^2)
r2.ols <- 1 - mse.ols/mean((xv.ols$data - mean(xv.ols$data))^2)
r2.wls <- 1 - mse.wls/mean((xv.wls$data - mean(xv.wls$data))^2)

data.frame(
  method = c("ML", "REML", "OLS", "WLS"),
  RMSE = sqrt(c(mse.ml, mse.reml, mse.ols, mse.wls)),
  R2 = c(r2.ml, r2.reml, r2.ols, r2.wls)
  ) %>% 
  kable(caption = paste0("LOOCV RMSE and R2 for various model parameter estimation methods. Using training data (n = ", length(train.index[train.index==T]), " site annual averages)" ),
        digits = 3) %>%
  kable_styling() %>%
  add_footnote(c("R2 = 1 – MSE / Var(Y)"))
  
```

### --> ?? need to use training dataset above this if use xvald() ??

Select error model and fit final model to entire dataset.

```{r}

max.dist <- summary(pls_geo)$distances.summary[["max"]]
max.plot.dist <- max.dist*.1

plot(pls_geo)

variog <- variog(pls_geo,
                     max.dist = max.plot.dist,
                     trend =  cov.trend)

#estimate intitial range & sill values
wls_ests <- variofit(variog, cov.model = "exp")

#use selected error model (from training data above)
reml <- likfit(pls_geo, ini=wls_ests, cov.model = "exp",
               trend = cov.trend,
               lik.method="REML")

```

Table of variogram parameters: nugget, partial sill, range (m).

Selecting REML method (for now) b/c it has the lowest RMSE, highest R2.

```{r}
# partial sill: sigma sq
# range: phi, 
# nugget: tau sq

reml.partial.sill <- reml$sigmasq
reml.range <- reml$phi
reml.nugget <- reml$tausq

data.frame(
  method = c("REML"),
  partial_sill = c(reml.partial.sill),
  range_m = c(reml.range),
  nugget = c(reml.nugget)
) %>%
  kable(caption = "Variogram parameters",
        digits = 3
        ) %>%
  kable_styling()
  
```

Model Validation
- CV RMSE, R2 (all mobile monitoring sites, excluding AQS sites)

```{r, results="hide"}
xv.reml <- xvalid(pls_geo, model = reml)

```

```{r}
print("REML LOOCV")
par(mfrow = c(5, 2))
plot(xv.reml) 

mse.reml <- mean((xv.reml$data - xv.reml$predicted)^2)
r2.reml <- 1 - mse.reml/mean((xv.reml$data - mean(xv.reml$data))^2)

```

- Out-of-sample RMSE, R2 (AQS sites)

### --> predict at these locations & calculate RMSE, R2 / ? how to do out of sample validation

"Error in trend.spatial(trend = krige$trend.l, geodata = list(coords = locations)): trend elements not found"

```{r, results="hide"}
# xv.reml.aqs <- xvalid(pls_geo, model = reml,
#                       data.xvalid = aqs_geo$data,
#                       locations.xvalid = aqs_geo$coords, 
#                       trend = trend.spatial(trend = cov.trend, aqs_geo)
#                       )
# summary(xv.reml.aqs)



```

```{r}
# print("REML LOOCV")
# par(mfrow = c(5, 2))
# plot(xv.reml) 
# 
# mse.reml <- mean((xv.reml$data - xv.reml$predicted)^2)
# r2.reml <- 1 - mse.reml/mean((xv.reml$data - mean(xv.reml$data))^2)

```


plot of predicted vs measured

```{r}


```

use UK to predict at participant homes 
### --> "step 3"
### --> load participant geocovariates 

```{r}


```

prediction boxplots overall and by important TRAP indicators

```{r}

```


Map predictions

```{r}

```


Amanda: "Compare distribution of PLS scores in monitoring vs cohort. See if combination of variables is unusual. Check predicted PLS values are not super large – suggests extrapolation. If have strange points, can look to see where they are on a map (e.g., a point is in middle of lake; or strange geographic area)."

```{r}

```


# Sensitivity Analyses

## Averaging of 1 sec data at each stop

??? Use averages

```{r}

```

## Annual averages 

Trim 10% observations

```{r}

```

Windosrize extreme stop readings 

```{r}
# #windosrized alternative 
# ufp <- ufp %>%
#   group_by(site_id) %>%
#   mutate(
#     ufp_wind = ifelse(ptrak_ct_cm3 > quantile(ptrak_ct_cm3, (1-trim_quantile), na.rm = T), quantile(ptrak_ct_cm3, (1-trim_quantile), na.rm = T), 
#                       ifelse(ptrak_ct_cm3 < quantile(ptrak_ct_cm3, (trim_quantile), na.rm = T), quantile(ptrak_ct_cm3, (trim_quantile), na.rm = T), 
#                              ptrak_ct_cm3)))


```

## UK Model

Change TRAP indicator variable transformations (native to log-transformed or vise versa)

```{r}


```

Use different annual averages

```{r}

```

? [time permitting] Normalize to the Roosevelt garage 
```{r}

```

