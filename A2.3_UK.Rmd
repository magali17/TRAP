---
title: "Aim 2: UK"
author: "Magali Blanco"
date: ' `r Sys.Date()` '
output:
  html_document:
    number_sections: yes
    df_print: paged
    toc: yes
    toc_depth: '3'
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: 3
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, 
                      cache=T, cache.comments = F, 
                      message = F, warning = F, 
                      tidy.opts=list(width.cutoff=60), tidy=TRUE,
                      fig.height = 8, fig.width = 8
                      )  

# Clear workspace of all objects and unload all extra (non-base) packages
rm(list = ls(all = TRUE))
if (!is.null(sessionInfo()$otherPkgs)) {
  res <- suppressWarnings(
    lapply(paste('package:', names(sessionInfo()$otherPkgs), sep=""),
      detach, character.only=TRUE, unload=TRUE, force=TRUE))
}

pacman::p_load(dplyr, tidyverse, 
               knitr, kableExtra, 
               Hmisc, EnvStats, 
               #descriptive statistics
               qwraps2,  
               ggpubr, VCA,
               pls,
               geoR, #UK
               #gstat #alternative for UK
               )    
#Himsc: describe(); EnvStats: summaryFull(); ggpubr: ggarrange()

options(knitr.kable.NA = '')
set.seed(1)
source("A2.0.1_Var&Fns.R")

#read in data
annual0 <- readRDS(file.path("Data", "Aim 2", "Mobile Monitoring", "annual.rda")) %>%
  drop_na(m_to_a1)
geo.mm <- readRDS(file.path("Data", "Aim 2", "Geocovariates", "geo.mm.rda")) %>%
  select(site_id, contains("lambert"), latitude, longitude)
geo.act <- read.csv(file.path("Data", "Aim 2", "ACT", "ACT_selected.csv"))  

```

# PLS to create summary features of geocovariates. 

#### --> use geocovariate code to update geocovariates, 
### --> issue: PLS only works when variables aren't scaled
  "errors when variables aren't scaled:...In FUN(X[[i]], ...) : Scaling with (near) zero standard deviation"

Create training,  test and AQS set.

```{r}
# loadings(), coef(pls.fit), scores(), ? loading.weights()

# 1. fit PLS using training set to ID # features to use
annual.all <- annual0 %>%
  select(site_id,
         ufp = yhat_s_tow2, 
         m_to_a1:ncol(annual0)) %>%
  drop_na() %>%
  mutate(
    #log-transform UFP
    log_ufp = log(ufp),
      #create test, train, aqs data set
    set = ifelse(grepl("MC", site_id), "aqs",
                 sample(c("train", "test"), size = sum(grepl("MS", site_id)), replace = T)
                 )
         )

# save site_ids for later
train.site_id <- annual.all$site_id[annual.all$set== "train"]
test.site_id <- annual.all$site_id[annual.all$set== "test"]
aqs.site_ids <- annual.all$site_id[annual.all$set== "aqs"]

annual <- annual.all %>%
  #drop aqs sites
  filter(set %in% c("train", "test"))  

annual.train <- annual.all %>%
  filter(set == "train")  

annual.test <- annual.all %>%
  filter(set == "test")  

annual.aqs <- annual.all %>%
  #only keep aqs sites
  filter(set == "aqs")  
 
```

fit PLS with training data 

```{r}
# --> ? issue: PLS only works when variables aren't scaled
pls.train <- plsr(log_ufp ~.,
             data=annual.train[,!(names(annual.train) %in% c('site_id','set'))],
             #scale=T,
             validation = "CV")

#summary(pls.train)
#plot(pls.train)

```

Calculate test RMSE and R2 for training models with a different number of components.

### --> fn for 10FCV instead of split set (test/train)

```{r}
n.comp <- 30

pls.comp.fit <- data.frame(
  pls_comp=seq(1, n.comp),
  RMSE = NA, 
  R2 = NA)

for (i in 1:n.comp) {
  pls.train <- plsr(log_ufp ~., 
             data = annual.train[,!(names(annual.train) %in% c('site_id','set'))], 
             ncomp = i,
             #scale=T
             )

  #predict test set using training model
  pls.pred <- predict(pls.train, annual.test, ncomp = i)
  
  pls.mse <- mean((pls.pred - annual.test$log_ufp)^2, na.rm=T)  
  pls.r2 <- 1 - pls.mse/mean((annual.test$log_ufp - mean(annual.test$log_ufp))^2)
  
  pls.comp.fit$RMSE[i] <- sqrt(pls.mse)
  pls.comp.fit$R2[i] <- pls.r2
  
  }

min.rmse <- min(pls.comp.fit$RMSE)  
min.comp <- pls.comp.fit$pls_comp[pls.comp.fit$RMSE == min.rmse]
r2.at.min.rmse <- pls.comp.fit$R2[pls.comp.fit$RMSE == min.rmse]  

fit.info <- paste0("Min RMSE: ", round(min.rmse, 3),
                   "\nComponents: ", min.comp, 
                   "\nR2: ", round(r2.at.min.rmse, 2)
                   )

pls.comp.fit %>%
  gather("eval", "value", RMSE:R2) %>%
  ggplot(aes(x=pls_comp, y = value, col = eval)) + 
  geom_point() + 
  geom_line() +
  geom_vline(xintercept = min.comp, 
             linetype = "dashed",
             alpha=0.5) +
  facet_wrap(~eval, scales = "free_y",
             ncol=1) +
  labs(x = "PLS components",
       title = "Test RMSE and R2 using various PLS Components",
       subtitle = fit.info,
       col = ""
       ) + 
  theme(legend.position = "none")
  
```

Plot of out-of-sample predictions at other stops (using stop test set).

### --> ? fn to calculate 10FCV predictions? 

```{r}
no.pls.comp <- min.comp #12 

#predict at out-of-sample stop sites, using selected number of components 
annual.test$pred_log_ufp <- predict(pls.train, annual.test, ncomp = no.pls.comp) %>% as.vector()

colo.plot(data.wide = annual.test, 
            x.variable = "log_ufp", 
            x.label = "Measured log UFP (log pt/cm3)",
            y.variable = "pred_log_ufp", 
            y.label = "Predicted log UFP (log pt/cm3)",
            rmse.digits = 2, 
            mytitle = paste0("Out of sample measured vs predicted UFP concentrations at stops", 
                             "\nnumber of PLS components: ", 
                             no.pls.comp
                             )
            )
```

Plot of out-of-sample predictions at co-located AQS sites (using AQS test set).

```{r}

#predict at out-of-sample co-location AQS sites 
annual.aqs$pred_log_ufp <- predict(pls.train, annual.aqs, ncomp = no.pls.comp) %>% as.vector()

lm1 <- lm(formula(paste("pred_log_ufp", "~", "log_ufp")), data = annual.aqs)
           
#rmse
rmse <- (annual.aqs$pred_log_ufp - annual.aqs$log_ufp)^2 %>%
 mean() %>% sqrt() %>%
 round(digits = 2)

fit.info <- paste0("y = ", round(coef(lm1)[1], 2), " + ", round(coef(lm1)[2], 2), 
                  "x \nR2 = ", round(summary(lm1)$r.squared, 2), 
                  "\nRMSE = ", rmse,
                  "\nno. pairs = ", nrow(annual.aqs))
#compare  
annual.aqs %>%
 ggplot(aes(x= log_ufp, y= pred_log_ufp)) + 
 geom_point(alpha=0.3) + 
 geom_abline(intercept = 0, slope = 1) +
 #geom_smooth(aes(fill="loess")) + 
 #geom_smooth(method = "lm", aes(fill="LS")) + 
 labs(title = paste0("Out of sample measured vs predicted UFP \nconcentration at AQS co-location sites"),
      subtitle = paste0("\nnumber of PLS components: ", 
                 no.pls.comp),
      x = "Measured log UFP (log pt/cm3)",
      y = "Predicted log UFP (log pt/cm3)"
      ) +
 annotate("text", -Inf, Inf, label = fit.info, hjust = 0, vjust = 1) + 
     geom_smooth(method = "lm") 

           
```

Fit PLS using all data and selected PLS components.    


```{r}
pls1 <- plsr(log_ufp ~., 
             #annual.all: aqs, train & test; annual: train & test
             data = annual.all[ ,!names(annual.all) %in% c("site_id", "set")], #annual, 
             ncomp = no.pls.comp,
             #scale=T
             )


```

Extract composite scores

```{r}
# extract scores for UK
pls1.scores <- scores(pls1) [,c(1:no.pls.comp)] %>%
  as.data.frame()
#same as above
#predict(pls1, type = "score", ncomp = 30)


# take out spaces in names
names(pls1.scores) <- gsub(" ", "", names(pls1.scores))

#wrong:
#scores(pls1, data=annual.aqs)

```

 
Plot the loadings. 

### --> ? why does loadings.weights() and loadings() produce diff results?

### --> only keep absolute elevation? 
### --> relabel covariates in plot

```{r}
pls.loadings <- pls1$loadings[] %>%
  as.data.frame() %>%
  rownames_to_column(var = "cov")

# rename variables if buffers
pls.loadings <- pls.loadings %>%
  mutate(
    buffer = substr(cov, nchar(cov)-4, nchar(cov)),
    buffer = as.numeric(ifelse(!is.na(as.integer(buffer)), buffer, NA)),
    cov2 = ifelse(is.na(buffer), cov, substr(cov, 1, nchar(cov)-5) )
    ) %>%
  select(contains("cov"), buffer, everything())

my.alpha=0.3

pls.loadings.l  <- pls.loadings %>%
  #make long format for faceting 
  gather(key = "Component", value = "Loading", contains("Comp")) %>%
  mutate(Component = as.numeric(substr(Component, 6, nchar(Component)))
         ) %>%
  #only show first few components for now
  filter(Component <= 3)   

pls.loadings.l  %>%
  #drop these in first points
  drop_na(buffer) %>%
  ggplot(aes(x = Loading, y = cov2)) + 
  geom_point(aes(size=buffer),
             shape=1,
             alpha=my.alpha) +  
  scale_size(breaks = c(min(pls.loadings$buffer, na.rm = T),  
                        max(pls.loadings$buffer, na.rm = T)
                        )
             ) + #500, 5000, 10000,
  
  geom_point(data = pls.loadings.l[is.na(pls.loadings.l$buffer),], 
           alpha=my.alpha,
           aes(shape="")) + 
  geom_vline(xintercept=0, 
             linetype="solid", 
             alpha=my.alpha) +
    facet_wrap(~Component,
               labeller = "label_both",
               ) +
  labs(#x = paste0("Loading"),
       y = "Geocovariate", 
       shape= "non-buffer", #"proximity,\nelevation",
       title = "PLS Geocovariate Component Loadings") + 
  theme(legend.position = "bottom")
 

```

# Create geodatasets  

```{r}
pls.all <- cbind(
  annual.all[c("site_id", "log_ufp")],
  pls1.scores
) %>%
  left_join(geo.mm) 

pls.train.test <- pls.all %>%
  filter(annual.all$set %in% c("train", "test"))
pls.test <- pls.all %>%
  filter(annual.all$set == "test")
pls.train <- pls.all %>%
  filter(annual.all$set == "train")

pls.aqs <- pls.all %>%
  filter(annual.all$set == "aqs")

#training data (? need??)
geo_train <- as.geodata(pls.train, 
                      coords.col = c("lambert_x", "lambert_y"), 
                      data.col = "log_ufp", 
                      covar.col = names(pls1.scores))
#stops 
geo_train_test <- as.geodata(pls.train.test, 
                      coords.col = c("lambert_x", "lambert_y"), 
                      data.col = "log_ufp", 
                      covar.col = names(pls1.scores))
#AQS sites
geo_aqs <- as.geodata(pls.aqs, 
                      coords.col = c("lambert_x", "lambert_y"), 
                      data.col = "log_ufp", 
                      covar.col = names(pls1.scores))

geo.s <- summary(geo_train)
max.dist <- geo.s$distances.summary[["max"]]

```

Training data 

```{r}
plot(geo_train)
#points(geo_train, pt.divide = "quintile")

```

Model the error term   
* check that exponential function models geostatistical structure appropriately

### --> variogram reaches sill (flattens out) very quickly

```{r}
#Empirical Variogram

#trend
cov.trend <-  as.formula(paste0("~ ", paste0(names(pls1.scores), collapse = " + " )))
max.plot.dist <- max.dist*.1

variog_train <- variog(geo_train, 
                     #common practice - set max distance to half max dist
                     max.dist=max.plot.dist,
                     #UK
                     trend =  cov.trend)

#plot(variog_train)


# cloud1 <- variog(geo_train, max.dist=max.plot.dist,
#                  option="cloud")
              #uvec = seq(0, max.dist/2, length=20))
#plot(cloud1)

```

estimating geostatistical model parameters using various estimation methods. 

```{r, results='hide'}
# partial sill: sigma sq
# range: phi, 
# nugget: tau sq

#estimate intitial range & sill values - use the variofit function with no initial values and let geoR try to estimate these. usin weighted least squares and an exponential fit
wls_ests_train <- variofit(variog_train, cov.model = "exp")
 
# try various estimatio methods. The two basic options are likfit which uses maximum likelihood to do the estimation (with options for ML vs REML), and variofit which uses a parametric model fit using either ordinary or weighted least squares. 

ml_train <- likfit(geo_train, ini=wls_ests_train, cov.model = "exp",
             trend = cov.trend,
             lik.method = "ML") 
reml_train <- likfit(geo_train, ini=wls_ests_train, cov.model = "exp",
               trend = cov.trend,
               lik.method="REML")
ols_train <- variofit(variog_train, ini=wls_ests_train, cov.model = "exp", 
                weights="equal")
wls_train <- variofit(variog_train, ini=wls_ests_train, cov.model = "exp",
                weights = "npairs")

# summary(ml_train)
# summary(reml_train)
# summary(ols_train)
# summary(wls_train)

```

```{r}
#plot potential exponential fits
par(mfrow = c(1, 1))
plot(variog_train, 
     main = "Binned empirical and modeled variograms using training data", 
     xlab = "Distance (m)" )
lines(variog_train, lty=1)
lines(ml_train, lty=2, col=2) 
lines(reml_train, lty=3, col=3) 
lines(ols_train, lty=4, col=4)
lines(wls_train, lty=5, col=5)
legend("bottomright", 
       legend = c("Binned Empirical", "ML Model","REML Model","OLS Model","WLS Model"), 
       lty=c(1:5), col = c(1:5),
       cex = 0.7
       )

```

use LOOCV to select variogram model

```{r, results='hide'}
xv.ml <- xvalid(geo_train, model = ml_train)
xv.reml <- xvalid(geo_train, model = reml_train)
xv.ols <- xvalid(geo_train, model = ols_train)
xv.wls <- xvalid(geo_train, model = wls_train)

```

```{r, ? delete.chunk?, eval=F}

# print("ML LOOCV")
# par(mfrow = c(5, 2))
# plot(xv.ml) 
# 
# print("REML LOOCV")
# #par(mfrow = c(3, 4))
# plot(xv.reml)
# 
# print("OLS LOOCV")
# #par(mfrow = c(3, 4))
# plot(xv.ols)
# 
# print("WLS LOOCV")
# #par(mfrow = c(3, 4))
# plot(xv.wls)

```

## --> ? why is worse looking model (REML) have y slighbetter RMSE, R2 than better looking model (OLS)?

```{r}
#calculate MSE and R2 (R2 = 1 – MSE / Var(Y))
mse.ml <- mean((xv.ml$data - xv.ml$predicted)^2)
mse.reml <- mean((xv.reml$data - xv.reml$predicted)^2)
mse.ols <- mean((xv.ols$data - xv.ols$predicted)^2)
mse.wls <- mean((xv.wls$data - xv.wls$predicted)^2)

r2.ml <- 1 - mse.ml/mean((xv.ml$data - mean(xv.ml$data))^2)
r2.reml <- 1 - mse.reml/mean((xv.reml$data - mean(xv.reml$data))^2)
r2.ols <- 1 - mse.ols/mean((xv.ols$data - mean(xv.ols$data))^2)
r2.wls <- 1 - mse.wls/mean((xv.wls$data - mean(xv.wls$data))^2)

data.frame(
  method = c("ML", "REML", "OLS", "WLS"),
  RMSE = sqrt(c(mse.ml, mse.reml, mse.ols, mse.wls)),
  R2 = c(r2.ml, r2.reml, r2.ols, r2.wls)
  ) %>% 
  kable(caption = paste0("LOOCV RMSE and R2 for various model parameter estimation methods. Using training data (n = ",  nrow(pls.train), " site annual averages)" ),
        digits = 3) %>%
  kable_styling() %>%
  add_footnote(c("R2 = 1 – MSE / Var(Y)"))
  
```

### --> ?? need to use training dataset above this if use xvald() ??

Fit final model to all stops (train & test set)

```{r}
plot(geo_train_test)

```

variogram with all stop data

```{r, results= "hide"}
max.dist <- summary(geo_train_test)$distances.summary[["max"]]
max.plot.dist <- max.dist*.1

variog <- variog(geo_train_test,
                     max.dist = max.plot.dist,
                     trend =  cov.trend)

#estimate intitial range & sill values
wls_ests <- variofit(variog, cov.model = "exp")

#use selected error model (from training data above)
reml <- likfit(geo_train_test, ini=wls_ests, cov.model = "exp",
               trend = cov.trend,
               lik.method="REML")

```

Table of variogram parameters: nugget, partial sill, range (m).

Selecting REML method (for now) b/c it has the lowest RMSE, highest R2.

```{r}
# partial sill: sigma sq
# range: phi, 
# nugget: tau sq

reml.partial.sill <- reml$sigmasq
reml.range <- reml$phi
reml.nugget <- reml$tausq

data.frame(
  method = c("REML"),
  partial_sill = c(reml.partial.sill),
  range_m = c(reml.range),
  nugget = c(reml.nugget)
) %>%
  kable(caption = paste0("Variogram parameters (n = ", nrow(pls.train.test), " sites)" ),
        digits = 3
        ) %>%
  kable_styling()
  
```

Model Validation
- LOOCV RMSE, R2 at all stops (excluding AQS sites)

```{r, results="hide"}
xv.reml <- xvalid(geo_train_test, model = reml)

```

```{r}
print("REML LOOCV")
par(mfrow = c(5, 2))
plot(xv.reml) 

mse.reml <- mean((xv.reml$data - xv.reml$predicted)^2)
r2.reml <- 1 - mse.reml/mean((xv.reml$data - mean(xv.reml$data))^2)

data.frame(
  no_sites = length(xv.reml$data),
  RMSE = sqrt(mse.reml),
  R2 = r2.reml
) %>%
  kable(caption = paste0("LOOCV using REML variogram parameters (n = ", nrow(pls.train.test), " sites)"), 
        digits = 2) %>%
  kable_styling()

```

- Out-of-sample RMSE, R2 at AQS sites

```{r}
# #error, doesn't work
# xvalid(geodata= geo_train_test, model = reml,
#        locations.xvalid = geo_aqs$coords,
#        data.xvalid = geo_aqs$data,
#        #trend = cov.trend
#        )

train_test_trend <- trend.spatial(trend = cov.trend, 
                                  geodata = geo_train_test)
aqs_trend <- trend.spatial(trend = cov.trend,
                           geodata = geo_aqs)

uk_aqs <- krige.conv(coords = geo_train_test$coords,
                            data = geo_train_test$data,
                            locations = geo_aqs$coords,
                            krige=krige.control(type = "ok",
                                                obj.model = reml,
                                                trend.d= train_test_trend,
                                                trend.l= aqs_trend))
#save UK predictions
geo_aqs$pred  <- uk_aqs$predict
 
#calculate out-of-sample RMSE and R2
mse.oos.aqs <- mean((geo_aqs$data - geo_aqs$pred)^2)
r2.oos.aqs <- 1 - mse.oos.aqs/mean((geo_aqs$data - mean(geo_aqs$data))^2)

data.frame(
  RMSE = sqrt(mse.oos.aqs),
  R2 = r2.oos.aqs
) %>%
  kable(caption = paste0("Out-of sample RMSE and R2 for AQS sites using UK model fit to stops (n = ", length(geo_aqs$data), " AQS sites)"), 
        digits = 2) %>%
  kable_styling()

```

```{r}
# print("REML LOOCV")
# par(mfrow = c(5, 2))
# plot(xv.reml) 
# 
# mse.reml <- mean((xv.reml$data - xv.reml$predicted)^2)
# r2.reml <- 1 - mse.reml/mean((xv.reml$data - mean(xv.reml$data))^2)

```


plot of predicted vs measured

```{r}


```

use UK to predict at participant homes 
### --> convert act locations to composite scores. multiply by loadings and betas???
### --> create geo dataset

```{r, eval=F}
# View(pls1)
# str(pls1)
# pls1$fitted.values

t <- predict(object = pls.train, #newdata = geo.act, 
             type="scores",
             ncomp = no.pls.comp)

 

###--> make geo.act geocovariates into scores


#create geodataset
geo_act <- as.geodata(pls.train, 
                      coords.col = c("lambert_x", "lambert_y"), 
                      data.col = "log_ufp", 
                      covar.col = names(pls1.scores))


#geo_act <- 





#trend
act_trend <- trend.spatial(trend = cov.trend,
                           geodata = geo_act)
#uk
uk_act <- krige.conv(coords = geo_train_test$coords,
                            data = geo_train_test$data,
                            locations = geo_act$coords,
                            krige=krige.control(type = "ok",
                                                obj.model = reml,
                                                trend.d= train_test_trend,
                                                trend.l= act_trend))
#save UK predictions
geo_act$pred  <- uk_act$predict
 
 


```

prediction boxplots overall and by important TRAP indicators

```{r}

```


Map predictions

### --> see 2.4.5

```{r}

```


Amanda: "Compare distribution of PLS scores in monitoring vs cohort. See if combination of variables is unusual. Check predicted PLS values are not super large – suggests extrapolation. If have strange points, can look to see where they are on a map (e.g., a point is in middle of lake; or strange geographic area)."

```{r}

```


# Sensitivity Analyses

## Averaging of 1 sec data at each stop

??? Use averages

```{r}

```

## Annual averages 

Trim 10% observations

```{r}

```

Windosrize extreme stop readings 

```{r}
# #windosrized alternative 
# ufp <- ufp %>%
#   group_by(site_id) %>%
#   mutate(
#     ufp_wind = ifelse(ptrak_ct_cm3 > quantile(ptrak_ct_cm3, (1-trim_quantile), na.rm = T), quantile(ptrak_ct_cm3, (1-trim_quantile), na.rm = T), 
#                       ifelse(ptrak_ct_cm3 < quantile(ptrak_ct_cm3, (trim_quantile), na.rm = T), quantile(ptrak_ct_cm3, (trim_quantile), na.rm = T), 
#                              ptrak_ct_cm3)))


```

## UK Model

Change TRAP indicator variable transformations (native to log-transformed or vise versa)

```{r}


```

Use different annual averages

```{r}

```

? [time permitting] Normalize to the Roosevelt garage 
```{r}

```

