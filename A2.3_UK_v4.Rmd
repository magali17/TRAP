---
title: "Aim 2: UK Models using PLS"
author: "Magali Blanco"
date: '`r Sys.Date()`'
output:
  html_document:
    number_sections: yes
    df_print: paged
    toc: yes
    toc_depth: '3'
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: 3
editor_options:
  chunk_output_type: console
---

```{r}
# --> TO DO

### --> use parallel::mcapply() to speed up processes [convert for() loops to lapply() --> then mcapply()]
  # Nancy's ppt: https://github.com/deohs/coders/blob/master/demos/hpc/parallel_processing/Parallel_Example.md 
  # on lapply(): https://rpubs.com/drquan/lapply_vs_docall 


### -->use latest NDVI variables
# ? --> switch from geoR to ___ 

```

# Summary of Script

**Approach**    

1. Split up the data (~ 308 locations) into a validation (10%) and a training/test (90%) set.

2. Conduct cross-validation using the training/test set to select the number of PLS components and variogram plotting distance for UK.

3. Fit a semi-final model to all of the training and test set data with the selected parameters. Use this model to estimate the out-of-sample model performance on the validation set.

4. Fit a final model to all of the data & Predict at ACT participant locations
* plot PLS component loadings to characterize the covariates and buffer sizes most heavily weighted in the PLS scores
* compared distribution of PLS scores between the monitoring locations and various ACT locations within monitoring, “study” and spatiotemporal modeling area, as well as nationwide    
* decompose the UK predictions into the LUR vs UK piece

6. Repeate for various sensitivity analyses.

7. Map predictions in study area using a grid

8. Additional analyses

* Lasso regression to identify a few covariates most associated w/ pollutants

# Analyses  

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, 
                      cache=F, cache.comments = F, 
                      message = F, warning = F, 
                      tidy.opts=list(width.cutoff=60), tidy=TRUE,
                      fig.height = 5, fig.width = 8
                      )  

# Clear workspace of all objects and unload all extra (non-base) packages
rm(list = ls(all = TRUE))
if (!is.null(sessionInfo()$otherPkgs)) {
  res <- suppressWarnings(
    lapply(paste('package:', names(sessionInfo()$otherPkgs), sep=""),
      detach, character.only=TRUE, unload=TRUE, force=TRUE))
}

pacman::p_load(knitr, kableExtra, 
               #descriptive statistics
               Hmisc, EnvStats, 
               # modeling
               pls, geoR, #gstat - alternative for UK
               akima, # interp() - interpolate predictions on map
               ggpubr, tidyverse,
               
               parallel,  # mclapply() for parallized processing;  detectCores()
               
               # for kaya - mapping
               ggmap, sf, #mapping
               ggspatial #mapping, adding scales, N arrows...
               )    
#Himsc: describe(); EnvStats: summaryFull(); ggpubr: ggarrange()

set.seed(1)

options(knitr.kable.NA = '')
source("0.Global_Fns.R")
source("A2.0.1_Var&Fns.R")

# act geocovariates
cov_act <- readRDS(file = file.path("Data", "Aim 2", "Geocovariates", "cov_act_preprocessed.rda"))

# all annual estimates
annual0 <- readRDS(file.path("Data", "Aim 2", "Mobile Monitoring", "annual_2020-10-02.rda"))  
  
#covariates

cov_mm <- readRDS(file.path("Data", "Aim 2", "Geocovariates", "cov_mm_preprocessed.rda")) %>%
  # drop pop90, pop_ ( & ??NDVI)
  select(-contains(c("pop90_", "pop_"))) %>%
  #drop Roosevelt garage & stop w/ 1 obeservation that was replaced by MS0601
  filter(!site_id %in% c("MS0000", "MS0398"))

site_loc_vars <- cov_mm %>%
  select(native_id, site_id, latitude, longitude, lambert_x, lambert_y) %>%
  names()

#variable names in log and native scale
non_proximity_vars <- names(cov_mm)[!grepl("m_to_", names(cov_mm)) & !names(cov_mm) %in% site_loc_vars]
proximity_vars_log <- names(cov_mm)[grepl("m_to_", names(cov_mm))]
proximity_vars_native <- str_replace(string = proximity_vars_log, pattern = "log_", replacement = "")
cov_names_log <- append(non_proximity_vars, proximity_vars_log)
cov_names_native <- append(non_proximity_vars, proximity_vars_native)

proximity_vars_native <- cov_mm %>%
  select(site_id, proximity_vars_log) %>%
  mutate_at(proximity_vars_log, ~exp(.)) %>%
  rename_at(proximity_vars_log, ~sub(x = ., "log_", ""))

annual <- annual0 %>%
  #convert to log
  mutate_at(vars(contains(c("ufp", "bc"))), ~log(.)) %>%
  mutate(
    ufp_native_scale = exp(ufp_primary),
    bc_native_scale = exp(bc_primary)) %>%
  left_join(cov_mm) %>%
  # add native scale proximity variables
  left_join(proximity_vars_native)

ufp_names <- names(annual)[grepl("ufp", names(annual))]
bc_names <- names(annual)[grepl("bc", names(annual))]
analysis_names <- c(ufp_names, bc_names)

# add native proximity variables to act
proximity_vars_native_act <- cov_act %>%
  select(site_id, proximity_vars_log) %>%
  mutate_at(proximity_vars_log, ~exp(.)) %>%
  rename_at(proximity_vars_log, ~sub(x = ., "log_", ""))

cov_act_all <- left_join(cov_act, proximity_vars_native_act)  

# group ACT locations by area
monitoring_ids <- cov_act_all$site_id[grepl("monitoring", cov_act_all$site_location)] %>% as.character() 

## index for sites outside monitoring area but within: study, ST, US
outside_monitoring_in_study_ids <- cov_act_all$site_id[grepl("study", cov_act_all$site_location)] %>% as.character() 
outside_monitoring_in_st_ids <- cov_act_all$site_id[grepl("study|st", cov_act_all$site_location)]%>% as.character() 
outside_monitoring_ids <-  cov_act_all$site_id[!grepl("monitoring", cov_act_all$site_location)]%>% as.character() 

study_ids <- append(monitoring_ids, outside_monitoring_in_study_ids)
st_ids <- append(monitoring_ids, outside_monitoring_in_st_ids)


## ACT cov for primary analysis
cov_act <- cov_act_all %>% filter(site_id %in% study_ids)

## ACT cov for sensitivity analyses 
#cov_act_monitoring <- cov_act_all %>% filter(site_id %in% monitoring_ids)
#cov_act_st <- cov_act_all %>% filter(site_id %in% st_ids)

##########################
# create validation index

validation_idx <- sample(c(TRUE, FALSE), replace = T,  
                         size = nrow(annual), 
                         prob = c(.1, .9))

#TEST new validation set to see if OOS R2/RMSE is still better than CV. # get same results
# set.seed(2) 
# validation_idx2 <- sample(c(TRUE, FALSE), replace = T,  
#                          size = nrow(annual), 
#                          prob = c(.1, .9))


```

number of unique covariates used in PLS: `r length(cov_names_log)`

```{r}
# stud grid
grid_covars <- readRDS(file = file.path("Data", "Aim 2", "Geocovariates", "cov_grid_preprocessed.rda"))  

# small grid  
small_grid_covars <- readRDS(file = file.path("Data", "Aim 2", "Geocovariates", "cov_grid_small_preprocessed.rda"))  



project_crs <- 4326

# only keep points in study area & not on water
## study area
study_area_shp <- read_sf(file.path("..", "GIS", "Shapefiles", "Study area", "oval_around_monitoring_area.shp")) %>%
  st_transform(project_crs)

## water
water_shp <- read_sf(file.path("..", "GIS", "Shapefiles", "Other features", "Water", "DNR_Hydrography__Water_Bodies", "DNR_Hydrography__Water_Bodies.shp")) %>%
  st_transform(project_crs)


#create sf object from grid
grid_covars_shp <- grid_covars %>%
  st_as_sf(., coords=c("longitude","latitude"), remove=F,
           crs=project_crs)

small_grid_covars_shp <- small_grid_covars %>%
  st_as_sf(., coords=c("longitude","latitude"), remove=F,
           crs=project_crs)


# intersection
grid_covars_shp$in_study_area <- st_intersects(grid_covars_shp, study_area_shp, sparse = F) %>%
  apply(., 1, any)

## should all be in study area
small_grid_covars_shp$in_study_area <- st_intersects(small_grid_covars_shp, study_area_shp, sparse = F) %>%
  apply(., 1, any)


#points not in water
grid_covars_shp$not_in_water <- !st_intersects(grid_covars_shp, water_shp, sparse = F) %>%
  apply(., 1, any)

small_grid_covars_shp$not_in_water <- !st_intersects(small_grid_covars_shp, water_shp, sparse = F) %>%
  apply(., 1, any)


#convert back to df
grid_covars <- grid_covars_shp %>%
  st_drop_geometry() %>% 
  #only need these points
  filter(in_study_area == TRUE,
         not_in_water == TRUE
         )

small_grid_covars <- small_grid_covars_shp %>%
  st_drop_geometry() %>% 
  #only need these points
  filter(in_study_area == TRUE,
         not_in_water == TRUE
         )

```


```{r}
# create background map for grid/mapping
bbox <- st_bbox(grid_covars_shp)
names(bbox) <- c("left", "bottom", "right", "top")

# make a bit wider
wider <- (max(grid_covars_shp$longitude) - min(grid_covars_shp$longitude))*0.3
bbox[1] <- bbox[1] - wider
#this side is wider for some reason
bbox[3] <- bbox[3] + wider*.6


# background map
map0 <- suppressMessages(get_stamenmap(bbox = bbox, 
                                      zoom = 11,
                                      #maptype = "toner" #"toner-lite"
                                      maptype = "terrain" 
                                      )
                         ) %>%
  # Make basic map image from the tiles
  ggmap(ggmap = ., darken = c(0.5, "white")) + theme_void()

## usage example: 
# map0 + geom_point()

```

```{r}
# same for small grid

# create background map for grid/mapping
bbox_small <- st_bbox(small_grid_covars_shp)
names(bbox_small) <- c("left", "bottom", "right", "top")

# make a bit wider
wider <- (max(small_grid_covars_shp$longitude) - min(small_grid_covars_shp$longitude))*0.3
bbox_small[1] <- bbox_small[1] - wider
#this side is wider for some reason
bbox_small[3] <- bbox_small[3] + wider*.6


# background map
small_map0 <- suppressMessages(get_stamenmap(bbox = bbox_small, 
                                      zoom = 15,
                                      maptype = "terrain" 
                                      )) %>%
  # Make basic map image from the tiles
  ggmap(ggmap = ., darken = c(0.5, "white")) + theme_void()

# small_map0 


```

```{r, eval=F}
#test code to see if map range looks OK
map0 +
  geom_point(data = subset(grid_covars_l,
                           subset = grepl("BC", Pollutant) &
                             grepl("Primary", Analysis)
                             ),
             aes(x=longitude, y=latitude, col=Prediction),
             alpha=a,
             size=pt_size
             ) +
  scale_color_gradient(name = "BC (ng/m3)",
                        low = "yellow", high = "red",
                        ) +
  # add scale & N arrow to top left
  geom_sf(data = grid_covars_shp, inherit.aes = FALSE,
          #don't actually show dots
          alpha=0
          ) +
    annotation_scale(data = grid_covars_shp, location = "tr") +
    annotation_north_arrow(location = "tr",
                           #point towards North Pole
                           which_north = "true",
                           pad_y = unit(0.5, "in"),
                           style = north_arrow_fancy_orienteering
                           ) +
  #markers
  geom_point(data = markers, aes(x=x, y=y, shape = marker)) +
  scale_shape_manual(values=c(12, 1, 2)) +
  #make longer legend text appear at top
  guides(col = guide_colorbar(order = 2),
              shape = guide_legend(order = 1)
         ) +

  theme_bw() +
  theme(
    legend.justification=c(0,1), #legend.justification=c(1,0),
    legend.position=c(0,1),  #legend.position=c(1,0),
    legend.background =  element_blank()
    ) +
  labs(title = "BC",
       x = "Longitude",
       y = "Latitude",
       shape = ""
       )

```


```{r}
#other map markers/locations 
markers <-data.frame(x = c(-122.335, 
                      -122.343,
                      #-122.295,
                      -122.310
                      ),
                y = c(47.606,
                      47.566,
                      #47.530,
                      47.454
                      
                      ),
                marker = c("Downtown Seattle", 
                             "Port of Seattle",
                            #"Major Truck Route",
                            "Sea-Tac Airport"
                             ),
                #select contrasting symbols
                point = c(3,
                          5,
                          #7,
                          10
                  
                )
                 
                )

#saveRDS(object = markers, file = file.path("Data", "GIS", "markers.rda"))

```


Kaya's grid - changed pop to pop10 in order to make model predictions.

```{r, eval=F, Kaya}
# Kaya's grid
kaya_grid0 <- readRDS(file = file.path("Data", "Kaya", "kaya_grid_preprocessed.rda")) 

kaya_grid <- kaya_grid0 %>%
  # artificially change pop_ variable names to pop10_ so model will predict using these variables
  rename_at(vars(starts_with("pop")), ~gsub("pop_", "pop10_", .) )



############################################################################

project_crs <- 4326
#convert grid to shp
kaya_grid_shp <- st_as_sf(kaya_grid,
                          coords=c("longitude","latitude"), remove=F, 
                             crs=project_crs
                          )


# CRS: 4151 lat/long
study_area_shp <- read_sf(file.path("..", "GIS", "Shapefiles", "Study area", "oval_around_monitoring_area.shp")) %>%
  st_transform(project_crs)

# CRS: 4269 lat/long
monitoring_area_shp <- read_sf(file.path("..", "GIS", "Shapefiles", "Study area", "monitoring_area_filled_in.shp")) %>%
  st_transform(project_crs)

 # # see if counties have points within. results in a matrix
kaya_grid$in_study_area <- st_intersects(kaya_grid_shp, study_area_shp, sparse = F) %>%
  apply(., 1, any)
kaya_grid$in_monitoring_area <- st_intersects(kaya_grid_shp, monitoring_area_shp, sparse = F) %>%
  apply(., 1, any)
  
kaya_grid_shp <- kaya_grid_shp %>%
  # points within each area
  mutate(in_study_area = apply(in_study_area,  1, any),
         in_monitoring_area = apply(in_monitoring_area,  1, any),
         )

kaya_grid <- left_join(kaya_grid,  
                       kaya_grid_shp[c("site_id", "in_study_area", "in_monitoring_area")]
                       )
 
############################################################################
## Kaya's grid: many prediction sites are outside of the monitoring & study areas


ggplot() + 
  geom_sf(data = kaya_grid_shp, 
          inherit.aes = FALSE,
          #size=2,
          alpha=0.3,
          aes(col=in_study_area)
          ) + 
  geom_sf(data = kaya_grid_shp[kaya_grid_shp$in_monitoring_area==TRUE,], 
          inherit.aes = FALSE,
          #size=2,
          alpha=0.3,
          ) + 
  labs(title = "Prediction locations relative to monitoring and study areas", 
       caption = "black dots = in monitoring area")

```



Description of analyses 

```{r}
as.data.frame(
  analysis_names) %>%
  label_analysis(var = "analysis_names") %>%
  select(Analysis) %>%
  unique() %>%
  cbind(Description = c("Using stop medians; trimming 5% of each site's highest and lowest stop concentrations; using regression to estimate season-, TOW2- and TOD5-adjusted annual avg concentrations; modeling log-transformed concentrations and proximity covariates (e.g., distance to...)",
             "Like Primary analysis, but using stop means (vs medians) to estimate annual averages",
             "Like Primary analysis, but trimming 10% of each site's highest and lowest stop concentrations (vs 5%) before estimating annual averages",
             "Like Primary analysis, but Windsorizing each site's highest and lowest 5% stop concentrations (vs trimming) before estimating annual averages",
             "Like Primary analysis, but calculating unweighted annual averages for each site (vs regression estimates adjusted for season-, time-of-week- and time-of-day)",
             
             "Like Primary analysis, but using finer regression adjustment for day - TOW7 (vs TOW2) and hour - TOD21 (vs TOD5)",
             
             "Like Primary analysis, but modeling annual average concentrations and proximity covariates on the native scale (vs log-transformed)"
             )) %>%
  kable(., caption = "Description of primary and sensitivity analyses") %>%
  kable_styling()

```

Cross-validation parameters considered.

```{r, echo = T}
fast <- F

if (fast == FALSE) {
    k <- 10
    pls_comps. <- c(1:3)  
    dist_fract. <- c(0.05, seq(0.1, 0.3, by=0.1)) # seq(0.1, 0.4, by=0.1)
  } else {
    k <- 2
    pls_comps. <- c(1:2)
    dist_fract. <- c(seq(0.1, 0.2, by=0.1))
  }

```

```{r}
data.frame(
  Parameter = c("PLS Components",
        "Variogram Distance Fraction"),
  Options_Considered = c(paste0(1, "-", max(pls_comps.)),
                         paste0(dist_fract., collapse = ", "))) %>% 
  kable(caption = "UK model parameters selected through CV") %>%
  kable_styling()

```
 
# CV 

PLS components and variogram distance fraction selected via cross-validation on the train/test set (90% of the data).

```{r, results="hide"}
cv_each_combo <- data.frame()

for (i in seq_along(analysis_names)) {
  #i=12
  # non-nataive scale analyses
  if(!grepl("native_scale", analysis_names[i])) {
    df_cv <-  pls_uk_cv_eval(dt2 = annual[!validation_idx,],
                   pls_comps = pls_comps.,
                   dist_fract = dist_fract., 
                   y_name.. = analysis_names[i],
                   k. = k,
                   # diff than if statement below
                   cov_names.. = cov_names_log, 
                   exponentiate_obs_and_pred = T)
    
    # save individual PLS-variogram results
    each_combo <- df_cv$cv_table %>%
      mutate(Analysis = analysis_names[i])
    
    cv_each_combo <- rbind(cv_each_combo, each_combo)
  }
  
  #native scale analyses
  if(grepl("native_scale", analysis_names[i])) {
    df_cv <-  pls_uk_cv_eval(dt2 = annual[!validation_idx,],
                   pls_comps = pls_comps.,
                   dist_fract = dist_fract., 
                   y_name.. = analysis_names[i],
                   k. = k,
                   # diff than if statement above
                   cov_names.. = cov_names_native,
                   exponentiate_obs_and_pred = F)
    
    # save individual PLS-variogram results
    each_combo <- df_cv$cv_table %>%
      mutate(Analysis = analysis_names[i])
    
    cv_each_combo <- rbind(cv_each_combo, each_combo)
    }
}

### --> ? replace cv_each_combo above w/ cv_results?
cv_results <- cv_each_combo

```


```{r}
# cv_results %>%
#   # relable analysis variable
#   label_analysis(var = "Analysis") %>%
#   kable(caption = "PLS components and variogram parameters selected via cross-validation for primary and sensitivity analyses", 
#         col.names = c("Pollutant", "Analysis", "PLS Components", "Variogram Plotting Max Dist Fraction", "Variogram Plotting Dist (m)", "CV RMSE", "CV R2"),
#         digits = 2) %>%
#   kable_styling() %>%
#   add_footnote("MSE-based R2")

```

```{r, results = "hide", eval=F}
# don't include this for now to simplify analyses 

### Using the same PLS components & distance fractions as primary analysis

cv_primary_pls_comp_ufp <- cv_results$PLS_Components[grepl("ufp_primary", cv_results$Analysis)]
cv_primary_dist_frac_ufp <- cv_results$Variogram_Distance_Fraction[grepl("ufp_primary", cv_results$Analysis)]

cv_primary_pls_comp_bc <- cv_results$PLS_Components[grepl("bc_primary", cv_results$Analysis)]
cv_primary_dist_frac_bc <- cv_results$Variogram_Distance_Fraction[grepl("bc_primary", cv_results$Analysis)]

#no need to change anything below for other pollutants
cv_each_combo <- data.frame()

for (i in seq_along(analysis_names)) {
  #i=7
  
  # use CV-selected PLS & variogram parameters for each pollutant
  if(grepl("ufp", analysis_names[i])) {
    cv_primary_pls_comp <- cv_primary_pls_comp_ufp
    cv_primary_dist_frac <- cv_primary_dist_frac_ufp
  }
  if(grepl("bc", analysis_names[i])) {
    cv_primary_pls_comp <- cv_primary_pls_comp_bc
    cv_primary_dist_frac <- cv_primary_dist_frac_bc
  }
  
  # non-nataive scale analyses
  if(!grepl("native_scale", analysis_names[i])) {
    df_cv <-  pls_uk_cv_eval(dt2 = annual[!validation_idx,],
                   pls_comps = cv_primary_pls_comp,
                   dist_fract = cv_primary_dist_frac, 
                   y_name.. = analysis_names[i],
                   k. = k,
                   # diff than if statement below
                   cov_names.. = cov_names_log, 
                   exponentiate_obs_and_pred = T)
    
    # save individual PLS-variogram results
    each_combo <- df_cv$cv_table %>%
      mutate(Analysis = analysis_names[i])
    
    cv_each_combo <- rbind(cv_each_combo, each_combo)
  }
  
  #native scale analyses
  if(grepl("native_scale", analysis_names[i])) {
    df_cv <-  pls_uk_cv_eval(dt2 = annual[!validation_idx,],
                   pls_comps = cv_primary_pls_comp,
                   dist_fract = cv_primary_dist_frac, 
                   y_name.. = analysis_names[i],
                   k. = k,
                   # diff than if statement above
                   cov_names.. = cov_names_native,
                   exponentiate_obs_and_pred = F)
    
    # save individual PLS-variogram results
    each_combo <- df_cv$cv_table %>%
      mutate(Analysis = analysis_names[i])
    
    cv_each_combo <- rbind(cv_each_combo, each_combo)
    }
}

cv_alt <- cv_each_combo

```

```{r, eval=F}
cv_alt %>%
  label_analysis(var = "Analysis") %>%
  kable(caption = "CV RMSE and R2 for analyses using the same PLS components and variogram parametesr as the primary analysis", 
        col.names = c("Pollutant", "Analysis", "PLS Components", "Variogram Dist Fraction", "Variogram Plotting Dist (m)", "RMSE", "R2"),
        digits = 2) %>%
  kable_styling() %>%
  add_footnote("MSE-based R2") 

```

## Out of sample model performance (using validation set)

calculate the out-of-sample R2 and RMSE for the UK model using the CV-selected parameters.

```{r, results = "hide"}
## TEST: see if results are diff w/ diff validation set  
# validation_idx <- validation_idx2

out_of_sample_valid <- data.frame(Analysis = analysis_names,
                                  RMSE = NA,
                                  R2 = NA, 
                                  stringsAsFactors = F)

for (i in seq_along(analysis_names)) {
  #i=1
  # get validation results
  pls_components <- cv_results$PLS_Components[i]
  pls_variogram_dist <- cv_results$Variogram_Distance_Fraction[i]
  
  if(!grepl("native_scale", analysis_names[i])) {
    
    df <-  uk_predictions(dt = annual[!validation_idx,],
                     cov_loc_new = annual[validation_idx,],
                      y_name = analysis_names[i],
                      cov_names. = cov_names_log,
                      pls_comp = pls_components, 
                      variogram_dist_fract = pls_variogram_dist)
    
    obs <- annual[validation_idx, analysis_names[i]] %>% exp() %>% pull()
    uk_pred <- df$dt$uk_pred %>% exp()
  
    }
  
  #native scale analyses
  #i=6
    if(grepl("native_scale", analysis_names[i])) {
      
      df <-  uk_predictions(dt = annual[!validation_idx,],
                     cov_loc_new = annual[validation_idx,],
                      y_name = analysis_names[i],
                      cov_names. = cov_names_native,
                      pls_comp = pls_components, 
                      variogram_dist_fract = pls_variogram_dist
                     )
    
    obs <- annual[validation_idx, analysis_names[i]] %>% pull()
    uk_pred <- df$dt$uk_pred  
  
    }
  
  out_of_sample_valid$RMSE[i] <- rmse(obs = obs, pred = uk_pred) %>% round()  
  out_of_sample_valid$R2[i] <- r2_mse_based(obs = obs, pred = uk_pred) %>% round(2)
  
}

```

Table of modeling parameters selected via CV and the out-of sample model performance.

A slightly different number of PLS components and variogram parameters were selected via cross-validation for both UFP and BC. CV RMSE and R2 (MSE-based) estimates varied slightly.

For UFP, the primary and unweighted annual average analysis performed best (RMSE: __ , R2: __ ), while the analyses using stop means and windsorized observtaions perfomred worst (RMSE: __ , R2: __ ). 

For BC, the primary and unweighted annual average analysis performed best (RMSE: __ , R2: __ ), while the analyses using the native scale and windsorized observtaions perfomred worst (RMSE: __ , R2: __ ). 

```{r}
cv_results %>%
  rename(CV_RMSE = RMSE,
         CV_R2 = R2) %>% 
  left_join(out_of_sample_valid)  %>%
  rename(OOS_RMSE = RMSE,
         OOS_R2 = R2) %>%
  label_analysis(var = "Analysis") %>%
  select(-Pollutant) %>%
  kable(caption ="UFP and BC PLS components and variogram parameters selected via cross-validation for primary and sensitivity analyses, as well as out-of-sample (OOS) model performances",
        col.names = c( "Analysis", "PLS Comp", "Variogram Max Dist Fraction", "Variogram Dist (m)", "CV RMSE", "CV R2", "OOS RMSE", "OOS R2"),
        digits = 2
        ) %>%
  
  pack_rows("UFP (pt/cm3)", 1, 8) %>%
  pack_rows("BC (ng/m3)", 9, nrow(cv_results)) %>%
  kable_styling() %>%
  add_footnote("MSE-based R2") 

# ## UFP
# cv_t %>%
#   filter(grepl("UFP", Pollutant)) %>%
#   select(-Pollutant) %>%
#   kable(caption = "UFP PLS components and variogram parameters selected via cross-validation for primary and sensitivity analyses, as well as out-of-sample (OOS) model performances", 
#         col.names = c( "Analysis", "PLS Comp", "Variogram Max Dist Fraction", "Variogram Dist (m)", "CV RMSE", "CV R2", "OOS RMSE", "OOS R2"),
#         digits = 2) %>%
#     kable_styling() %>%
#   add_footnote("MSE-based R2") 
# 
# ## BC
# cv_t %>%
#   filter(grepl("BC", Pollutant)) %>%
#   select(-Pollutant) %>%
#   kable(caption = "BC PLS components and variogram parameters selected via cross-validation for primary and sensitivity analyses, as well as out-of-sample (OOS) model performances", 
#         col.names = c("Analysis", "PLS Comp", "Variogram Max Dist Fraction", "Variogram Dist (m)", "CV RMSE", "CV R2", "OOS RMSE", "OOS R2"),
#         digits = 2) %>%
#     kable_styling() %>%
#   add_footnote("MSE-based R2") 





## save results to compare in A3.1_UK.Rmd
#saveRDS(cv_results, file = file.path("Output", "Aim 2", "Tables", "3. UK", "cv_results.rda"))

```
 

```{r, results = "hide", eval=F}
#Using same PLS component number and variogram distance fraction as primary analysis. 

out_of_sample_valid_alt <- data.frame(ufp_names,
                                  RMSE = NA,
                                  R2 = NA)

for (i in seq_along(ufp_names)) {
  #i=1
  # get validation results
  pls_components <- cv_primary_pls_comp
  pls_variogram_dist <- cv_primary_dist_frac
  
  if(!grepl("native_scale", ufp_names[i])) {
    
    df <-  uk_predictions(dt = annual[!validation_idx,],
                     cov_loc_new = annual[validation_idx,],
                      y_name = ufp_names[i],
                      cov_names. = cov_names_log,
                      pls_comp = pls_components, 
                      variogram_dist_fract = pls_variogram_dist
                     )
    
    obs <- annual[validation_idx, ufp_names[i]] %>% exp()
    uk_pred <- df$dt$uk_pred %>% exp()
    
    }
  
  #native scale analyses
  #i=6
    if(grepl("native_scale", ufp_names[i])) {
      
      df <-  uk_predictions(dt = annual[!validation_idx,],
                     cov_loc_new = annual[validation_idx,],
                      y_name = ufp_names[i],
                      cov_names. = cov_names_native,
                      pls_comp = pls_components, 
                      variogram_dist_fract = pls_variogram_dist
                     )
    
    obs <- annual[validation_idx, ufp_names[i]]  
    uk_pred <- df$dt$uk_pred  
  
    }
  
  out_of_sample_valid_alt$RMSE[i] <- rmse(obs = obs, pred = uk_pred)
  out_of_sample_valid_alt$R2[i] <- r2_mse_based(obs = obs, pred = uk_pred)
  
  
} 

```

```{r, eval=F}
out_of_sample_valid_alt %>%
    filter(!grepl("primary", ufp_names)) %>%
  mutate(RMSE = round(RMSE),
         Analysis = recode_factor(ufp_names,
                                  "primary_ufp" = "Primary", 
                                  "stop_means_ufp" = "Stop Means",
                                  "trim10_ufp" = "Trim 10%",
                                  "windsorize_ufp" = "Windsorize",
                                   "uw_ufp" = "Unweighted Annual Avg",
                                  "native_scale_ufp" = "Native Scale")) %>%
  select(Analysis, RMSE, R2) %>%
  kable(caption = "Out-of-sample RMSE and R2 using model parameters similar to primary analysis  (calculated on validation set).",
        digits = 2) %>%
  kable_styling()

```

# Fit UK model to all data and predict at ACT locations 

```{r, variogram plots}
# variogram plots 
# variograms are for the non-validation set (90% of the data) of the primary analysis 

set.seed(1)
my_pollutant <- c("ufp_primary", "bc_primary")

# create 1 pdf for each pollutant
for (p in seq_along(my_pollutant)) {

  pdf(file.path("Output", "Aim 2", "Images", "3. UK", paste0(my_pollutant[p], "_variograms.pdf")))
  
  print("variograms are for the non-validation set (90% of the data) of the primary analysis")
  
  dt <- annual %>%
    rename(log_ufp = my_pollutant[p]) %>%
    filter(!validation_idx)
  
  use_n_scores <- 3
  
  dist_fractions <- c(0.05, seq(0.1, 0.4, by=0.1))
  
  par(mfrow = c(3, 2))
  
  for(i in seq_len(use_n_scores)) {
    #i=1
    score_n_names <- paste0("Comp", 1:i)
    
    pls_train_test <- plsr(log_ufp ~.,
                 data=dt[,c("log_ufp", cov_names_log)], 
                 ncomp = i,
                 scale=T
                 )
    
    # extract scores for UK
    scores_train_test <- scores(pls_train_test)[,c(1:i)] %>%
      as.data.frame()
    
    # take out spaces in names
    names(scores_train_test) <- score_n_names
    
    # dataset w/ UFP measurements, geocovariates, location
    pls_df_train_test <- cbind(dt, scores_train_test)  
  
    ################################ UK ################################
     geo_train_test <- as.geodata(pls_df_train_test, 
                          coords.col = c("lambert_x", "lambert_y"), 
                          data.col = "log_ufp", 
                          covar.col = score_n_names)
  
    ##trend
    cov_trend <-  as.formula(paste0("~ ", paste0(score_n_names, collapse = " + " )))
    
    #plotting distances
    max.dist <- summary(geo_train_test)$distances.summary[["max"]]
    ## plot nearby locations more finely - improves variogram model fit?
    by1_pt <- 300
    brk_pt <- 1000
    by2_pt <- 1000
    
    ############################ model residuals ######################################
    for(j in seq_along(dist_fractions)) {
      #j=1
      max.plot.dist <- max.dist*dist_fractions[j]
  
    ##Empirical Variogram
    variog_train_test <- variog(geo_train_test,
                           #create equally spaced bins for all distances plotted
                          uvec =c(seq(0, brk_pt, by = by1_pt), seq((brk_pt + by2_pt), max.plot.dist, by= by2_pt)),
                          #UK
                          trend = cov_trend, 
                          messages = F  
                          )
  
    #plot(variog_train)
   
    wls_ests_train_test <- variofit(variog_train_test, cov.model = "exp",
                                                messages = F)
    
    resid_model_train_test <- variofit(vario = variog_train_test, 
                        ini = wls_ests_train_test, 
                        cov.model = "exp",
                        #weights = "equal") #ols
                        weights = "npairs",
                        messages = F
                        ) #wls
    
     plot(variog_train_test, 
         main = paste0("Comp: ", i, ", Dist fract: ", dist_fractions[j]), 
         cex.main=0.8
           )
    lines(variog_train_test)
    lines(resid_model_train_test, col=2)
  
    }
  }
  dev.off()
}

```


```{r, results = "hide"}
set.seed(1)

uk_names <- paste0(analysis_names, "_uk")

# empty columns to save predictions
cov_act_all[,uk_names] <- NA
residual_model_param <- data.frame()  
pls_models <- list()
empirical_variograms <- list()
residual_models <- list()
uk_betas <- data.frame() #list()

for (i in seq_along(analysis_names)) {
  #i=1
  # validation results
  pls_components <- cv_results$PLS_Components[cv_results$Analysis == analysis_names[i]]  
  pls_variogram_dist <- cv_results$Variogram_Distance_Fraction[cv_results$Analysis == analysis_names[i]]  
  
  if(!grepl("native_scale", analysis_names[i])) {
    
    df <-  uk_predictions(dt = annual,
                     cov_loc_new = cov_act_all,
                      y_name = analysis_names[i],
                      cov_names. = cov_names_log,
                      pls_comp = pls_components, 
                      variogram_dist_fract = pls_variogram_dist)
    
    # save predictions in native scale
    cov_act_all[uk_names[i]] <- exp(df$dt$uk_pred)
    }

  #native scale analyses
  #i=6
    if(grepl("native_scale", analysis_names[i])) {
      
      df <-  uk_predictions(dt = annual,
                     cov_loc_new = cov_act_all,
                      y_name = analysis_names[i],
                      cov_names. = cov_names_native,
                      pls_comp = pls_components, 
                      variogram_dist_fract = pls_variogram_dist)
      
      cov_act_all[uk_names[i]] <- df$dt$uk_pred
    }
  
    # save final residual model parameters
    residual_model_param <- rbind(residual_model_param, df$residual_model_table)
    
    # save geodataset
    # geodatasets[i] <- list(df$geo_dataset)
    # names(geodatasets)[i] <- uk_names[i]
    
    #pls_models
    pls_models[i] <- list(df$pls_model)
    names(pls_models)[i] <- uk_names[i]
    
    # variograms/residual models
    empirical_variograms[i] <- list(df$empirical_variogram)
    names(empirical_variograms)[i] <- uk_names[i]
    
    residual_models[i] <- list(df$residual_model)
    names(residual_models)[i] <- uk_names[i]
    
    # UK betas
    #uk_betas[i] <- df$betas
    uk_betas1 <- data.frame(df$betas) %>%
      rownames_to_column(var = "beta") %>% 
      rename(est = df.betas) %>%
      mutate(Analysis = uk_names[i])
    
    uk_betas <- rbind(uk_betas, uk_betas1)
   
}

```

```{r, likfit example, eval=F}
# ### --> compare my results form primary analysis (3 PLS, 10% plotting distance) to using likfit()
#   dt = annual
#   y_name = "ufp_primary"
#   cov_names. = cov_names_log
#   #CV folds
#   k = 3
#   # max PLS components to evaluate
#   use_n_scores. = 3
#    
#   score_n_names <- paste0("Comp", 1:use_n_scores.)
#   
#   dt <- dt %>% rename(y_name = y_name) %>%
#     drop_na() %>%
#     #create folds for test/train set
#     mutate(set = sample(c(1:k), size = nrow(.), replace = T),
#            cv_prediction = NA)
#  
#   for(f in seq_len(k)) {
#     #f=1
#     train_grp <- dt$set != f
#     
#     dt_train <- dt %>% filter(train_grp)  
#     
#     dt_test <- dt %>% filter(!train_grp)   
#     
#     #fit PLS to training data
#     pls_train <- plsr(y_name ~.,
#                       data=dt_train[,c("y_name", cov_names.)], 
#                       ncomp = use_n_scores.,
#                       scale=T)
#     
#     # extract scores for UK
#     scores_train <- scores(pls_train)[,c(1:use_n_scores.)] %>% 
#       as.data.frame()
#     scores_test <- predict(object = pls_train,
#                            newdata = dt_test,
#                            ncomp = 1:use_n_scores.,
#                            type = "score") %>%
#       as.data.frame()
#     
#     # take out spaces in names
#     names(scores_train) <- score_n_names
#     names(scores_test) <- score_n_names
#     
#     # dataset w/ UFP measurements, geocovariates & location info
#     pls_df_train <- cbind(dt_train, scores_train)  
#     
#     pls_df_test <- cbind(dt_test, scores_test)  
#     
#     ################################ UK ################################
#     geo_train <- as.geodata(pls_df_train, 
#                             coords.col = c("lambert_x", "lambert_y"), 
#                             data.col = "y_name", 
#                             covar.col = score_n_names)
#     
#     # --> ? don't need this now?
#     geo_test <- as.geodata(pls_df_test, 
#                            coords.col = c("lambert_x", "lambert_y"), 
#                            data.col = "y_name", 
#                            covar.col = score_n_names)
#     
#     ##trend
#     cov_trend <-  as.formula(paste0("~ ", paste0(score_n_names,  collapse = " + " )))
#     
#     # # issue: still have to tell it a plotting distance 
#     # variog_train <- variog(geo_train,
#     #                         uvec=c(seq(0, 1000, by = 300), seq((1000 + 1000),  by= 1000)))
#     
#     
#     # can run variog() w/o plotting maximum, but range is very large ~ 10k
#     #variog_train <- variofit(variog(geo_train), cov.model = "exp" ) 
#      
#     #plot(variog(geo_train))
# set.seed(1)
# lf1 <- likfit(geodata = geo_train, 
#        trend = cov_trend,
#        #sig squared (partial sill), phi/theta (range). manually estimate these by looking at variogram plots
#        
#        #ini.cov.pars = c(0.025, 4000),  # Dr. Sampson suggested a range of 5000 ##variog_train
#        
#        # ? or use a matrix
#        ini.cov.pars = as.matrix(cbind(seq(0, 1, l=4), seq(1000, 5000, l=4)) ) , 
#         
#        
#        #ini.cov.pars = variog_train,
# 
#        # ? helps? is this like using a matrix of values for ini.cov.pars?
#        limits = pars.limits(sigmasq=c(.01, .9), phi=c(300, 10000)),
#        
#        fix.nugget = FALSE,
#        cov.model = "exponential",
#        lik.method = "ML"
#        )
# 
# # doesn't work? #use summary.likGRF() to pring summary of fitted model. e.g., see $parameters.summary    
# lf1$parameters.summary
# 
# 
# ### --> ?? how do u make predictions? 
# predict(lf1, newdata = geo_test) 
#     
#      
#     
#   }
#   
#     #save CV predictions
#     dt$cv_prediction[!train_grp] <- kc_cv$predict
#     
# ########################################################################################################################
#     
#   #   max.dist <- summary(geo_train)$distances.summary[["max"]]
#   #   
#   #   # --> ? select this variogram parameter through CV??
#   #   max.plot.dist <- max.dist*dist_fract. #[dist_fract_index] 
#   #   
#   #   ############################ model residuals ###################################### 
#   #   ##Empirical Variogram
#   #   brk_pt <- 1000
#   #   by1_pt <- 300
#   #   by2_pt <- 1000
#   #   
#   #   variog_train <- variog(geo_train,
#   #                          uvec=c(seq(0, brk_pt, by = by1_pt), seq((brk_pt + by2_pt), max.plot.dist, by= by2_pt)),
#   #                          #UK
#   #                          trend = cov_trend, 
#   #                          messages = F)
#   #   
#   #   #use geoR try to estimate intitial range & sill values. using WLS and an exponential fit
#   #   wls_ests_train <- variofit(variog_train, cov.model = "exp", 
#   #                              messages = F)
#   #   
#   #   # --> ? select this variogram parameter through CV??
#   #   #don't need initial values above since estimates seem to be the same w/ or w/o ini = wls_ests_train (based on small sample)?
#   #   resid_model_train <- variofit(vario = variog_train, 
#   #                                 ini = wls_ests_train, 
#   #                                 cov.model = "exp",
#   #                                 #weights = "equal", #ols
#   #                                 weights = "npairs",#wls
#   #                                 #messages = F
#   #                                 ) 
#   #   
#   #   #trend
#   #   train_trend <- trend.spatial(trend = cov_trend, geo_train)
#   #   test_trend <- trend.spatial(trend = cov_trend, geo_test)
#   #   
#   #   ############################# Predict #############################
#   #   kc_cv <- krige.conv(coords = geo_train$coords,
#   #                       data = geo_train$data,
#   #                       locations = geo_test$coords,
#   #                       krige = krige.control(type = "ok",
#   #                                             obj.model = resid_model_train, 
#   #                                             trend.d = train_trend,
#   #                                             trend.l = test_trend))
#   #   
#   #   #save CV predictions
#   #   dt$cv_prediction[!train_grp] <- kc_cv$predict
#   # 
#   # }
#   # 
#   # 
#   # result <- list(dt = dt,
#   #                max_plot_dist = max.plot.dist,
#   #                max_dist = max.dist)
#   # 
#   # return(result)
#   # #return(dt)
 

```


## PLS results 

### PLS component loadings for primary analysis

- UFP 

The PLS component loadings indicated that the covariates most negatively associated with both UFP and BC concentrations included: NDVI (with smaller buffers having stronger negative associations), distance to port and rail yard. The covariates most positively associated with pollutant concentrations included: highly developed land use, imperviousness, major road length within a buffer, the number of intersections within a buffer, and population density (with smaller buffers generally having stronger positive associations).

```{r, fig.height=8}
my.alpha=0.3

pls_loadings <- pls_models[["ufp_primary_uk"]]$loadings[] %>%
  as.data.frame() %>%
  rownames_to_column(var = "cov") %>%
  # rename variables if buffers
  split_cov_name(cov = "cov") %>%
  #make long format for faceting
  gather(key = "Component", value = "Loading", contains("Comp")) %>%
  mutate(Component = as.numeric(substr(Component, 6, nchar(Component))))   

pls_loadings  %>%
  #buffered covariates
  drop_na(buffer) %>%
  ggplot(aes(x = Loading, y = cov)) +
  geom_point(aes(size=buffer),
             shape=1,
             alpha=my.alpha) +
  scale_size(breaks = c(min(pls_loadings$buffer, na.rm = T),
                        max(pls_loadings$buffer, na.rm = T)
                        )) + #500, 5000, 10000,
  #non-buffered covariates
  geom_point(data = pls_loadings[is.na(pls_loadings$buffer),],
           alpha=my.alpha,
           aes(shape="")) +
  geom_vline(xintercept=0,
             linetype="solid",
             alpha=my.alpha) +
    facet_wrap(~Component,
               labeller = "label_both",
               nrow = 1) +
  labs(y = "Geocovariate",
       shape= "non-buffer", #"proximity,\nelevation",
       title = "PLS Geocovariate Component Loadings \nfor Annual Average UFP Concentration") +
  theme(legend.position = "bottom")


```

- BC 

```{r, fig.height=8}

pls_loadings <- pls_models[["bc_primary_uk"]]$loadings[] %>%
  as.data.frame() %>%
  rownames_to_column(var = "cov") %>%
  # rename variables if buffers
  split_cov_name(cov = "cov") %>%
  #make long format for faceting
  gather(key = "Component", value = "Loading", contains("Comp")) %>%
  mutate(Component = as.numeric(substr(Component, 6, nchar(Component))))

pls_loadings  %>%
  #buffered covariates
  drop_na(buffer) %>%
  ggplot(aes(x = Loading, y = cov)) +
  geom_point(aes(size=buffer),
             shape=1,
             alpha=my.alpha) +
  scale_size(breaks = c(min(pls_loadings$buffer, na.rm = T),
                        max(pls_loadings$buffer, na.rm = T)
                        )) + #500, 5000, 10000,
  #non-buffered covariates
  geom_point(data = pls_loadings[is.na(pls_loadings$buffer),],
           alpha=my.alpha,
           aes(shape="")) +
  geom_vline(xintercept=0,
             linetype="solid",
             alpha=my.alpha) +
    facet_wrap(~Component,
               labeller = "label_both",
               nrow = 1) +
  labs(y = "Geocovariate",
       shape= "non-buffer", #"proximity,\nelevation",
       title = "PLS Geocovariate Component Loadings \nfor Annual Average BC Concentration") +
  theme(legend.position = "bottom")

```

### Distribution of PLS component scores for mobile monitoring stops and ACT locations. 

Checking that the distribution of PLS values are not different between monitoring stops and ACT locations – this suggests extrapolation.  

- UFP 

Mobile monitoring stops and ACT cohort locations within the study area have similar PLS scores, though cohort locations appera to have slightly PLS scores.

```{r}
# for all ACT locations within study area
pls_in_study_ufp <- compare_pls_scores(my_pls_model = pls_models[["ufp_primary_uk"]],
                        #new_prediction_sites = cov_act, 
                        new_prediction_sites = cov_act_all[cov_act_all$site_id %in% study_ids,], 
                        my_title = "Cohort locations within the study area", 
                        #new_site_label = 
                        ) 

pls_in_study_ufp$plot

```

The PLS scores of ACT cohort locations within the monitoring area are similar to the mobile monitoring stops PLS scores.

```{r, fig.height=4}
compare_pls_scores(my_pls_model = pls_models[["ufp_primary_uk"]],
                        new_prediction_sites = cov_act_all[cov_act_all$site_id %in% monitoring_ids,],
                        my_title = "Cohort locations in the monitoring area")$plot

```

ACT cohort locations outside the monitoring area but still within the study area, however, have PLS scores that are slightly lower but still within range of the mobile monitoring stop PLS scores. 

```{r}
# in study but outside monitoring area
compare_pls_scores(my_pls_model = pls_models[["ufp_primary_uk"]],
                        new_prediction_sites = cov_act_all[cov_act_all$site_id %in% outside_monitoring_in_study_ids,], 
                        my_title = "Cohort locations outside the monitoring but within the study area")$plot

# # for all ACT locations within study area 
# compare_pls_scores(my_pls_model = pls_models[["ufp_primary_uk"]],
#                         new_prediction_sites = cov_act_all[cov_act_all$site_id %in% study_ids,],
#                         my_title = "Cohort locations within the study area")$plot


```

Outside monitoring, inside ST area 

```{r}
compare_pls_scores(my_pls_model = pls_models[["ufp_primary_uk"]],
                        new_prediction_sites = cov_act_all[cov_act_all$site_id %in% outside_monitoring_in_st_ids,], 
                        my_title = "UFP: Cohort locations outside the monitoring but within the spatiotemporal modeling region")$plot

```


Map of PLS component 1 & 3 scores 

Locations with component 1 Scores < 15 are those in the westmost part of the study area. 

```{r}
pls_locations <- cbind(cov_act[c("site_id", "latitude", "longitude")], pls_in_study_ufp$scores_new_locs)

pls_locations %>%
  map_fn(color_by = "Comp1", 
         map_title = "PLS Component 1 Scores for UFP Model") +
       scale_color_gradient2(name = "Score", 
                        low = "black", mid= "white", high = "red",
                        midpoint = -16) 

# pls_locations %>%
#   map_fn(color_by = "Comp3", 
#          map_title = "PLS Component 3 Scores for UFP Model") +
#        scale_color_gradient2(name = "Score", 
#                         low = "black", mid= "white", high = "red",
#                         midpoint = -9) 

```

```{r}
# for later?
## all ACT locations
primary_scores_ufp_all_loc <- compare_pls_scores(my_pls_model = pls_models[["ufp_primary_uk"]],
                        new_prediction_sites = cov_act_all,
                        my_title = "All Cohort locations")

```

- BC 

The same is true for BC.

```{r}
# for all ACT locations within study area
pls_in_study_bc <- compare_pls_scores(my_pls_model = pls_models[["bc_primary_uk"]],
                        new_prediction_sites = cov_act,  
                        my_title = "Cohort locations within the study area")

pls_in_study_bc$plot

```

```{r, fig.height=4}
compare_pls_scores(my_pls_model = pls_models[["bc_primary_uk"]],
                        new_prediction_sites = cov_act_all[cov_act_all$site_id %in% monitoring_ids,],
                        my_title = "Cohort locations in the monitoring area")$plot

```

```{r}
# in study but outside monitoring area
compare_pls_scores(my_pls_model = pls_models[["bc_primary_uk"]],
                        new_prediction_sites = cov_act_all[cov_act_all$site_id %in% outside_monitoring_in_study_ids,], 
                        my_title = "Cohort locations outside the monitoring but within the study area")$plot

 
```

outside monitoring but inside ST area 

```{r}
compare_pls_scores(my_pls_model = pls_models[["bc_primary_uk"]],
                        new_prediction_sites = cov_act_all[cov_act_all$site_id %in% outside_monitoring_in_st_ids,], 
                        my_title = "BC: Cohort locations outside the monitoring but within the spatiotemporal modeling region")$plot
```


```{r}
pls_locations <- cbind(cov_act[c("site_id", "latitude", "longitude")], pls_in_study_bc$scores_new_locs)

pls_locations %>%
  map_fn(color_by = "Comp1", 
         map_title = "PLS Component 1 Scores for BC Model") +
       scale_color_gradient2(name = "Score", 
                        low = "black", mid= "white", high = "red",
                        midpoint = -16) 

# pls_locations %>%
#   map_fn(color_by = "Comp3",
#          map_title = "PLS Component 3 Scores for UFP Model") +
#        scale_color_gradient2(name = "Score",
#                         low = "black", mid= "white", high = "red",
#                         midpoint = -6)


```

 

```{r}
# for later?
## all ACT locations
primary_scores_bc_all_loc <- compare_pls_scores(my_pls_model = pls_models[["bc_primary_uk"]],
                        new_prediction_sites = cov_act_all,
                        my_title = "All Cohort locations")

```

## Residual UK Model parameters

```{r}

cbind(Analysis = uk_names, residual_model_param) %>%
  mutate(Range_m = round(Range_m)) %>% 
    label_analysis(var = "Analysis", end_character = 3) %>%
  mutate(Pollutant = ifelse(!grepl("native", tolower(Analysis)), paste0("log ", Pollutant), Pollutant)) %>%
  rename("Pollutant Modelled" = Pollutant) %>%
  kable(caption = "UK residual model parameters for primary and sensitivity analyses", 
        digits = 3) %>%
  kable_styling()

```

Variogram for primary analyses. Modeled variograms are fit as exponential functions using weighted least squares.

- UFP 

```{r}
plot_pollutant <- "ufp"

plot_variogram <- empirical_variograms[[paste0(plot_pollutant, "_primary_uk")]]
plot_residual_model <- residual_models[[paste0(plot_pollutant, "_primary_uk")]]

plot(plot_variogram,
     main = paste0(
       #"Binned empirical and modeled variogram for primary ", 
       toupper(plot_pollutant) #, 
       #" analysis"
       ),
     xlab = "Distance (m)")

lines(plot_variogram, lty=1)
lines(plot_residual_model, lty=2, col=2)
legend("bottomright",
       legend = c("Empirical", paste0("Residual Model")),
       lty=c(1:2), col = c(1:2))

```

- BC 

```{r}
plot_pollutant <- "bc"

plot_variogram <- empirical_variograms[[paste0(plot_pollutant, "_primary_uk")]]
plot_residual_model <- residual_models[[paste0(plot_pollutant, "_primary_uk")]]

plot(plot_variogram,
     main = paste0(
       #"Binned empirical and modeled variogram for primary ", 
       toupper(plot_pollutant) #, 
       #" analysis"
       ),
     xlab = "Distance (m)")
lines(plot_variogram, lty=1)
lines(plot_residual_model, lty=2, col=2)
legend("bottomright",
       legend = c("Empirical", paste0("Residual Model")),
       lty=c(1:2), col = c(1:2))
```

## LUR vs Kriging 

UK Yhat = LUR + kriging 

How much does  kriging vs LUR (the mean part of the model) contribute to the final UK model prediction? If not much (indicated by paired points closer to the one-to-one line), we are less worried about using observations from 2019 to smooth observations back in time since (Aim 3). 

The plots below are colored to indicate the increasing distance of ACT cohort locations from the monitoring area. Locations in the "study" are those outside the monitoring area, but inside the study area. Locations in the "ST" are those outside the study area, but inside the ST modeling area. Locations in the "U.S." are those outside the ST modeling area, but within the U.S. (i.e., all other available locations).

- UFP 

Locations further away from the monitoring area tend to be close to the one-to-one line, indicating that little kriging is being done to the LUR prediction because of their distance from the monitoring sites.

In the study area, the LUR piece of UK explains about __ 64% __ of the variation in the overall UK predictions.

```{r, fig.height=8}
my_analysis <- "ufp_primary_uk"

# UK betas
betas1 <- uk_betas %>% filter(Analysis == my_analysis)

beta_names <- betas1$beta
                              # subtract beta0
max_beta <- length(beta_names) - 1

                  # --> change here for new datasets  
new_score_values <- primary_scores_ufp_all_loc$scores_new_locs 

uk_decompose <- cov_act_all[my_analysis] 

#  yhat = alpha + ß1*X1
uk_decompose$regression_prediction <- betas1$est[betas1$beta=="beta0"] + betas1$est[betas1$beta=="beta1"]*new_score_values$Comp1

## update regression prediction if more than 1 component
if(max_beta > 1) {
  for(i in c(2:max_beta)) {

    beta_no <- paste0("beta", i)
    comp_no <- paste0("Comp", i)

      uk_decompose$regression_prediction <- uk_decompose$regression_prediction + betas1$est[betas1$beta==beta_no]*new_score_values[[comp_no]]
  }
}

uk_decompose <- uk_decompose %>% 
  mutate(
    # transform predictions to native scale before plotting
    regression_prediction = exp(regression_prediction),
    krige_and_error = uk_decompose[[my_analysis]] - regression_prediction)

# add id info for plotting purposes
uk_decompose <- cbind(cov_act_all[c("site_id", "site_location", "latitude", "longitude")], 
                      uk_decompose) %>% 
  # add location columns
  mutate(
    site_location = as.character(site_location),
    site_location = ifelse(is.na(site_location), "U.S.", site_location),
    monitoring_area = ifelse(site_id %in% monitoring_ids, TRUE, NA),
    study_area = ifelse(site_id %in% study_ids, TRUE, NA),
    st_area = ifelse(site_id %in% st_ids, TRUE, NA),
    all_area = TRUE)

p_monitoring <- uk_decompose %>%
    filter(monitoring_area) %>%
  colo.plot(y.variable = my_analysis, y.label = "UK",
          x.variable = "regression_prediction", x.label = "LUR", 
          mytitle = "ACT locations in the monitoring area",
          col.by = "site_location")  

p_study <- uk_decompose %>%
    filter(study_area) %>%
  colo.plot(y.variable = my_analysis, y.label = "UK",
          x.variable = "regression_prediction", x.label = "LUR", 
          mytitle = "ACT locations in the study area", 
          col.by = "site_location")  
p_study_ufp <- p_study

p_st <- uk_decompose %>%
    filter(st_area) %>%
  colo.plot(y.variable = my_analysis, y.label = "UK",
          x.variable = "regression_prediction", x.label = "LUR", 
          mytitle = "ACT locations in the Spatiotemporal model area", 
          col.by = "site_location")  

p_all <- uk_decompose %>%
  colo.plot(y.variable = my_analysis, y.label = "LUR + Kriging",
          x.variable = "regression_prediction", x.label = "LUR", 
          mytitle = "All ACT locations in the U.S.", 
          col.by = "site_location") 

 

# other areas
ggarrange(p_monitoring, p_study, p_st, p_all) %>%
  annotate_figure(top = "UK vs regression UFP predictions (pt/cm3) from the primary analysis for ACT locations")

```

Map of the degree of kriging occurring at ACT locations within the study area.  

Locations near the Port of Seattle and northeast of the airport are krigged the most.   
- Higher kriging near the Port of Seattle and the airport indicate that these are locations where the LUR piece of UK is  characterizing concentrations less well (based on geocovariates alone).

```{r}
# uk_decompose %>%
#   filter(study_area) %>%
#   map_fn(color_by = "krige_and_error", 
#          color_units = "UFP (pt/cm3)",
#          map_title = "Degree of Kriging in the UK Predictions\nUFP")

```

```{r}
pt_size <- 2.5
a <- 0.8

# save & plot together w/ BC later
ufp_p <- map0 +
  geom_point(data = subset(uk_decompose, 
                           subset = study_area == TRUE
                             ),
             aes(x=longitude, y=latitude, col=krige_and_error),
             alpha=a,
             size=pt_size
             ) + 
  scale_color_gradient(name = "Kriging\n(UFP pt/cm3)", 
                        low = "yellow", high = "red",
                        ) +
  # add scale & N arrow to top left
  geom_sf(data = grid_covars_shp, inherit.aes = FALSE,
          #don't actually show dots
          alpha=0
          ) +
    annotation_scale(data = grid_covars_shp, location = "tr") +
    annotation_north_arrow(location = "tr",
                           #point towards North Pole
                           which_north = "true",
                           pad_y = unit(0.5, "in"),
                           style = north_arrow_fancy_orienteering
                           ) +
  #markers
  geom_point(data = markers, aes(x=x, y=y, shape = marker)) +
  scale_shape_manual(values=c(12, 1, 2)) +
  #make longer legend text appear at top
  guides(col = guide_colorbar(order = 2), 
              shape = guide_legend(order = 1)
         ) +
  #scale_x_continuous( n.breaks = 4) +
  theme_bw() +
  theme(
    legend.justification=c(0,1), #legend.justification=c(1,0), 
    legend.position=c(0,1),  #legend.position=c(1,0), 
    legend.background =  element_blank()
    ) +
  labs(title = "UFP",
       x = "Longitude",
       y = "Latitude",
       shape = ""
       )

# ufp_p
```

- BC 

In the study area, the LUR piece of UK explains about __ 51% __ of the variation in the overall UK predictions.

```{r, fig.height=8}
my_analysis <- "bc_primary_uk"

# UK betas
betas1 <- uk_betas %>% filter(Analysis == my_analysis)

beta_names <- betas1$beta
                              # subtract beta0
max_beta <- length(beta_names) - 1

                  # --> change here for new datasets  
new_score_values <- primary_scores_bc_all_loc$scores_new_locs 

uk_decompose <- cov_act_all[my_analysis] 

#  yhat = alpha + ß1*X1
uk_decompose$regression_prediction <- betas1$est[betas1$beta=="beta0"] + betas1$est[betas1$beta=="beta1"]*new_score_values$Comp1

## update regression prediction if more than 1 component
if(max_beta > 1) {
  for(i in c(2:max_beta)) {

    beta_no <- paste0("beta", i)
    comp_no <- paste0("Comp", i)

      uk_decompose$regression_prediction <- uk_decompose$regression_prediction + betas1$est[betas1$beta==beta_no]*new_score_values[[comp_no]]
  }
}

uk_decompose <- uk_decompose %>% 
  mutate(
    # transform predictions to native scale before plotting
    regression_prediction = exp(regression_prediction),
    krige_and_error = uk_decompose[[my_analysis]] - regression_prediction)

# add id info for plotting purposes
uk_decompose <- cbind(cov_act_all[c("site_id", "site_location", "latitude", "longitude")], 
                      uk_decompose) %>% 
  # add location columns
  mutate(
    site_location = as.character(site_location),
    site_location = ifelse(is.na(site_location), "U.S.", site_location),
    monitoring_area = ifelse(site_id %in% monitoring_ids, TRUE, NA),
    study_area = ifelse(site_id %in% study_ids, TRUE, NA),
    st_area = ifelse(site_id %in% st_ids, TRUE, NA),
    all_area = TRUE)

p_monitoring <- uk_decompose %>%
    filter(monitoring_area) %>%
  colo.plot(y.variable = my_analysis, y.label = "UK",
          x.variable = "regression_prediction", x.label = "LUR", 
          mytitle = "ACT locations in the monitoring area",
          col.by = "site_location")  

p_study <- uk_decompose %>%
    filter(study_area) %>%
  colo.plot(y.variable = my_analysis, y.label = "UK",
          x.variable = "regression_prediction", x.label = "LUR", 
          mytitle = "ACT locations in the study area", 
          col.by = "site_location")  
p_study_bc <- p_study

p_st <- uk_decompose %>%
    filter(st_area) %>%
  colo.plot(y.variable = my_analysis, y.label = "UK",
          x.variable = "regression_prediction", x.label = "LUR", 
          mytitle = "ACT locations in the Spatiotemporal model area", 
          col.by = "site_location")  

p_all <- uk_decompose %>%
  colo.plot(y.variable = my_analysis, y.label = "UK",
          x.variable = "regression_prediction", x.label = "LUR", 
          mytitle = "All ACT locations in the U.S.", 
          col.by = "site_location") 

ggarrange(p_monitoring, p_study, p_st, p_all) %>%
  annotate_figure(top = "UK vs regression BC predictions (ng/m3) from the primary analysis for ACT locations")

```

Map of the degree of kriging occurring at ACT locations within the study area.  

Locations near the Port of Seattle and northeast of the airport are krigged the most.   
- Higher kriging near the Port of Seattle and I-5 indicate that these are locations where the LUR piece of UK is  characterizing concentrations less well (based on geocovariates alone).

```{r}
# uk_decompose %>%
#   filter(study_area) %>%
#   map_fn(color_by = "krige_and_error", 
#          color_units = "BC (ng/m3)",
#          map_title = "Degree of Kriging in the UK Predictions\nBC")

```


```{r}

# save & plot together w/ BC later
bc_p <- map0 +
  geom_point(data = subset(uk_decompose, 
                           subset = study_area == TRUE
                             ),
             aes(x=longitude, y=latitude, col=krige_and_error),
             alpha=a,
             size=pt_size
             ) + 
  scale_color_gradient(name = "Kriging\n(BC ng/m3)", 
                        low = "yellow", high = "red",
                        ) +
  # add scale & N arrow to top left
  geom_sf(data = grid_covars_shp, inherit.aes = FALSE,
          #don't actually show dots
          alpha=0
          ) +
    annotation_scale(data = grid_covars_shp, location = "tr") +
    annotation_north_arrow(location = "tr",
                           #point towards North Pole
                           which_north = "true",
                           pad_y = unit(0.5, "in"),
                           style = north_arrow_fancy_orienteering
                           ) +
  #markers
  geom_point(data = markers, aes(x=x, y=y, shape = marker)) +
  scale_shape_manual(values=c(12, 1, 2)) +
  #make longer legend text appear at top
  guides(col = guide_colorbar(order = 2), 
              shape = guide_legend(order = 1)
         ) +

  theme_bw() +
  #avoid overlapping x-axis labels
  #scale_x_continuous( n.breaks = 4) +
  theme(
    legend.justification=c(0,1), #legend.justification=c(1,0), 
    legend.position=c(0,1),  #legend.position=c(1,0), 
    legend.background =  element_blank()
    ) +
  labs(title = "BC",
       x = "Longitude",
       y = "Latitude",
       shape = ""
       )

# bc_p
 
```

```{r, fig.height=8}
#plot UFP & BC together
gridExtra::grid.arrange(ufp_p, bc_p, 
                        nrow=1,
                        top = "Degree of Kriging in the UK Predictions"
                        )
```



Scatterplots to compare UFP vs BC predictions in study area.

```{r}

p_study_ufp <- p_study_ufp + labs(title = "")
p_study_bc <- p_study_bc + labs(title = "")

ggarrange(p_study_ufp, p_study_bc, 
          labels = c("UFP", "BC"), 
          common.legend = T, legend = "bottom") %>%
  annotate_figure(top = "UK vs regression UFP (pt/cm3) & BC predictions (ng/m3) from the primary analysis for ACT locations")

```

## Predictions for ACT locations  

```{r}
# combine all results 
uk_predictions_df <- cov_act_all %>%
  filter(site_id %in% study_ids) %>%
  select(site_id, latitude, longitude, uk_names) %>% 
  gather("Analysis", "prediction", contains("uk")) %>%
  mutate(Analysis = str_replace(Analysis, "_uk", ""))

# sensitivity for ACT locations in monitoring area
uk_predictions_df_monitoring <- cov_act_all %>%
  filter(site_id %in% monitoring_ids) %>%
  select(site_id, latitude, longitude, contains("primary_uk")) %>% 
  gather("Analysis", "prediction", contains("uk")) %>%
  mutate(Analysis = str_replace(Analysis, "primary_uk", "monitoring_area"))

uk_predictions_df_l <- rbind(uk_predictions_df,
                             uk_predictions_df_monitoring
                             )
  
  
  # uk_predictions_df %>% 
  # gather("Analysis", "prediction", contains("uk")) %>%
  # mutate(Analysis = str_replace(Analysis, "_uk", ""))

```

Predictions are overall similar for primary and sensitivity analyses of both annual average BC and UFP concentrations. 

- Similar results across these analyses indicate that model predictions are robust and simply an artifact of our modeling decisions.

```{r}
# table 
uk_t <- uk_predictions_df_l %>%
  label_analysis(var = "Analysis") %>%
  
  #check that this works
  #mutate(Pollutant = gsub("ug/m3|ng/m3", "", Pollutant)) %>%  
  
  group_by(Pollutant, Analysis) %>%
  distribution.table(var.string = "prediction") %>%
  ungroup()

uk_t %>%
  select(-Pollutant) %>%
  kable(caption = "UFP (pt/cm3) and BC (ng/m3) UK model predictions for ACT locations from primary and sensitivity analyses") %>%
  pack_rows("UFP (pt/cm3)", 1, 8) %>%
  pack_rows("BC (ng/m3)", 9, nrow(uk_t)) %>%
  kable_styling()

# ## UFP
# uk_t %>%
#   filter(grepl("UFP", Pollutant)) %>%  
#   select(-Pollutant) %>%
#   kable(caption = "UFP (pt/cm3) UK model predictions for ACT locations from primary and sensitivity analyses") %>%
#   kable_styling()
# 
# # BC  
# uk_t %>%
#   filter(grepl("BC", Pollutant)) %>%  
#   select(-Pollutant) %>%
#   kable(caption = "BC (ng/m3) UK model predictions for ACT locations from primary and sensitivity analyses") %>%
#   kable_styling()

```

```{r}
# density plot
uk_predictions_df_l %>%
  label_analysis(var = "Analysis") %>%
  ggplot(aes(x=prediction, fill = Analysis)) + 
  geom_density(alpha = 0.2) + 
  facet_wrap(~Pollutant, scales= "free") +
  labs(title = "Distribution of UK predictions for ACT locations from primary and sensitivity analyses",
       x = "UK Prediction") + 
  theme(legend.position = "bottom")
   
```

- UFP 

Scatterplots around 1-1 line show similar predictions for the primary analysis and sensitivity analysis. Predictions from the windsorized analysis were generally higher, while predictions from the 10% trimmed analysis were generally lower than the primary analysis. Some low concentration predictions from the primary analysis were much lower from the native scale analysis.

```{r}
my_pollutant <- "ufp"
max_ufp_plot <- max(uk_predictions_df_l$prediction[grepl(my_pollutant, uk_predictions_df_l$Analysis)])
min_ufp_plot <- min(uk_predictions_df_l$prediction[grepl(my_pollutant, uk_predictions_df_l$Analysis)])
  
uk_predictions_df %>%
  spread(., Analysis, prediction) %>%
  gather("Analysis", "prediction", contains("ufp"), -contains("primary")) %>%
    label_analysis(var = "Analysis") %>%
                # update var for other pollutants here
  ggplot(aes(x = ufp_primary, y = prediction, col = Analysis)) + 
  geom_abline(intercept = 0, slope = 1) + 
  geom_point(alpha = 0.1) + 
  geom_smooth() +
  xlim(min_ufp_plot, max_ufp_plot) +
  ylim(min_ufp_plot, max_ufp_plot) +
  labs(x = "Primary Analysis",
       y = "Sensitivity Analysis",
       title = "UFP predictions (pt/cm3) from different UK models") 
  
```

- BC

Similarly, the windsorized analysis resulted in some higher predictions than the primary analysis, while the opposite was true for the native scale analysis.

```{r}
my_pollutant <- "bc"
max_ufp_plot <- max(uk_predictions_df_l$prediction[grepl(my_pollutant, uk_predictions_df_l$Analysis)])
min_ufp_plot <- min(uk_predictions_df_l$prediction[grepl(my_pollutant, uk_predictions_df_l$Analysis)])
  
uk_predictions_df %>%
   spread(., Analysis, prediction) %>%
  gather("Analysis", "prediction", contains("bc"), -contains("primary")) %>%
    label_analysis(var = "Analysis") %>%
                # update var for other pollutants here
  ggplot(aes(x = bc_primary, y = prediction, col = Analysis)) + 
  geom_abline(intercept = 0, slope = 1) + 
  geom_point(alpha = 0.1) + 
  geom_smooth() +
  xlim(min_ufp_plot, max_ufp_plot) +
  ylim(min_ufp_plot, max_ufp_plot) +
  labs(x = "Primary Analysis",
       y = "Sensitivity Analysis",
       title = "BC predictions (ng/m3) from different UK models") 
  
```

# UK Predictions on a grid

```{r, results = "hide"}
# make predictions on the grid from all analyses 

 # empty columns to save predictions
grid_covars[,uk_names] <- NA
 
for (i in seq_along(analysis_names)) {
  #i=1
  # validation results
  pls_components <- cv_results$PLS_Components[cv_results$Analysis==analysis_names[i]]
  pls_variogram_dist <- cv_results$Variogram_Distance_Fraction[cv_results$Analysis==analysis_names[i]]
  
  if(!grepl("native_scale", analysis_names[i])) {
    
    df <-  uk_predictions(dt = annual,
                     cov_loc_new = grid_covars,
                      y_name = analysis_names[i],
                      cov_names. = cov_names_log,
                      pls_comp = pls_components, 
                      variogram_dist_fract = pls_variogram_dist)
    
    # save predictions in native scale
    grid_covars[uk_names[i]] <- exp(df$dt$uk_pred)
    }

  #native scale analyses
    if(grepl("native_scale", analysis_names[i])) {
      
      df <-  uk_predictions(dt = annual,
                     cov_loc_new = grid_covars,
                      y_name = analysis_names[i],
                      cov_names. = cov_names_native,
                      pls_comp = pls_components, 
                      variogram_dist_fract = pls_variogram_dist)
      
      grid_covars[uk_names[i]] <- df$dt$uk_pred
    }
}

```

```{r}
#repeate above for small grid

# empty columns to save predictions
small_grid_covars[,uk_names] <- NA
 
for (i in seq_along(analysis_names)) {
  #i=1
  # validation results
  pls_components <- cv_results$PLS_Components[cv_results$Analysis==analysis_names[i]]
  pls_variogram_dist <- cv_results$Variogram_Distance_Fraction[cv_results$Analysis==analysis_names[i]]
  
  if(!grepl("native_scale", analysis_names[i])) {
    
    df <-  uk_predictions(dt = annual,
                     cov_loc_new = small_grid_covars,
                      y_name = analysis_names[i],
                      cov_names. = cov_names_log,
                      pls_comp = pls_components, 
                      variogram_dist_fract = pls_variogram_dist)
    
    # save predictions in native scale
    small_grid_covars[uk_names[i]] <- exp(df$dt$uk_pred)
    }

  #native scale analyses
    if(grepl("native_scale", analysis_names[i])) {
      
      df <-  uk_predictions(dt = annual,
                     cov_loc_new = small_grid_covars,
                      y_name = analysis_names[i],
                      cov_names. = cov_names_native,
                      pls_comp = pls_components, 
                      variogram_dist_fract = pls_variogram_dist)
      
      small_grid_covars[uk_names[i]] <- df$dt$uk_pred
    }
}



```



```{r, results = "hide", eval=F}
# predictions for KAYA's grid

# make predictions on the grid from all analyses 

 # empty columns to save predictions
kaya_grid[,uk_names] <- NA
 
for (i in seq_along(analysis_names)) {
  #i=1
  # validation results
  pls_components <- cv_results$PLS_Components[cv_results$Analysis==analysis_names[i]]
  pls_variogram_dist <- cv_results$Variogram_Distance_Fraction[cv_results$Analysis==analysis_names[i]]
  
  if(!grepl("native_scale", analysis_names[i])) {
    
    df <-  uk_predictions(dt = annual,
                     cov_loc_new = kaya_grid,
                      y_name = analysis_names[i],
                      cov_names. = cov_names_log,
                      pls_comp = pls_components, 
                      variogram_dist_fract = pls_variogram_dist)
    
    # save predictions in native scale
    kaya_grid[uk_names[i]] <- exp(df$dt$uk_pred)
    }

  #native scale analyses
    if(grepl("native_scale", analysis_names[i])) {
      
      df <-  uk_predictions(dt = annual,
                     cov_loc_new = kaya_grid,
                      y_name = analysis_names[i],
                      cov_names. = cov_names_native,
                      pls_comp = pls_components, 
                      variogram_dist_fract = pls_variogram_dist)
      
      kaya_grid[uk_names[i]] <- df$dt$uk_pred
    }
}


```




## Predictions

- UFP 

We see higher predicted UFP concentrations between downtown and the airport as well as along major highways. Some possible explanations for these hotspots are:

* elevated concentrations are seen in Seattle's industrial district where many industrial operations exist, including cargo handling and storage, ship and boat manufacturing, concrete manufacturing, and paper and metal fabrications. It is bounded by, among other features, the Duwamish Waterway, I-5, railroads from the two largest freight-hauling railroads in the word (BNSF Railway and Union Pacific Railroad) and the International District, all of which contain large emission sources. I-5, for example, has busy truck routes in this area (e.g., FGTS T-1 class roads - carry more tonage) (Schulte 2015 DEEDS study)

* Elevated concentrations between downtoan and northwest of the airport may also be due to predominant winds from the south and southwest as well as airport landing pattern (MOV-UP report Fig 11).


```{r}
#make long format for plotting

grid_covars_l <- grid_covars %>%
  select(native_id, latitude, longitude, contains(c("ufp", "bc"))) %>%
  gather("Analysis", "Prediction", contains(c("ufp", "bc"))) %>%
  label_analysis(var="Analysis", end_character = 3) 
   
#small grid
small_grid_covars_l <- small_grid_covars %>%
  select(native_id, latitude, longitude, contains(c("ufp", "bc"))) %>%
  gather("Analysis", "Prediction", contains(c("ufp", "bc"))) %>%
  label_analysis(var="Analysis", end_character = 3) 


```

Primary UFP & BC predictions  

```{r, fig.height=8}
pt_size <- 2.5
a <- 0.8

bc_p <- map0 +
  geom_point(data = subset(grid_covars_l, 
                           subset = grepl("BC", Pollutant) &
                             grepl("Primary", Analysis)
                             ),
             aes(x=longitude, y=latitude, col=Prediction),
             alpha=a,
             size=pt_size
             ) + 
  scale_color_gradient(name = "BC (ng/m3)", 
                        low = "yellow", high = "red",
                        ) +
  # add scale & N arrow to top left
  geom_sf(data = grid_covars_shp, inherit.aes = FALSE,
          #don't actually show dots
          alpha=0
          ) +
    annotation_scale(data = grid_covars_shp, location = "tr") +
    annotation_north_arrow(location = "tr",
                           #point towards North Pole
                           which_north = "true",
                           pad_y = unit(0.5, "in"),
                           style = north_arrow_fancy_orienteering
                           ) +
  #markers
  geom_point(data = markers, aes(x=x, y=y, shape = marker)) +
  scale_shape_manual(values=c(12, 1, 2)) +
  #make longer legend text appear at top
  guides(col = guide_colorbar(order = 2), 
              shape = guide_legend(order = 1)
         ) +
  #scale_x_continuous(n.breaks = 4) +
  theme_bw() +
  theme(
    legend.justification=c(0,1), #legend.justification=c(1,0), 
    legend.position=c(0,1),  #legend.position=c(1,0), 
    legend.background =  element_blank()
    ) +
  labs(title = "BC",
       x = "Longitude",
       y = "Latitude",
       shape = ""
       )

# bc_p


ufp_p <- map0 +
  geom_point(data = subset(grid_covars_l, 
                           subset = grepl("UFP", Pollutant) &
                             grepl("Primary", Analysis)
                             ),
             aes(x=longitude, y=latitude, col=Prediction),
             alpha=a,
             size=pt_size
             ) + 
  #make plot same dimensions as BC 
  geom_sf(data = grid_covars_shp, inherit.aes = FALSE,
          #don't actually show dots
          alpha=0
          ) + 
  scale_color_gradient(name = "UFP (pt/cm3)", 
                        low = "yellow", high = "red",
                        ) +
  #markers
  geom_point(data = markers, aes(x=x, y=y, shape = marker)) +
  scale_shape_manual(values=c(12, 1, 2)) +
  #make longer legend text appear at top
  guides(col = guide_colorbar(order = 2), 
              shape = guide_legend(order = 1)
         ) +
  #scale_x_continuous(n.breaks = 4) +
  theme_bw() +
  theme(
    legend.justification=c(0,1), 
    legend.position=c(0,1), 
    legend.background =  element_blank()
    ) +
  labs(title = "UFP",
       x = "Longitude",
       y = "Latitude",
       shape = ""
       )

#ufp_p
 
gridExtra::grid.arrange(ufp_p,bc_p, 
                        nrow=1,
                        top = "Grid predictions from primary models"
                        )

```


### --> reduce small grid extent to update the scale & help w/ spatial contrast? 

### --> add other markers?
### --> change eval=F

```{r, eval=F}
pt_size <- 3
a <- 1 #.8 #0.8

# xlim = c(-122.335, -122.32)
# ylim = c(47.515, 47.53)
 
bc_p <- small_map0 +
  geom_point(data = subset(small_grid_covars_l, 
                           subset = grepl("BC", Pollutant) &
                             grepl("Primary", Analysis) &
                             longitude >=-122.335 & longitude <=-122.32 &
                             latitude >= 47.515 & latitude <= 47.53
                             
                             ),
             aes(x=longitude, y=latitude, col=Prediction),
             alpha=a,
             size=pt_size
             ) + 
  scale_color_gradient(name = "BC (ng/m3)", 
                        low = "yellow", high = "red",
                        ) +
  # add scale & N arrow to top left
  geom_sf(data = small_grid_covars_shp, inherit.aes = FALSE,
          #don't actually show dots
          alpha=0
          ) +
    annotation_scale(data = small_grid_covars_shp, location = "tr") +
    annotation_north_arrow(location = "tr",
                           #point towards North Pole
                           which_north = "true",
                           pad_y = unit(0.5, "in"),
                           style = north_arrow_fancy_orienteering
                           ) +
  
  # #markers
  # geom_point(data = markers, aes(x=x, y=y, shape = marker)) +
  # scale_shape_manual(values=c(12, 1, 2)) +
  # #make longer legend text appear at top
  # guides(col = guide_colorbar(order = 2), 
  #             shape = guide_legend(order = 1)
  #        ) +
  
  #scale_x_continuous(n.breaks = 4) +
  
  theme_bw() +
  theme(
    legend.justification=c(0,1), #legend.justification=c(1,0), 
    legend.position=c(0,1),  #legend.position=c(1,0), 
    legend.background =  element_blank()
    ) +
  labs(title = "BC",
       x = "Longitude",
       y = "Latitude",
       shape = ""
       )  + 
  coord_sf(
    xlim = c(-122.335, -122.32),
    ylim = c(47.515, 47.53) #, expand = FALSE
    )

 bc_p



 
 ufp_p <- small_map0 +
  geom_point(data = subset(small_grid_covars_l, 
                           subset = grepl("UFP", Pollutant) &
                             grepl("Primary", Analysis)
                             ),
             aes(x=longitude, y=latitude, col=Prediction),
             alpha=a,
             size=pt_size
             ) + 
  #make plot same dimensions as BC 
  geom_sf(data = small_grid_covars_shp, inherit.aes = FALSE,
          #don't actually show dots
          alpha=0
          ) + 
  scale_color_gradient(name = "UFP (pt/cm3)", 
                        low = "yellow", high = "red",
                        ) +
  
  # #markers
  # geom_point(data = markers, aes(x=x, y=y, shape = marker)) +
  # scale_shape_manual(values=c(12, 1, 2)) +
  # #make longer legend text appear at top
  # guides(col = guide_colorbar(order = 2), 
  #             shape = guide_legend(order = 1)
  #        ) +
  
  #scale_x_continuous(n.breaks = 4) +
  theme_bw() +
  theme(
    legend.justification=c(0,1), 
    legend.position=c(0,1), 
    legend.background =  element_blank()
    ) +
  labs(title = "UFP",
       x = "Longitude",
       y = "Latitude",
       shape = ""
       )

#ufp_p
 
gridExtra::grid.arrange(ufp_p,bc_p, 
                        nrow=1,
                        top = "Small grid predictions from primary models"
                        )


```






UFP - primary & sensitivity 

```{r, fig.height=10}

map0 +
  geom_point(data = subset(grid_covars_l, 
                           subset = grepl("UFP", Pollutant)),
             aes(x=longitude, y=latitude, col=Prediction),
             #alpha=1,
             #size=0.6
             ) + 
  scale_color_gradient(name = "UFP\n(pt/cm3)", 
                        low = "yellow", high = "red",
                        ) +
  facet_wrap(~Analysis) +
  theme(
    legend.justification=c(1,0), 
    legend.position=c(1,0.1), 
    legend.background =  element_blank()
    ) +

  labs(title = "Annual average UFP grid predictions from different UK models"
       )

```


```{r,delete, fig.height=10}
# # dot map
# my_pollutant <- "ufp"
# pollutant_uk_names <- uk_names[grepl(my_pollutant, uk_names)]
# # map titles
# map_titles <- factor(substr(pollutant_uk_names, nchar(my_pollutant)+2, nchar(pollutant_uk_names)-nchar("_uk")))
# map_titles <- recode_factor(map_titles,
#                             "stop_means" = "Stop means",
#                             "trim10" = "10% trimmed mean",
#                             "windsorize" = "Windsorized mean",
#                             "uw" = "Unweighted mean",
#                             "native_scale" = "Native scale modeling")
# 
# dot_maps <-list()# min and max difference of all estimates
# 
# # min and max difference of all estimates
# plot_range <- grid_covars %>%
#   select(pollutant_uk_names) %>%
#   dplyr::summarize(min = min(.),
#                    max = max(.)) %>%
#   round()
# 
# for(i in seq_along(pollutant_uk_names)) {
#   #i=1    #used to be: uk_predictions
#    mymap <- suppressMessages(
#      grid_covars %>% map_fn(color_by = pollutant_uk_names[i], outline_points = F,
#                                    map_title = map_titles[i]) +
#     scale_color_gradient(name = "UFP (pt/cm3)", 
#                         low = "yellow", high = "red",
#                         #ensure that all plots on same scale
#                         limits = c(plot_range$min, plot_range$max)
#                         ))
#    
#    dot_maps[i] <- list(mymap)
#    names(dot_maps)[i] <- pollutant_uk_names[i]
#   }
# 
# ggarrange(plotlist = dot_maps,
#           #ncol = 3, #nrow = 2,
#           common.legend = T, legend = "right") %>%
#   annotate_figure(top = "Annual average UFP grid predictions from different UK models" ) 
   
```

- BC -  primary & sensitivity 

We see elevated predicted BC concentrations between the Port of Seattle and the SEA-TAC airport as well as along major highways. The contrasts betwen high and low concentration areas appear to be somewhat clearer for BC than for UFP. 

```{r, fig.height=10}

map0 +
  geom_point(data = subset(grid_covars_l, 
                           subset = grepl("BC", Pollutant)),
             aes(x=longitude, y=latitude, col=Prediction),
             #alpha=1,
             #size=0.8
             ) + 
  scale_color_gradient(name = "BC\n(ng/m3)", 
                        low = "yellow", high = "red",
                        ) +
  facet_wrap(~Analysis) + 
  theme(
    legend.justification=c(1,0), 
    legend.position=c(1,0.1), 
    legend.background =  element_blank()
    ) +
  labs(title = "Annual average BC grid predictions from different UK models"#,
       )
```


```{r,delete1, fig.height=10}
# # dot map
# my_pollutant <- "bc"
# pollutant_uk_names <- uk_names[grepl(my_pollutant, uk_names)]
# # map titles
# map_titles <- factor(substr(pollutant_uk_names, nchar(my_pollutant)+2, nchar(pollutant_uk_names)-nchar("_uk")))
# map_titles <- recode_factor(map_titles,
#                             "stop_means" = "Stop means",
#                             "trim10" = "10% trimmed mean",
#                             "windsorize" = "Windsorized mean",
#                             "uw" = "Unweighted mean",
#                             "native_scale" = "Native scale modeling")
# 
# dot_maps <-list()# min and max difference of all estimates
# 
# # min and max difference of all estimates
# plot_range <- grid_covars %>%
#   select(pollutant_uk_names) %>%
#   dplyr::summarize(min = min(.),
#                    max = max(.)) %>%
#   round()
# 
# for(i in seq_along(pollutant_uk_names)) {
#   #i=1    #used to be: uk_predictions
#    mymap <- suppressMessages(
#      grid_covars %>% map_fn(color_by = pollutant_uk_names[i], outline_points = F,
#                                    map_title = map_titles[i]) +
#     scale_color_gradient(name = "BC (ng/m3)", 
#                         low = "yellow", high = "red",
#                         #ensure that all plots on same scale
#                         limits = c(plot_range$min, plot_range$max)
#                         ))
#    
#    dot_maps[i] <- list(mymap)
#    names(dot_maps)[i] <- pollutant_uk_names[i]
#   }
# 
# ggarrange(plotlist = dot_maps,
#           #ncol = 3, #nrow = 2,
#           common.legend = T, legend = "right") %>%
#   annotate_figure(top = "Annual average BC grid predictions from different UK models" ) 
#    
```


- Kaya's grid (check that looks ok)

```{r, eval=F}

# dot map
my_pollutant <- "ufp"
pollutant_uk_names <- uk_names[grepl(my_pollutant, uk_names)]
# map titles
map_titles <- factor(substr(pollutant_uk_names, nchar(my_pollutant)+2, nchar(pollutant_uk_names)-nchar("_uk")))
map_titles <- recode_factor(map_titles,
                            "stop_means" = "Stop means",
                            "trim10" = "10% trimmed mean",
                            "windsorize" = "Windsorized mean",
                            "uw" = "Unweighted mean",
                            "native_scale" = "Native scale modeling")

dot_maps <-list()# min and max difference of all estimates

# min and max difference of all estimates
plot_range <- kaya_grid %>%
  select(pollutant_uk_names) %>%
  dplyr::summarize(min = min(.),
                   max = max(.)) %>%
  round()

for(i in seq_along(pollutant_uk_names)) {
  #i=1    #used to be: uk_predictions
   mymap <- suppressMessages(
     kaya_grid %>% map_fn(color_by = pollutant_uk_names[i], outline_points = F,
                                   map_title = map_titles[i]) +
    scale_color_gradient(name = "UFP (pt/cm3)", 
                        low = "yellow", high = "red",
                        #ensure that all plots on same scale
                        limits = c(plot_range$min, plot_range$max)
                        ))
   
   dot_maps[i] <- list(mymap)
   names(dot_maps)[i] <- pollutant_uk_names[i]
  }

dot_maps[1]

# ggarrange(plotlist = dot_maps,
#           #ncol = 3, #nrow = 2,
#           common.legend = T, legend = "right") %>%
#   annotate_figure(top = "Kaya - Annual average UFP grid predictions from different UK models" ) 
   
```


## Prediction differences 

### Prediction differences for ACT cohort locations in the study area 

```{r}
# calculate difference between sensitivity & primary estimates
pred_differences <- uk_predictions_df %>%
  spread(., Analysis, prediction) %>%
  #gather("Analysis", "prediction", contains("ufp"), -contains("primary")) %>%
  mutate_at(vars(contains("ufp"), -contains("primary")), ~.-ufp_primary) %>%
  mutate_at(vars(contains("bc"), -contains("primary")), ~.-bc_primary) %>% 
  select(-contains("primary"))

```

Compared to predictions from the primary analysis, predictions from the windsorized and stop means analysis tend to be higher, while predictions from the 10% trimmed analysis tend to be lower for both UFPs and BC. 

-Annual average predictions for both UFP and BC tend to be higher for analyses that keep higher stop concentrations (e.g., windsorized analyses) and lower for those that drop high stop concentraions (e.g., 10% trimmed analysis).

```{r}
#density plot
pred_differences %>% 
  gather("Analysis", "prediction_diff", contains(c("bc", "ufp"))) %>%
  label_analysis(var = "Analysis") %>%
  ggplot(aes(x = prediction_diff, fill = Analysis)) +
  geom_density(alpha = 0.3) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  facet_wrap(~Pollutant, scales="free") +
  labs(x = "Prediction difference",
       title = "Difference in annual average predictions relative to the primary analysis"
       ) +
  theme(legend.position = "bottom")

```

```{r}
# distribution table
pred_differences %>% 
  gather("Analysis", "prediction_diff", contains(c("bc", "ufp"))) %>%
    label_analysis(var = "Analysis") %>%
  group_by(Pollutant, Analysis) %>%
  distribution.table(var.string = "prediction_diff") %>%
  kable(caption = "Difference in annual average pollutant predictions relative to primary UK model") %>%
  kable_styling()

```

### Prediction differences on a grid

```{r}
# grid: calculate difference between sensitivity & primary estimates
pred_differences_grid <- grid_covars %>%
  # #drop points not in study area
  # filter(in_study_area == TRUE,
  #        not_in_water == TRUE
  #        ) %>%
  select(site_id, latitude, longitude, contains(analysis_names)) %>%
    mutate_at(vars(contains("ufp"), -contains("primary")), ~.-ufp_primary_uk) %>%
  mutate_at(vars(contains("bc"), -contains("primary")), ~.-bc_primary_uk) %>% 
  select(-contains("primary"))

# make long format for plotting
pred_differences_grid_l <- pred_differences_grid %>%
  gather("Analysis", "Diff", contains(c("ufp", "bc"))) %>%
  label_analysis(var="Analysis", end_character = 3) 

```

- UFP

Compared to UFP predictions from the primary analysis, predictions from the stop means analysis are slightly higher, generally along I-5. 

The opposite is true for the 10% trimmed analysis. 

Windosrized predictions tend to be higher throughout the study area, with a special emphasis near the airport. Windosorized predictiosn are lower near I-5 on the southern end of the study area. 

Unweighted average stop predictions are ___________

Native scale predictions are generally higher along major highways and lower on the east side of the study area where concentration predictions from the primary analysis were already low.


```{r, fig.height=10}
map0 +
  geom_point(data = subset(pred_differences_grid_l, 
                           subset = grepl("UFP", Pollutant)),
             aes(x=longitude, y=latitude, col=Diff),
             #size=0.8
             ) + 
   scale_color_gradient2(name = "UFP Diff\n(pt/cm3)", 
                        low = "black", mid= "white", high = "red",
                        midpoint = 0
                        ) +  
  facet_wrap(~Analysis) + 
  labs(title = "Difference in annual average UFP predictions relative to primary UK model")


```


```{r, delete2, fig.height=8}
# # maps 
# my_pollutant <- "ufp"
# 
# # min and max difference of all estimates
# diff_range <- pred_differences_grid %>%
#   select(contains(my_pollutant)) %>%
#   dplyr::summarize(min = min(.),
#                    max = max(.)) %>%
#   round()
# 
# uk_names_diff <- names(pred_differences_grid)[grepl(my_pollutant, names(pred_differences_grid))]
# # map titles
# map_titles <- factor(substr(uk_names_diff, nchar(my_pollutant)+2, nchar(uk_names_diff)-nchar("_uk")))
# map_titles <- recode_factor(map_titles,
#                             "stop_means" = "Stop means",
#                             "trim10" = "10% trimmed mean",
#                             "windsorize" = "Windsorized mean",
#                             "uw" = "Unweighted mean",
#                             "native_scale" = "Native scale modeling")
# 
# difference_maps <-list()
# 
# for(i in seq_along(uk_names_diff)) {
#   #i=1
#   mymap <- suppressMessages(
#      pred_differences_grid %>% map_fn(color_by = uk_names_diff[i], outline_points = F,
#                                    map_title = map_titles[i]) +
#        scale_color_gradient2(name = "UFP (pt/cm3)", 
#                         low = "black", mid= "white", high = "red",
#                         midpoint = 0, 
#                         limits = c(diff_range$min, diff_range$max))  
#      
#   )
#   
#   difference_maps[i] <- list(mymap)
#   
#   names(difference_maps)[i] <- uk_names_diff[i]
#   
# }
# 
# 
# 
# ggarrange(plotlist = difference_maps,
#           #ncol = 3, nrow = 2,
#           common.legend = T, legend = "right") %>%
#   annotate_figure(top = "Difference in annual average UFP predictions relative to primary UK model" ) 
 
```

- BC

Compared to BC predictions from the primary analysis, predictions from the stop means analysis are slightly higher near the water, but lower along major highways.

Predictions from the the 10% trimmed analysis were somewhat lower. 

Windosrized predictions were higher throughout the study area, particulalry in the Seattle area and south of the city along I-5.

Unweighted average stop predictions were similar. 

Native scale predictions were more extreme, with both higher and lower predictions throughout the study area.

```{r, delete3, fig.height=8}
# # maps 
# my_pollutant <- "bc"
# 
# # min and max difference of all estimates
# diff_range <- pred_differences_grid %>%
#   select(contains(my_pollutant)) %>%
#   dplyr::summarize(min = min(.),
#                    max = max(.)) %>%
#   round()
# 
# uk_names_diff <- names(pred_differences_grid)[grepl(my_pollutant, names(pred_differences_grid))]
# # map titles
# map_titles <- factor(substr(uk_names_diff, nchar(my_pollutant)+2, nchar(uk_names_diff)-nchar("_uk")))
# map_titles <- recode_factor(map_titles,
#                             "stop_means" = "Stop means",
#                             "trim10" = "10% trimmed mean",
#                             "windsorize" = "Windsorized mean",
#                             "uw" = "Unweighted mean",
#                             "native_scale" = "Native scale modeling")
# 
# difference_maps <-list()
# 
# for(i in seq_along(uk_names_diff)) {
#   #i=2
#   mymap <- suppressMessages(
#      pred_differences_grid %>% 
#        map_fn(color_by = uk_names_diff[i], outline_points = F,
#                                    map_title = map_titles[i]) +
#        scale_color_gradient2(name = "BC (ng/m3)", 
#                         low = "black", mid= "white", high = "red",
#                         midpoint = 0, 
#                         limits = c(diff_range$min, diff_range$max)
#                         )  
#   )
#   
#   difference_maps[i] <- list(mymap)
#   names(difference_maps)[i] <- uk_names_diff[i]
# }
# 
# ggarrange(plotlist = difference_maps,
#           #ncol = 3, nrow = 2,
#           common.legend = T, legend = "right") %>%
#   annotate_figure(top = "Difference in annual average BC predictions relative to primary UK model" ) 
 
```

```{r, fig.height=10}
map0 +
  geom_point(data = subset(pred_differences_grid_l, 
                           subset = grepl("BC", Pollutant)),
             aes(x=longitude, y=latitude, col=Diff),
             #size=0.8
             ) + 
   scale_color_gradient2(name = "BC Diff\n(ng/m3)", 
                        low = "black", mid= "white", high = "red",
                        midpoint = 0
                        ) +  
  facet_wrap(~Analysis) + 
  labs(title = "Difference in annual average BC predictions relative to primary UK model")

```



# Additional   
## Covariates most associated with UFP 

Using Lasso regression to select the covarites most associated with annual average mobile monitoring estimates from the primary analysis.

- UFP

```{r, fig.height=8}
# find covariates associated with annual average UFP from mobile monitoring observations
act_cov_lasso <- lasso_fn(dt = annual, x_names = cov_names_log, y_name = "ufp_primary", 
              lambda. = .01
              )

selected_cov <- act_cov_lasso$results$cov
  
# plot 
annual %>%
  gather("cov", "value", selected_cov) %>%
  ggplot(aes(x=value, y=ufp_primary)) + 
  geom_point(aes(col=cov), alpha=0.5) +
  geom_smooth() +
  facet_wrap(~cov, scales = "free_x") + 
  theme(legend.position = "none") + 
  labs(x = "",
       y = "Estimated annual average UFP (pt/cm3)",
       title = "Annual average mobile monitoring estimates of UFP and associated geocovariates"
       ) 
```
 
- BC

```{r, fig.height=8}
# find covariates associated with annual average UFP from mobile monitoring observations
act_cov_lasso <- lasso_fn(dt = annual, x_names = cov_names_log, y_name = "bc_primary", 
              lambda. = .01
              )

selected_cov <- act_cov_lasso$results$cov
  
# plot 
annual %>%
  gather("cov", "value", selected_cov) %>%
  ggplot(aes(x=value, y=ufp_primary)) + 
  geom_point(aes(col=cov), alpha=0.5) +
  geom_smooth() +
  facet_wrap(~cov, scales = "free_x") + 
  theme(legend.position = "none") + 
  labs(x = "",
       y = "Estimated annual average BC (ng/m3)",
       title = "Annual average mobile monitoring estimates of BC and associated geocovariates"
)
```






```{r}
# # save datasets.

# # UK predictions from primary and sensitivity analyses 
# ## for ACT cohort locations in the study area 
# saveRDS(uk_predictions_df, file.path("Data", "Aim 2", "Predictions", "uk_predictions.rda"))
# saveRDS(pred_differences, file.path("Data", "Aim 2", "Predictions", "uk_prediction_diffs.rda"))
# 
# ## on a grid
# saveRDS(grid_covars[, c("site_id", "latitude", "longitude", uk_names)], file.path("Data", "Aim 2", "Predictions", "grid_uk_predictions.rda"))
# saveRDS(small_grid_covars[, c("site_id", "latitude", "longitude", uk_names)], file.path("Data", "Aim 2", "Predictions", "small_grid_uk_predictions.rda"))


# saveRDS(pred_differences_grid, file.path("Data", "Aim 2", "Predictions", "grid_uk_prediction_diffs.rda"))
#
# ### as CSV
# write.csv(grid_covars[, c("site_id", "latitude", "longitude", uk_names)],
#           file.path("Data", "Aim 2", "Predictions", "grid_uk_predictions.csv"))
# write.csv(small_grid_covars[, c("site_id", "latitude", "longitude", uk_names)],
#           file.path("Data", "Aim 2", "Predictions", "small_grid_uk_predictions.csv"))
#
#kaya
# write.csv(kaya_grid[, c("site_id", "latitude", "longitude", uk_names, "in_monitoring_area", "in_study_area")],
#           file.path("Data", "Kaya", "kaya_grid_uk_predictions.csv"))

```


\newpage
# Code

```{r,ref.label=knitr::all_labels(),echo=TRUE,eval=FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=60), include=T}
```
 