---
title: "Aim 2: UK"
author: "Magali Blanco"
date: '`r Sys.Date()`'
output:
  html_document:
    number_sections: yes
    df_print: paged
    toc: yes
    toc_depth: '3'
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: 3
editor_options:
  chunk_output_type: console
---

```{r}
# --> to do
 

```

# Notes





```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, 
                      cache=T, cache.comments = F, 
                      message = F, warning = F, 
                      tidy.opts=list(width.cutoff=60), tidy=TRUE,
                      fig.height = 6, fig.width = 8
                      )  

# Clear workspace of all objects and unload all extra (non-base) packages
rm(list = ls(all = TRUE))
if (!is.null(sessionInfo()$otherPkgs)) {
  res <- suppressWarnings(
    lapply(paste('package:', names(sessionInfo()$otherPkgs), sep=""),
      detach, character.only=TRUE, unload=TRUE, force=TRUE))
}

pacman::p_load(knitr, kableExtra, 
               #descriptive statistics
               Hmisc, EnvStats, 
                  #qwraps2, #mean_sd, median_iqr
               # modeling
               pls, geoR, #gstat - alternative for UK
               akima, # interp() - interpolate predictions on map
               ggpubr, tidyverse #dplyr, 
               )    
#Himsc: describe(); EnvStats: summaryFull(); ggpubr: ggarrange()

options(knitr.kable.NA = '')
set.seed(1)
source("0.Global_Fns.R")
source("A2.0.1_Var&Fns.R")

images_path <- file.path(images_path0, "3. UK")
tables_path <- file.path(tables_path0, "3. UK")

#read in data
annual0 <- readRDS(file.path("Data", "Aim 2", "Mobile Monitoring", "annual_ufp_and_cov.rda"))  
          
loc_vars <- c("lambert_x", "lambert_y")

site_locations <- annual0 %>%
  select(site_id, loc_vars) #latitude, longitude

site_locations_stops <- site_locations$site_id[grepl("MS", site_locations$site_id)]
  
cov_names <- annual0 %>%
  select(elev_elevation:log_m_to_a23) %>%
  names()

cov_act <- readRDS(file = file.path("Data", "Aim 2", "Geocovariates", "cov_act_log_scale_preprocessed.rda"))

############################### SENSITIVITY ANALYSES ###############################

# native scale proximity covariates
annual_native_scale <- annual0 %>%
  select(-log_ufp) %>%
  mutate_at(vars(contains("log_")), ~exp(.)) %>%
  rename_at(vars(contains("log_")), ~substr(., 5, nchar(.)) )

cov_act_native_scale <- cov_act %>%
  mutate_at(vars(contains("log_")), ~exp(.)) %>%
  rename_at(vars(contains("log_")), ~substr(., 5, nchar(.)) )

cov_names_native <- annual_native_scale %>%
  select(elev_elevation:m_to_a23) %>%
  names()

# using stop means
stop_means <- readRDS(file.path("Data", "Aim 2", "Mobile Monitoring", "annual_ufp_stop_means.rda")) %>%
  # log-transform predictions (proximity covariates also in log format)
  mutate(log_ufp = log(yhat)) %>%
  # add stop location and geocovariates
  left_join(annual0[,c("site_id", loc_vars, cov_names)], by = "site_id") %>%
  drop_na()

# trim 10% top/bottom stop observations
trim10 <- readRDS(file.path("Data", "Aim 2", "Mobile Monitoring", "annual_ufp_trim10.rda")) %>%
  mutate(log_ufp = log(yhat)) %>%
  left_join(annual0[,c("site_id", loc_vars, cov_names)], by = "site_id")%>%
  drop_na()

# Windosrize extremes
windsorized <- readRDS(file.path("Data", "Aim 2", "Mobile Monitoring", "annual_ufp_windsorized.rda")) %>%
  mutate(log_ufp = log(yhat)) %>%
  left_join(annual0[,c("site_id", loc_vars, cov_names)], by = "site_id")%>%
  drop_na()

# different regression to estimate annual average
uw <- readRDS(file.path("Data", "Aim 2", "Mobile Monitoring", "annual_ufp_uw_means.rda")) %>%
  mutate(log_ufp = log(yhat)) %>%
  left_join(annual0[,c("site_id", loc_vars, cov_names)], by = "site_id")%>%
  drop_na()


```

# Approach    

1. use 10 folds to split up the data (annual averages and geocovariates for ~ 308 locations) into a training (90%) and validation (10%) dataset 
  
2. Run PLS on the training set to calculate the scores for various components (start with 1 component).

3. Fit UK models to the training data with the selected number of PLS components and variogram settings (e.g., max distance)

4. Repeat steps 2-3 nine more times to calculate cross-validated RMSE and R2

5. Repeat steps 2-4 with an increasing numbber of PLS components (e.g., 1-10) and for different variogram settings

6. Fit a final model with the selected parameters (number of PLS components, residual model) and all of the data.

7. Estimate the out-of-sample RMSE and R2 for AQS sites using the final UK model.

8. Predict at ACT participant locations.


# PLS to create summary features of geocovariates. 

Create training,  test and AQS set.

```{r, results="hide"}
#for creating sets in data & CV later 
k <- 10

# 1. fit PLS using training set to ID # features to use
annual_all <- annual0 %>%
  select(site_id,
         loc_vars,
         log_ufp,
         cov_names) %>%
  drop_na() %>%
  #create test, train, aqs data set
  mutate(set = ifelse(grepl("MC", site_id), "aqs",
                 sample(c(1:k), size = sum(grepl("MS", site_id)), replace = T)),
    cv_prediction = NA
         )

annual_aqs <- annual_all %>%
  #only keep aqs sites
  filter(set == "aqs")  

annual_train_test <- annual_all %>%
  #drop aqs sites
  filter(set  != "aqs")  

```

Determine what the best plotting distance is when a different number of PLS components are used.

plots of different variogram distances. Using all data.

```{r, results="hide"}
pdf(file.path(images_path, "variogram plots.pdf"))

use_n_scores <- 10

dist_fractions <- c(0.05,  seq(0.1, 0.5, by=0.1))  
#dist_fractions <- seq(0.01, 0.12, by=0.01)  

par(mfrow = c(3, 2))

for(i in seq_len(use_n_scores)) {
  #i=1
  score_n_names <- paste0("Comp", 1:i)
  
  #fit PLS. calling it "train", but using entire dataset
  pls_train_test <- plsr(log_ufp ~.,
               data=annual_train_test[,c("log_ufp", cov_names)], 
               ncomp = i,
               scale=T
               )
  
  # extract scores for UK
  scores_train_test <- scores(pls_train_test)[,c(1:i)] %>%
    as.data.frame()
  
  # take out spaces in names
  names(scores_train_test) <- score_n_names
  
  # dataset w/ UFP measurements, geocovariates, location
  pls_df_train_test <- cbind(
    annual_train_test, #[c("site_id", "log_ufp")],
    scores_train_test
  ) #%>% left_join(site_locations, by = "site_id") 

  ################################ UK ################################
   geo_train_test <- as.geodata(pls_df_train_test, 
                        coords.col = c("lambert_x", "lambert_y"), 
                        data.col = "log_ufp", 
                        covar.col = score_n_names)

  ##trend
  cov_trend <-  as.formula(paste0("~ ", paste0(score_n_names, collapse = " + " )))
  
  #plotting distances
  max.dist <- summary(geo_train_test)$distances.summary[["max"]]
  ## plot nearby locations more finely - improves variogram model fit?
  brk_pt <- 1000
  by1_pt <- 300
  by2_pt <- 1000
  
  ############################ model residuals ######################################
  for(j in seq_along(dist_fractions)) {
    #j=1
    max.plot.dist <- max.dist*dist_fractions[j]

  
  ##Empirical Variogram
  variog_train_test <- variog(geo_train_test,
                         #create equally spaced bins for all distances plotted
                         uvec=c(seq(0, brk_pt, by = by1_pt), seq((brk_pt + by2_pt), max.plot.dist, by= by2_pt)),
                        #UK
                        trend = cov_trend, 
                        messages = F  
                        )

  #plot(variog_train)
  
  wls_ests_train_test <- variofit(variog_train_test, cov.model = "exp",
                                              messages = F)
  
  resid_model_train_test <- variofit(vario = variog_train_test, 
                      ini = wls_ests_train_test, 
                      cov.model = "exp",
                      #weights = "equal") #ols
                      weights = "npairs",
                      messages = F
                      ) #wls
  
   plot(variog_train_test, 
       main = paste0("Comp: ", i, ", Dist fract: ", dist_fractions[j]), 
       cex.main=0.8
         )
  lines(variog_train_test)
  lines(resid_model_train_test, col=2)

  }
  
}

dev.off()

```

CV to evalute best number of PLS components. Folds (k) = `r k`

```{r, results="hide"}
set.seed(1)
################# variables for later ###############
# max PLS components to use in UK
use_n_scores <- 5 

dist_fract <-  c(0.05, seq(0.1, 0.4, by=0.1))  

#df to save CV RMSE
cv.eval <- data.frame(
  expand.grid(pls_comp = c(1:use_n_scores),
            dist_fract = dist_fract),
  RMSE = NA,
  R2 = NA
  )
#cv.eval <- cv.eval %>% left_join(variog_dist_to_plot)

################## CV loops ##################################
#estimate CV RMSE and r2 for diff number of PLS component combinations 
for(i in seq_len(use_n_scores)) {
  #i=1
  score_n_names <- paste0("Comp", 1:i)
  
  for(j in seq_along(dist_fract)) {
  #j=1
  #10FCV for each PLS component combination
  for(f in seq_len(k)) {
  #f=1
  train_grp <- annual_train_test$set != f
  
  annual_train <- annual_train_test %>%
    filter(train_grp)  
  
  annual_test <- annual_train_test %>%
    filter(!train_grp)   
  
  #fit PLS to training data
  pls_train <- plsr(log_ufp ~.,
               data=annual_train[,c("log_ufp", cov_names)], 
               ncomp = i,
               scale=T
                  )
  
  #pls_train$scores
  
  # extract scores for UK
  scores_train <- scores(pls_train)[,c(1:i)] %>%
    as.data.frame()
  scores_test <- predict(object = pls_train,
                         newdata = annual_test,
                         ncomp = 1:i,
                         type = "score"
                         ) %>%
    as.data.frame()
  
  # take out spaces in names
  names(scores_train) <- score_n_names
  names(scores_test) <- score_n_names
  
  # dataset w/ UFP measurements, geocovariates, location
  pls_df_train <- cbind(
    annual_train, #[c("site_id", "log_ufp")],
    scores_train) #%>% left_join(site_locations,by = "site_id") 
  
  pls_df_test <- cbind(
    annual_test, #[c("site_id", "log_ufp")],
    scores_test) #%>% left_join(site_locations,by = "site_id")

  ################################ UK ################################
   geo_train <- as.geodata(pls_df_train, 
                        coords.col = c("lambert_x", "lambert_y"), 
                        data.col = "log_ufp", 
                        covar.col = score_n_names)
  geo_test <- as.geodata(pls_df_test, 
                        coords.col = c("lambert_x", "lambert_y"), 
                        data.col = "log_ufp", 
                        covar.col = score_n_names)
  
  ##trend
  cov_trend <-  as.formula(paste0("~ ", paste0(score_n_names, collapse = " + " )))
  
  max.dist <- summary(geo_train)$distances.summary[["max"]]
 
  max.plot.dist <- max.dist*dist_fract[j] 

  ############################ model residuals ###################################### 
  ##Empirical Variogram
  brk_pt <- 1000
  by1_pt <- 300
  by2_pt <- 1000
  
  variog_train <- variog(geo_train,
                         #uvec=c(seq(0, brk_pt, by = by1_pt), seq((brk_pt + by2_pt), max.plot.dist, by= by2_pt)),
                         #uvec=seq(0, max.plot.dist, by = by2_pt),
                         uvec=c(seq(0, brk_pt, by = by1_pt), seq((brk_pt + by2_pt), max.plot.dist, by= by2_pt)),
                        #UK
                        trend = cov_trend, 
                        messages = F  
                        )

  #use geoR try to estimate intitial range & sill values. using WLS and an exponential fit
  wls_ests_train <- variofit(variog_train, cov.model = "exp", 
                             messages = F)
   
  # REML does slightly better in terms of RMSE and R2 when using fewer components (< ~10), otherwise does about the same. BUT variogram Looks worse than the rest. OLS seems to do better visually.
  
  # resid_model_train <- likfit(geodata = geo_train,
  #                      ini=wls_ests_train,
  #                      cov.model = "exp",
  #                      trend = cov_trend,
  #                      lik.method="REML") # "ML"
  
  resid_model_train <- variofit(vario = variog_train, 
                      ini = wls_ests_train, 
                      cov.model = "exp",
                      #weights = "equal", #ols
                      weights = "npairs",#wls
                      messages = F
                      ) 

  # plot(variog_train)
  # lines(variog_train)
  # lines(resid_model_train, col=2)
  
  #trend
    train_trend <- trend.spatial(trend = cov_trend, geo_train)
    test_trend <- trend.spatial(trend = cov_trend, geo_test)
    
    ############################# Predict #############################
    
    kc_cv <- krige.conv(coords = geo_train$coords,
                        data = geo_train$data,
                        locations = geo_test$coords,
                        krige = krige.control(type = "ok",
                                            obj.model = resid_model_train, 
                                              trend.d = train_trend,
                                              trend.l = test_trend
                                              ), 
                        )
    
    #save CV predictions temporarily  
    annual_train_test$cv_prediction[!train_grp] <- kc_cv$predict
  
  }
   # ? calculate performance statistics on native scale - what we care about? 
    cv.eval$RMSE[cv.eval$pls_comp== i & cv.eval$dist_fract==dist_fract[j]] <- rmse(obs = exp(annual_train_test$log_ufp), pred = exp(annual_train_test$cv_prediction))
   cv.eval$R2[cv.eval$pls_comp== i & cv.eval$dist_fract==dist_fract[j]] <-  r2_mse_based(obs = exp(annual_train_test$log_ufp), pred = exp(annual_train_test$cv_prediction))
    
    }

}

```

Performance statistics calculated for UFP on native scale (exp(log UFP)).

```{r}
######################### select CV parameters #########################
min_rmse <- min(cv.eval$RMSE)
rmse_inx <- cv.eval$RMSE == min_rmse

cv_r2 <- cv.eval$R2[rmse_inx] 
cv_comp <- cv.eval$pls_comp[rmse_inx]
cv_dist_fract <- cv.eval$dist_fract[rmse_inx]
  
cv_comp_names <- paste0("Comp", c(1:cv_comp)) 
 
#trends
cv_cov_trend <-  as.formula(paste0("~ ", paste0(cv_comp_names, collapse = " + " )))

######################## Plot CV RMSE & R2 ################################## 
   
fit_info <- paste0("Min RMSE: ", round(min_rmse, 0),
                   "\nR2: ", round(cv_r2, 2),
                   "\nPLS Components: ", cv_comp,
                   "\nFraction of maximum distance plotted in variogram: ", cv_dist_fract
                   )

cv.eval %>%
  gather("variable", "value", RMSE:R2) %>%
  ggplot(aes(x=pls_comp, y=value, 
             col=factor(dist_fract)
             )) + 
  geom_point(alpha=0.7) + 
  geom_line(alpha=0.7) +
  geom_vline(xintercept = cv_comp, 
             linetype = "dashed",
             alpha=0.5) +
  facet_wrap(~variable, 
             scales = "free", 
             ncol=1
             ) +
  scale_x_continuous(breaks= scales::pretty_breaks(n = cv_comp)) +
  labs(
    x = "PLS Components",
    col = "Fract of max dist\nplotted in variogram",
    title = "10FCV Model Performance",
    subtitle = fit_info,
    caption = "MSE-based R2 = max(0, 1 – MSE / Var(Y))"
  ) 

```


# Fit final UK model to all stop data    
* exclude AQS co-location sites

```{r}
############################# PLS #############################
# fit model to all data
pls_train_test <- plsr(log_ufp ~.,
             data=annual_train_test[,c("log_ufp", cov_names)], 
             ncomp = cv_comp,
             scale=T
                )

# extract scores for UK
scores_train_test <- scores(pls_train_test)[,c(1:cv_comp)] %>%
  as.data.frame()

# take out spaces in names
names(scores_train_test) <- cv_comp_names

```

Plot of PLS loadings. 

```{r, fig.height=15}
pls.loadings <- pls_train_test$loadings[] %>%
  as.data.frame() %>%
  rownames_to_column(var = "cov")

pls.loadings <- pls.loadings %>%
  # rename variables if buffers
  split_cov_name(cov = "cov")

my.alpha=0.3

pls.loadings.l  <- pls.loadings %>%
  #make long format for faceting
  gather(key = "Component", value = "Loading", contains("Comp")) %>%
  mutate(Component = as.numeric(substr(Component, 6, nchar(Component))))  
   
pls.loadings.l  %>%
  #drop these in first points
  drop_na(buffer) %>%
  ggplot(aes(x = Loading, y = cov)) +
  geom_point(aes(size=buffer),
             shape=1,
             alpha=my.alpha) +
  scale_size(breaks = c(min(pls.loadings$buffer, na.rm = T),
                        max(pls.loadings$buffer, na.rm = T)
                        )
             ) + #500, 5000, 10000,

  geom_point(data = pls.loadings.l[is.na(pls.loadings.l$buffer),],
           alpha=my.alpha,
           aes(shape="")) +
  geom_vline(xintercept=0,
             linetype="solid",
             alpha=my.alpha) +
    facet_wrap(~Component,
               labeller = "label_both",
               ncol = 2
               ) +
  labs(#x = paste0("Loading"),
       y = "Geocovariate",
       shape= "non-buffer", #"proximity,\nelevation",
       title = "PLS Geocovariate Component Loadings") +
  theme(legend.position = "bottom")


```

Geodataset.

```{r}

############################# UK #############################
# add site_id, ufp & lat/long
pls_df_train_test <- cbind(
  annual_train_test[c("site_id", "log_ufp")],
  scores_train_test
) %>%
  left_join(site_locations) 

# create geodata for test set
geo_train_test <- as.geodata(pls_df_train_test , 
                        coords.col = c("lambert_x", "lambert_y"), 
                        data.col = "log_ufp", 
                        covar.col = cv_comp_names)
# trend
trend_train_test <- trend.spatial(trend = cv_cov_trend,
                           geodata = geo_train_test)

# residual model

max.dist <- summary(geo_train_test)$distances.summary[["max"]]

```

Residual model.   

```{r, results="hide"} 
max.plot.dist <- max.dist*cv_dist_fract

##Empirical Variogram
variog_train_test <- variog(geo_train_test,
                       #uvec=seq(0, max.plot.dist, by = 1000),
                       max.dist=max.plot.dist,
                      #UK
                      trend = cv_cov_trend 
                      )
#plot(variog_train_test)

#estimate range & sill values. using WLS and an exponential fit
wls_ests <- variofit(vario = variog_train_test, cov.model = "exp")

# resid_model <- likfit(geodata = geo_train_test, 
#                            ini=wls_ests, 
#                            trend = cv_cov_trend,
#                            cov.model = "exp",
#                            #lik.method="ML") #ML
#                            lik.method="REML") # REML

# ols: equal; wls: npairs
resid_model <- variofit(vario = variog_train_test, 
                      ini=wls_ests, 
                      cov.model = "exp",
                      #weights="equal") #ols
                      weights = "npairs") #wls

# partial sill: sigma sq
# range: phi, 
# nugget: tau sq
resid_model.s <- summary(resid_model)
model_partial_sill <- resid_model.s$estimated.pars[["sigmasq"]]
model_range <- resid_model.s$estimated.pars[["phi"]]
model_nugget <- resid_model.s$estimated.pars[["tausq"]]

```

```{r}
#par(mfrow = c(1, 1))
plot(variog_train_test,
     main = paste0("Binned empirical and modeled variogram\nfor all stop data (excldues AQS sites). Using ", cv_comp, " PLS components"),
     xlab = "Distance (m)",
     #xlim=c(0,10000)
     )
lines(variog_train_test, lty=1)
lines(resid_model, lty=2, col=2)
legend("bottomright",
       legend = c("Empirical", paste0("Residual Model")),
       lty=c(1:2), col = c(1:2),
       #cex = 0.7
       )

```

Residual model parameters.

```{r}
data.frame(
  method = c("OLS"),
  Partial_Sill = c(model_partial_sill),
  Range_m = c(model_range),
  Nugget = c(model_nugget)
)%>% 
  kable(caption = "Residual model parameters for final model",  
        col.names = c("Method", "Partial Sill", "Range (m)", "Nugget"),
        digits = 4
        ) %>%
  kable_styling()

```

# Out-of-sample RMSE and R2 for AQS sites 

### --> ? compare against true observations (not yhat estimates)?

```{r,results="hide"}
#AQS test set
#estimate scores for test set
scores_test_aqs <- predict(object = pls_train_test,
                       newdata = annual_aqs,
                       ncomp = 1:cv_comp, 
                       type = "score"
                       ) %>%
  as.data.frame()

#take out spaces 
names(scores_test_aqs) <- names(scores_train_test) 

#add site_id, ufp & lat/long
pls_df_test_aqs <- cbind(
  annual_aqs[c("site_id", "log_ufp")],
  scores_test_aqs
) %>%
  left_join(site_locations) 

#create geodata for test set
geo_test_aqs <- as.geodata(pls_df_test_aqs , 
                        coords.col = c("lambert_x", "lambert_y"), 
                        data.col = "log_ufp", 
                        covar.col = cv_comp_names)

trend_test_aqs <- trend.spatial(trend = cv_cov_trend,
                           geodata = geo_test_aqs)

uk_test_aqs <- krige.conv(coords = geo_train_test$coords,
                            data = geo_train_test$data,
                            locations = geo_test_aqs$coords,
                            krige=krige.control(type = "ok",
                                                obj.model = resid_model,
                                                trend.d= trend_train_test,
                                                trend.l= trend_test_aqs))

#save UK predictions
pls_df_test_aqs$pred  <- uk_test_aqs$predict

#calculate out-of-sample RMSE and R2
rmse_test_aqs <- rmse(obs =  exp(pls_df_test_aqs$log_ufp), pred = exp(pls_df_test_aqs$pred))
r2_test_aqs <- r2_mse_based(obs =  exp(pls_df_test_aqs$log_ufp), pred = exp(pls_df_test_aqs$pred))

```

Results table.

```{r}
data.frame(
  N = c(length(geo_test_aqs$data)),
  RMSE = c(rmse_test_aqs),
  R2 = c(r2_test_aqs)
) %>%
  kable(caption = paste0("Out-of sample RMSE and R2 for AQS co-location sites"),
        digits = 2) %>%
  kable_styling()

```
 
plot of Out-of-sample predictions vs measured UFP concentrations.

```{r}

pls_df_test_aqs %>%
  colo.plot(x.variable = "log_ufp", x.label = "Measure log UFP (log pt/cm3)",
            y.variable = "pred", y.label = "Predicted log UFP (log pt/cm3)", 
            rmse.digits = 2, 
            mytitle = paste0("Out-of-sample predictions vs measured UFP",
                                              "\nat AQS co-location stops"),
            mysubtitle = paste0("UK model wtih ", cv_comp, " PLS components"),
            mycaption = "MSE-based R2: max(0, 1 – MSE / Var(Y))"
            )

```


# Predict at participant homes

```{r, results="hide"}
#estimate PLS scores using ACT geocovariate dataset
scores_act <- predict(object = pls_train_test,
                       newdata = cov_act,
                       ncomp = 1:cv_comp, 
                       type = "score"
                       ) %>%
  as.data.frame()

#take out spaces 
names(scores_act) <- names(scores_train_test) 

#add site ientifier & lat/long
pls_df_act <- cbind(cov_act[c("site_id", "lambert_x", "lambert_y")],
                    scores_act)  

#create geodataset
### --> duplicate locations issue?
geo_act <- as.geodata(pls_df_act, 
                      coords.col = c("lambert_x", "lambert_y"), 
                      covar.col = names(scores_act))

#trend
trend_act <- trend.spatial(trend = cv_cov_trend,
                           geodata = geo_act)
#uk
uk_act <- krige.conv(coords = geo_train_test$coords,
                            data = geo_train_test$data,
                            locations = geo_act$coords,
                            krige=krige.control(type = "ok",
                                                obj.model = resid_model,
                                                trend.d= trend_train_test,
                                                trend.l= trend_act))
                           
#save UK predictions
cov_act$ufp_uk_pred  <- exp(uk_act$predict)
 
```

prediction boxplots overall and by important TRAP indicators

```{r}
# find covariaets predictive of UFP (for plotting/tabulation purposes)
act_cov_lasso <- lasso_fn(dt = cov_act, x_names = cov_names, y_name = "ufp_uk_pred", 
              lambda. = 530
              )

covs_to_desc_log <- act_cov_lasso$results$cov
  
covs_df <- cov_act %>%
  select(contains("ufp"), covs_to_desc_log) %>%
  # convert log distnaces to native scale
  mutate_at(vars(contains("log_")), ~exp(.)) %>%
  # take out "log" in name
  rename_at(vars(contains("log_")), ~sub("log_", "", .)) 
   
covs_to_desc_native <- sub("log_", "", covs_to_desc_log)

new_vars <- covs_df[,covs_to_desc_native] %>%
  rename_all(~paste0(., "_cat")) %>%
  #categorize variables into 3 groups, up to dig.lab digits before using sci notation
  mutate_all(~cut_interval(., n=3, dig.lab = 10))

new_var_names <- names(new_vars)

covs_df <- cbind(covs_df, new_vars)  

```

Tables

```{r, results='asis'}
# table of overall predicted UFP distribution
covs_df %>%
  distribution.table(var.string = "ufp_uk_pred") %>%
  kable(caption = "Distribution of UFP Predictions for the ACT cohort") %>%
  kable_styling()

#new_var_names
 
# # delete?  too much info - plots above are clearer?
# #only works /w results = "asis" chunk setting
# for (i in seq_along(new_var_names)) {
#   #i=1
# covs_df %>%
#   group_by(get(new_var_names[i])) %>%
#     distribution.table(var.string = "ufp_uk_pred") %>%
#     rename_if(grepl("new_var_names", names(.)), ~new_var_names[i]) %>%
#     kable() %>%
#     kable_styling() %>%
#     print()
# }

```

plots

```{r}
# overall
cov_act %>%
  ggplot(aes(x=ufp_uk_pred)) + 
  geom_density(aes(), fill = "black") + 
  labs(x = "UFP Prediction (pt/cm3)",
       title = "Distribution of UFP Predictions"
       )
  
# by geocovariates  
cov_act %>%
  select(contains("ufp"), covs_to_desc_log) %>%
  gather("cov", "value", -contains("ufp")) %>%
  ggplot(aes(x=value, y=ufp_uk_pred)) + 
  geom_point(aes(col=cov), alpha=0.1) +
  geom_smooth() +
  facet_wrap(~cov, scales = "free_x") + 
  theme(legend.position = "none") + 
  labs(x = "",
       y = "UFP Prediction (pt/cm3)",
       title = "UFP predictions for the ACT cohort by selected correlated covariates"
       )

```


# Map of interpolated predictions    

### --->: replace ACT w/ Puget Sound grid for predictions
### --> ? does this help?? only for plot()? locations.inside(interpolation_grid, act_predictions$borders) https://www.stat.washington.edu/peter/591/geoR_sln.html
          
Point map

```{r}
act_predictions <- cov_act %>%
  select(site_id, ufp_uk_pred,
         latitude, longitude)  

# dot map
act_predictions %>%
  map_fn(color_by = "ufp_uk_pred", 
         map_title = "Annual Average UFP Predictions for the ACT Cohort", 
         include_study_area = T
         )

```

Interpolation surface 

```{r }
# Interpolate to a regularly spaced grid and store as a list
interpolation_grid <- with(act_predictions, interp(x = longitude, y = latitude, z = ufp_uk_pred))  

# expand grid to dataframe
grid_dens_expand <- with (interpolation_grid, expand.grid(x=x, y=y)) %>%
  mutate(z = as.vector(interpolation_grid$z),
         z = ifelse(is.na(z), 0, z)) 

# base map 
base_map_pred <- grid_dens_expand %>%
  #mutate_all(as.numeric) %>%
  map_base(latitude_name = "y", longitude_name = "x",
           include_study_area = T,
           map_title = "Interpolation of Annual Average UFP Predictions") 

base_map_pred + stat_contour(aes(x = x, y = y,
                                 z = z, fill = ..level..,),
                             alpha = 0.05,
                            geom = "polygon", 
                            bins = 50,
                            data = grid_dens_expand) +
  scale_fill_gradient(name = "UFP (pt/cm3)",
                      low = "yellow", high = "red")


```

# QC  

## for Aim 3: How much does the regression vs kriging contribute to your esimate?    
Paul Sampson suggestion: decompose the 2019 UK model to see how much of the prediction is generated by regression vs observed interpolations (kriging). If regression is most responsible, less worried about issues w/ smoothing back in time.

UK Yhat = regression_prediction + kriging + error

* the regression prediction (yhat_regression) from UK excludes kriging and model error 
* the UK prediction (yhat_uk) includes the regression prediction, kriging and model error (final estimate used)

```{r}
# save UK betas
uk_betas <- uk_act$beta.est 
beta_names <- names(uk_betas)
beta_no <- length(beta_names)

uk_decompose <- scores_act %>%
  mutate(
    uk_prediction = uk_act$predict) #,

uk_decompose$regression_prediction <- uk_act$beta.est[["beta0"]] + uk_act$beta.est[["beta1"]]*scores_act$Comp1 

if("beta2" %in% beta_names) {
  uk_decompose$regression_prediction <- uk_decompose$regression_prediction + uk_act$beta.est[["beta2"]]*scores_act$Comp2 
}
if("beta3" %in% beta_names) {
  uk_decompose$regression_prediction <- uk_decompose$regression_prediction + uk_act$beta.est[["beta3"]]*scores_act$Comp3
  }
if("beta4" %in% beta_names) {
  uk_decompose$regression_prediction <- uk_decompose$regression_prediction + uk_act$beta.est[["beta4"]]*scores_act$Comp4
}
if("beta5" %in% beta_names) {
  uk_decompose$regression_prediction <- uk_decompose$regression_prediction + uk_act$beta.est[["beta5"]]*scores_act$Comp5
  }

    # regression_prediction = 
    #   uk_act$beta.est[["beta0"]] + 
    #   uk_act$beta.est[["beta1"]]*Comp1 + 
    #   uk_act$beta.est[["beta2"]]*Comp2 +
    #   uk_act$beta.est[["beta3"]]*Comp3,
    # error: structured & unstructured
uk_decompose <- uk_decompose %>% 
  mutate(
    krige_and_error = uk_prediction - regression_prediction,
    #krige_var = uk_act$krige.var,
    reg_prediction_normalized =regression_prediction/uk_prediction 
  )
  
uk_decompose %>%
colo.plot(y.variable = "uk_prediction", y.label = "UK Prediction",
          x.variable = "regression_prediction", x.label = "Regression Prediction",
          rmse.digits = 2,
          coef_digits = 2
          ) 
  
uk_decompose %>%
  ggplot(aes(x=reg_prediction_normalized)) + 
  geom_histogram() + 
  labs(title = "Distribution of regression predictions normalized to UK predictions",
       subtitle = "(yhat_regression) / (yhat_uk)",
       x = "Normalized regression prediction"
      )

uk_decompose %>%
  distribution.table(var.string = "reg_prediction_normalized", round.int = 2) %>%
  mutate(Q5 = quantile(uk_decompose$reg_prediction_normalized, probs = 0.05),
         Q95 = quantile(uk_decompose$reg_prediction_normalized, probs = 0.95)
         ) %>%
  kable(caption = "Distribution of regression predictions normalized to UK predictions (yhat_regression/yhat_uk)", 
        digits = 2) %>%
  add_footnote("the regression prediction (yhat_regression) from UK excludes kriging and model error ") %>%
  add_footnote("the UK prediction (yhat_uk) includes the regression prediction, kriging and model error (final estimate used)") %>%
  kable_styling()

```

## Compare distribution of PLS scores in monitoring vs cohort. See if combination of variables is unusual. Check predicted PLS values are not super large – suggests extrapolation.  

```{r}
scores_train_test$site_type <- "mobile monitoring"
scores_act$site_type <- "ACT cohort"

scores_mm_act <- rbind(scores_train_test, scores_act)

scores_mm_act %>%
  gather("Component", "Score", contains("Comp")) %>%
  mutate(Component = substr(Component, 5, 5)) %>%
  ggplot(aes(x=Score, fill = site_type)) + 
  geom_density(alpha = 0.4) + 
  facet_wrap(~Component, labeller = "label_both") + 
  labs(fill = "Site Type",
       title = "Distribution of PLS component scores for the ACT cohort and mobile monitoring stops"
       ) + 
  theme(legend.position = "bottom")
  

```

### --> ? variogram for something w/ more spatial structure, e.g., PM2.5

```{r}

```


# Sensitivity Analyses

```{r}
 
# # using stop means
# stop_means 
# 
# # trim 10% top/bottom stop observations
# trim10 
# 
# # Windosrize extremes
# windsorized 
# 
# # different regression to estimate annual average
# uw 



```

## CV 
only using stops (excluding AQS sites)
 
```{r}
set.seed(1)
## --> update these later
max_pls_comp. <- 5
dist_fract. <- c(0.05, seq(0.1, 0.4, by=0.1))
folds <- k
 
```

Native Scale UFP and proximity variables

```{r, results="hide"}

# dt2 = annual_native_scale
# max_pls_comp = max_pls_comp.
# dist_fract = dist_fract.
# # to be passed to pls_uk_cv_predictions()
# y_name.. = "ufp"
# cov_names.. = cov_names_native
# #CV folds
# k. = 3
# exponentiate_obs_and_pred = F
# site_locations.. = site_locations


# primary analysis
  #%>% left_join(site_locations, by = "site_id") %>%
primary_analysis_cv <-  pls_uk_cv_eval(dt2 = annual_train_test,
               max_pls_comp = max_pls_comp.,
               dist_fract = dist_fract., 
               y_name.. = "log_ufp",
               cov_names.. = cov_names, 
               k. = folds,
               exponentiate_obs_and_pred = T) 
################3
# t <- annual_train_test %>% left_join(site_locations, by = "site_id")
# dt2 = t
# max_pls_comp = max_pls_comp.
#  dist_fract = dist_fract.
#  y_name.. = "log_ufp"
#  cov_names.. = cov_names
#  k. = folds
#  exponentiate_obs_and_pred = T


###################

# native scale 
native_scale_cv <- pls_uk_cv_eval(dt2 = annual_native_scale,
               max_pls_comp = max_pls_comp.,
               dist_fract = dist_fract., 
               # to be passed to pls_uk_cv_predictions()
               y_name.. = "ufp",
               cov_names.. = cov_names_native,
               #CV folds
               k. = folds,
               # already in native scale
               exponentiate_obs_and_pred = F)

#using stop means 
means_cv <- pls_uk_cv_eval(dt2 = stop_means,
               max_pls_comp = max_pls_comp.,
               dist_fract = dist_fract., 
               y_name.. = "log_ufp",
               cov_names.. = cov_names, 
               k. = folds,
               exponentiate_obs_and_pred = T) 

#trim 10
trim10_cv <- pls_uk_cv_eval(dt2 = trim10 ,
               max_pls_comp = max_pls_comp.,
               dist_fract = dist_fract., 
               y_name.. = "log_ufp",
               cov_names.. = cov_names, 
               k. = folds,
               exponentiate_obs_and_pred = T) 

# Windosrize extremes 
windsorize_cv <- pls_uk_cv_eval(dt2 = windsorized,
               max_pls_comp = max_pls_comp.,
               dist_fract = dist_fract., 
               y_name.. = "log_ufp",
               cov_names.. = cov_names, 
               k. = folds,
               exponentiate_obs_and_pred = T) 

# unweighted annual averages
uw_cv <- pls_uk_cv_eval(dt2 = uw,
               max_pls_comp = max_pls_comp.,
               dist_fract = dist_fract., 
               y_name.. = "log_ufp",
               cov_names.. = cov_names, 
               k. = folds,
               exponentiate_obs_and_pred = T) 
```

PLS components and variogram distance fraction selected via cross-validation

```{r}
# compare results 
Analysis = c("Primary analysis: using stop medians; trimming 5%; regression to estimate season-, TOW2- and TOD5-adjusted UFP; log-transformed UFP and proximity covariates",
              "Native scale predictions and proximity variables (vs log-transformed)",
             "Using stop means (vs medians)",
             "Trim 10% of each site's highest and lowest stop readings (vs 5%)",
             "Windsorize each site's highest and lowest 5% stop readings (vs trimming)",
             "Use site-specific, unweighted annual averages (vs season-, TOW2-, TOD5-adjusted)"
             )

cbind(Analysis, 
      rbind(
        primary_analysis_cv$cv_table,
        native_scale_cv$cv_table,
        means_cv$cv_table,
        trim10_cv$cv_table,
        windsorize_cv$cv_table,
        uw_cv$cv_table
        )) %>%   
  mutate(RMSE = round(RMSE),
         R2 = round(R2, 2)
         ) %>%
  kable(caption = "PLS components and variogram distance fraction selected via cross-validation for sensitivity analyses") %>%
  kable_styling()

```

# fit model to all data & make predictions at ACT locations

### --> see native scale partial sill - why so large?

```{r, results = "hide"}
set.seed(1)

  #%>% left_join(site_locations, by = "site_id") %>%
primary_analysis_uk <-  uk_predictions(dt = annual_train_test,
                 cov_loc_new = cov_act[,names(cov_act) != "ufp_uk_pred"],
                  y_name = "log_ufp",
                  cov_names. = cov_names,
                  pls_comp = primary_analysis_cv$cv_table$PLS_Components, 
                  variogram_dist_fract = primary_analysis_cv$cv_table$Variogram_Distance_Fraction)

native_scale_uk <- uk_predictions(dt = annual_native_scale,
                                  cov_loc_new = cov_act_native_scale,
                                  y_name = "ufp",
                                  cov_names. = cov_names_native,
                                  pls_comp = native_scale_cv$cv_table$PLS_Components, 
                                  variogram_dist_fract = native_scale_cv$cv_table$Variogram_Distance_Fraction)
###########
#native_scale_uk$residual_model_table



########

means_uk <- uk_predictions(dt = stop_means,
                                                      # don't include prediction from main analysis
                                cov_loc_new = cov_act[,names(cov_act) != "ufp_uk_pred"],
                                y_name = "log_ufp",
                                cov_names. = cov_names,
                                pls_comp = means_cv$cv_table$PLS_Components, 
                                variogram_dist_fract = means_cv$cv_table$Variogram_Distance_Fraction)
 
trim10_uk <- uk_predictions(dt = trim10,
                                cov_loc_new = cov_act[,names(cov_act) != "ufp_uk_pred"],
                                y_name = "log_ufp",
                                cov_names. = cov_names,
                                pls_comp = trim10_cv$cv_table$PLS_Components, 
                                variogram_dist_fract = trim10_cv$cv_table$Variogram_Distance_Fraction)

windsorize_uk <- uk_predictions(dt = windsorized,
                                cov_loc_new = cov_act[,names(cov_act) != "ufp_uk_pred"],
                                y_name = "log_ufp",
                                cov_names. = cov_names,
                                pls_comp = windsorize_cv$cv_table$PLS_Components, 
                                variogram_dist_fract = windsorize_cv$cv_table$Variogram_Distance_Fraction)
 
uw_uk <- uk_predictions(dt = uw,
                        cov_loc_new = cov_act[,names(cov_act) != "ufp_uk_pred"],
                        y_name = "log_ufp",
                        cov_names. = cov_names,
                        pls_comp = uw_cv$cv_table$PLS_Components, 
                        variogram_dist_fract = uw_cv$cv_table$Variogram_Distance_Fraction)

```

Residual model parameters for final UK models

```{r}
cbind(Analysis, 
      rbind(
        primary_analysis_uk$residual_model_table, 
        native_scale_uk$residual_model_table, 
        means_uk$residual_model_table, 
        trim10_uk$residual_model_table, 
        windsorize_uk$residual_model_table, 
        uw_uk$residual_model_table 
        )) %>%   
  mutate(Range_m = round(Range_m),
         # Partial_Sill = round(Partial_Sill, 3),
         # Nugget = round(Nugget, 3)
         
         ) %>%
  kable(caption = "Final UK residual model parameters for sensitivity analyses", 
        digits = 3
        ) %>%
  kable_styling()

```

# All results

# Predictions

combine all results 

```{r}
join_dfs_by <- c("site_id", "uk_pred")
  
all_predictions <- primary_analysis_uk$dt %>% 
  select(site_id, latitude, longitude, 
         primary_uk = uk_pred) %>%
  #native scale
  left_join(native_scale_uk$dt[,join_dfs_by], by = "site_id") %>%
  rename(native_scale_uk = uk_pred) %>%
  # stop means
  left_join(means_uk$dt[,join_dfs_by], by = "site_id") %>%
  rename(means_uk = uk_pred) %>%
  # trim 10%
  left_join(trim10_uk$dt[,join_dfs_by], by = "site_id") %>%
  rename(trim10_uk = uk_pred) %>%
  # windsorized
  left_join(windsorize_uk$dt[,join_dfs_by], by = "site_id") %>%
  rename(windsorize_uk = uk_pred) %>%
  # windsorized
  left_join(uw_uk$dt[,join_dfs_by], by = "site_id") %>%
  rename(uw_uk = uk_pred) %>%
  # estimates on native scale if not already
  mutate_at(vars(contains("uk"), -contains("native")) , ~exp(.))

uk_names <- names(all_predictions)[grepl("uk", names(all_predictions))]

```

UFP prediction distribution 
 
```{r}
all_predictions_l <- all_predictions %>% gather("analysis", "prediction", contains("uk")) %>%
  mutate(analysis = str_replace(analysis, "_uk", ""),
         analysis = relevel(factor(analysis), ref = "primary")) 

# table 
all_predictions_l %>%
  group_by(analysis) %>%
  distribution.table(var.string = "prediction") %>%
  kable(caption = "Distribution of UFP predictions (pt/cm3) for primary and sensitivity analyses") %>%
  kable_styling()

# density plot
all_predictions_l %>%
  ggplot(aes(x=prediction, fill = analysis)) + 
  geom_density(alpha = 0.2) + 
  labs(fill = "Analysis",
       title = "Distribution of UFP predictions (pt/cm3) for primary and sensitivity analyses")
   
```

Scatterplots around 1-1 line

```{r}
max_plot <- max(all_predictions_l$prediction)
min_plot <- min(all_predictions_l$prediction)
  
all_predictions %>%
  gather("analysis", "prediction", contains("uk"), -contains("primary")) %>%
  mutate(analysis = str_replace(analysis, "_uk", "")) %>% 
  ggplot(aes(x = primary_uk, y = prediction, group = analysis, col = analysis)) + 
  geom_point(aes(col = analysis), alpha = 0.2) + 
  geom_abline(intercept = 0, slope = 1) + 
  geom_smooth() +
  xlim(min_plot, max_plot) +
  ylim(min_plot, max_plot) +
  labs(x = "Primary Analysis",
       y = "Sensitivity Analysis",
       title = "UFP predictions (pt/cm3) from different UK models") + 
  facet_wrap(~analysis)
  
```

Maps
 
```{r, fig.height=10}
# dot map
#list to store maps
dot_maps <-list()

for(i in seq_along(uk_names)) {
  #i=1
  mymap <- all_predictions %>% map_fn(color_by = uk_names[i], map_title = "")
  
  dot_maps[i] <- list(mymap)
  names(dot_maps)[i] <- uk_names[i]
}

ggarrange(plotlist = dot_maps,
          # dot_maps$primary_uk,
          # dot_maps$native_scale_uk, 
          # dot_maps$means_uk,
          # dot_maps$trim10_uk,
          # dot_maps$windsorize_uk,
          # dot_maps$uw_uk,
          ncol = 3, nrow = 2,
          common.legend = T, legend = "right",
          labels = uk_names
          ) %>%
  annotate_figure(top = "Annual average UFP predictions from different UK models" ) 
   
```

```{r, interpolation code exp}
### --> ?edit? - example of interpolation code

# # Interpolate to a regularly spaced grid and store as a list
# interpolation_grid <- with(act_predictions, interp(x = longitude, y = latitude, z = ufp_uk_pred))  
# 
# # expand grid to dataframe
# grid_dens_expand <- with (interpolation_grid, expand.grid(x=x, y=y)) %>%
#   mutate(z = as.vector(interpolation_grid$z),
#          z = ifelse(is.na(z), 0, z)) 
# 
# # base map 
# base_map_pred <- grid_dens_expand %>%
#   #mutate_all(as.numeric) %>%
#   map_base(latitude_name = "y", longitude_name = "x",
#            include_study_area = T,
#            map_title = "Interpolation of Annual Average UFP Predictions") 
# 
# base_map_pred + stat_contour(aes(x = x, y = y,
#                                  z = z, fill = ..level..,),
#                              alpha = 0.05,
#                             geom = "polygon", 
#                             bins = 50,
#                             data = grid_dens_expand) +
#   scale_fill_gradient(name = "UFP (pt/cm3)",
#                       low = "yellow", high = "red")

```

## Prediction differences 

```{r}
# calculate difference between sensitivity & primary estimates
pred_differences <- all_predictions %>%
  mutate_at(vars(contains("uk"), -contains("primary")), ~.-primary_uk) %>%
  select(-primary_uk)

#density plot
pred_differences %>% gather("analysis", "prediction_diff", contains("uk")) %>%
  ggplot(aes(x = prediction_diff, fill = analysis)) + 
  geom_density(alpha = 0.3) + 
  geom_vline(xintercept = 0, linetype = "dashed") +
  facet_wrap(~analysis, 
             #scales = "free"
             ) +
  labs(x = "UFP prediction differnce (pt/cm3)",
       title = "Difference in annual average UFP predictions relative to primary UK model"
       )

# distribution table
pred_differences %>% gather("analysis", "prediction_diff", contains("uk")) %>%
  group_by(analysis) %>%
  distribution.table(var.string = "prediction_diff") %>%
  kable(caption = "Difference in annual average UFP predictions relative to primary UK model") %>%
  kable_styling()

```

Maps

```{r, fig.height=10}

difference_maps <-list()

for(i in seq_along(uk_names[2:length(uk_names)])) {
  #i=2
  pred_differences$color_by <- pred_differences[[uk_names[i+1]]]
  
  mymap <- pred_differences %>% map_base() +
    geom_point(data = pred_differences,
               aes(x = longitude, y = latitude, col = color_by),
               alpha = 0.3, size = 2) +
    scale_color_gradient(name = "pt/cm3")  
  
  difference_maps[i] <- list(mymap)
  names(difference_maps)[i] <- uk_names[i+1]
}

ggarrange(plotlist = difference_maps,
          ncol = 3, nrow = 2,
          common.legend = T, legend = "right",
          labels = uk_names[2:length(uk_names)]
          ) %>%
  annotate_figure(top = "Difference in annual average UFP predictions relative to primary UK model" ) 
 
```

