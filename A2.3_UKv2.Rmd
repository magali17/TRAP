---
title: "Aim 2: UK"
author: "Magali Blanco"
date: '`r Sys.Date()`'
output:
  html_document:
    number_sections: yes
    df_print: paged
    toc: yes
    toc_depth: '3'
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: 3
editor_options:
  chunk_output_type: console
---

```{r}
# --> to do
# map predictions (mm & ACT) 
 

```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, 
                      cache=T, cache.comments = F, 
                      message = F, warning = F, 
                      tidy.opts=list(width.cutoff=60), tidy=TRUE,
                      fig.height = 8, fig.width = 8
                      )  

# Clear workspace of all objects and unload all extra (non-base) packages
rm(list = ls(all = TRUE))
if (!is.null(sessionInfo()$otherPkgs)) {
  res <- suppressWarnings(
    lapply(paste('package:', names(sessionInfo()$otherPkgs), sep=""),
      detach, character.only=TRUE, unload=TRUE, force=TRUE))
}

pacman::p_load(knitr, kableExtra, 
               #descriptive statistics
               Hmisc, EnvStats, 
                  #qwraps2, #mean_sd, median_iqr
               # modeling
               pls, geoR, #gstat - alternative for UK
               akima, # interp() - interpolate predictions on map
               ggpubr, tidyverse #dplyr, 
               )    
#Himsc: describe(); EnvStats: summaryFull(); ggpubr: ggarrange()

options(knitr.kable.NA = '')
set.seed(1)
source("0.Global_Fns.R")
source("A2.0.1_Var&Fns.R")

images_path <- file.path(images_path0, "3. UK")
tables_path <- file.path(tables_path0, "3. UK")

#read in data
annual0 <- readRDS(file.path("Data", "Aim 2", "Mobile Monitoring", "annual_ufp_and_cov.rda")) 
          
site_locations <- annual0 %>%
  select(site_id, contains("lambert")) #latitude, longitude

cov_names <- annual0 %>%
  select(elev_elevation:log_m_to_a23) %>%
  names()

cov_act <- readRDS(file = file.path("Data", "Aim 2", "Geocovariates", "cov_act_log_scale_preprocessed.rda"))

```

# Approach    

1. use 10 folds to split up the data (annual averages and geocovariates for ~ 308 locations) into a training (90%) and validation (10%) dataset 
  
2. Run PLS on the training set to calculate the scores for various components (start with 1 component).

3. Fit UK models to the training data with the selected number of PLS components and variogram settings (e.g., max distance)

4. Repeat steps 2-3 nine more times to calculate cross-validated RMSE and R2

5. Repeat steps 2-4 with an increasing numbber of PLS components (e.g., 1-10) and for different variogram settings

6. Fit a final model with the selected parameters (number of PLS components, residual model) and all of the data.

7. Estimate the out-of-sample RMSE and R2 for AQS sites using the final UK model.

8. Predict at ACT participant locations.


# PLS to create summary features of geocovariates. 

Create training,  test and AQS set.

```{r, results="hide"}
#for creating sets in data & CV later 
k <- 3 #10

# 1. fit PLS using training set to ID # features to use
annual_all <- annual0 %>%
  select(site_id,
         log_ufp,
         cov_names) %>%
  drop_na() %>%
  #create test, train, aqs data set
  mutate(set = ifelse(grepl("MC", site_id), "aqs",
                 sample(c(1:k), size = sum(grepl("MS", site_id)), replace = T)),
    cv_prediction = NA
         )

annual_aqs <- annual_all %>%
  #only keep aqs sites
  filter(set == "aqs")  

annual_train_test <- annual_all %>%
  #drop aqs sites
  filter(set  != "aqs")  

```

Determine what the best plotting distance is when a different number of PLS components are used.

plots of different variogram distances. Using all data.

```{r, results="hide"}
pdf(file.path(images_path, "variogram plots.pdf"))

use_n_scores <- 10

dist_fractions <- c(0.05,  seq(0.1, 0.5, by=0.1))  
#dist_fractions <- seq(0.01, 0.12, by=0.01)  

par(mfrow = c(3, 2))

for(i in seq_len(use_n_scores)) {
  #i=1
  score_n_names <- paste0("Comp", 1:i)
  
  #fit PLS. calling it "train", but using entire dataset
  pls_train_test <- plsr(log_ufp ~.,
               data=annual_train_test[,c("log_ufp", cov_names)], 
               ncomp = i,
               scale=T
               )
  
  # extract scores for UK
  scores_train_test <- scores(pls_train_test)[,c(1:i)] %>%
    as.data.frame()
  
  # take out spaces in names
  names(scores_train_test) <- score_n_names
  
  # dataset w/ UFP measurements, geocovariates, location
  pls_df_train_test <- cbind(
    annual_train_test[c("site_id", "log_ufp")],
    scores_train_test
  ) %>%
    left_join(site_locations, by = "site_id") 

  ################################ UK ################################
   geo_train_test <- as.geodata(pls_df_train_test, 
                        coords.col = c("lambert_x", "lambert_y"), 
                        data.col = "log_ufp", 
                        covar.col = score_n_names)

  ##trend
  cov_trend <-  as.formula(paste0("~ ", paste0(score_n_names, collapse = " + " )))
  
  #plotting distances
  max.dist <- summary(geo_train_test)$distances.summary[["max"]]
  ## plot nearby locations more finely - improves variogram model fit?
  brk_pt <- 1000
  by1_pt <- 300
  by2_pt <- 1000
  
  ############################ model residuals ######################################
  for(j in seq_along(dist_fractions)) {
    #j=1
    max.plot.dist <- max.dist*dist_fractions[j]

  
  ##Empirical Variogram
  variog_train_test <- variog(geo_train_test,
                         #create equally spaced bins for all distances plotted
                         uvec=c(seq(0, brk_pt, by = by1_pt), seq((brk_pt + by2_pt), max.plot.dist, by= by2_pt)),
                        #UK
                        trend = cov_trend, 
                        messages = F  
                        )

  #plot(variog_train)
  
  wls_ests_train_test <- variofit(variog_train_test, cov.model = "exp",
                                              messages = F)
  
  resid_model_train_test <- variofit(vario = variog_train_test, 
                      ini = wls_ests_train_test, 
                      cov.model = "exp",
                      #weights = "equal") #ols
                      weights = "npairs",
                      messages = F
                      ) #wls
  
   plot(variog_train_test, 
       main = paste0("Comp: ", i, ", Dist fract: ", dist_fractions[j]), 
       cex.main=0.8
         )
  lines(variog_train_test)
  lines(resid_model_train_test, col=2)

  }
  
}

dev.off()

```

CV to evalute best number of PLS components. Folds (k) = `r k`

```{r, results="hide"}
set.seed(1)
################# variables for later ###############
# max PLS components to use in UK
use_n_scores <- 3  #6 

dist_fract <-  c(0.05, 
                 seq(0.1, 0.5, by=0.2)) # by = 0.1
#df to save CV RMSE
cv.eval <- data.frame(
  expand.grid(pls_comp = c(1:use_n_scores),
            dist_fract = dist_fract
            ),
  RMSE = NA,
  R2 = NA
  )
#cv.eval <- cv.eval %>% left_join(variog_dist_to_plot)

################## CV loops ##################################
#estimate CV RMSE and r2 for diff number of PLS component combinations 
for(i in seq_len(use_n_scores)) {
  #i=1
  score_n_names <- paste0("Comp", 1:i)
  
  for(j in seq_along(dist_fract)) {
  #j=1
  #10FCV for each PLS component combination
  for(f in seq_len(k)) {
  #f=1
  train_grp <- annual_train_test$set != f
  
  annual_train <- annual_train_test %>%
    filter(train_grp)  
  
  annual_test <- annual_train_test %>%
    filter(!train_grp)   
  
  #fit PLS to training data
  pls_train <- plsr(log_ufp ~.,
               data=annual_train[,c("log_ufp", cov_names)], 
               ncomp = i,
               scale=T
                  )
  
  #pls_train$scores
  
  # extract scores for UK
  scores_train <- scores(pls_train)[,c(1:i)] %>%
    as.data.frame()
  scores_test <- predict(object = pls_train,
                         newdata = annual_test,
                         ncomp = 1:i,
                         type = "score"
                         ) %>%
    as.data.frame()
  
  # take out spaces in names
  names(scores_train) <- score_n_names
  names(scores_test) <- score_n_names
  
  # dataset w/ UFP measurements, geocovariates, location
  pls_df_train <- cbind(
    annual_train[c("site_id", "log_ufp")],
    scores_train
  ) %>%
    left_join(site_locations,by = "site_id") 
  
  pls_df_test <- cbind(
    annual_test[c("site_id", "log_ufp")],
    scores_test
  ) %>%
    left_join(site_locations,by = "site_id")

  ################################ UK ################################
   geo_train <- as.geodata(pls_df_train, 
                        coords.col = c("lambert_x", "lambert_y"), 
                        data.col = "log_ufp", 
                        covar.col = score_n_names)
  geo_test <- as.geodata(pls_df_test, 
                        coords.col = c("lambert_x", "lambert_y"), 
                        data.col = "log_ufp", 
                        covar.col = score_n_names)
  
  ##trend
  cov_trend <-  as.formula(paste0("~ ", paste0(score_n_names, collapse = " + " )))
  
  max.dist <- summary(geo_train)$distances.summary[["max"]]
 
  max.plot.dist <- max.dist*dist_fract[j] 

  ############################ model residuals ###################################### 
  ##Empirical Variogram
  brk_pt <- 1000
  by1_pt <- 300
  by2_pt <- 1000
  
  variog_train <- variog(geo_train,
                         #uvec=c(seq(0, brk_pt, by = by1_pt), seq((brk_pt + by2_pt), max.plot.dist, by= by2_pt)),
                         #uvec=seq(0, max.plot.dist, by = by2_pt),
                         uvec=c(seq(0, brk_pt, by = by1_pt), seq((brk_pt + by2_pt), max.plot.dist, by= by2_pt)),


                        #UK
                        trend = cov_trend, 
                        messages = F  
                        )

  #use geoR try to estimate intitial range & sill values. using WLS and an exponential fit
  wls_ests_train <- variofit(variog_train, cov.model = "exp", 
                             messages = F)
   
  # REML does slightly better in terms of RMSE and R2 when using fewer components (< ~10), otherwise does about the same. BUT variogram Looks worse than the rest. OLS seems to do better visually.
  
  # resid_model_train <- likfit(geodata = geo_train,
  #                      ini=wls_ests_train,
  #                      cov.model = "exp",
  #                      trend = cov_trend,
  #                      lik.method="REML") # "ML"
  
  resid_model_train <- variofit(vario = variog_train, 
                      ini = wls_ests_train, 
                      cov.model = "exp",
                      #weights = "equal", #ols
                      weights = "npairs",#wls
                      messages = F
                      ) 

  # plot(variog_train)
  # lines(variog_train)
  # lines(resid_model_train, col=2)
  
  #trend
    train_trend <- trend.spatial(trend = cov_trend, geo_train)
    test_trend <- trend.spatial(trend = cov_trend, geo_test)
    
    ############################# Predict #############################
    
    kc_cv <- krige.conv(coords = geo_train$coords,
                        data = geo_train$data,
                        locations = geo_test$coords,
                        krige = krige.control(type = "ok",
                                            obj.model = resid_model_train, 
                                              trend.d = train_trend,
                                              trend.l = test_trend
                                              ), 
                        )
    
    #save CV predictions temporarily  
    annual_train_test$cv_prediction[!train_grp] <- kc_cv$predict
  
  }
   cv.eval$RMSE[cv.eval$pls_comp== i & cv.eval$dist_fract==dist_fract[j]] <- rmse(obs = annual_train_test$log_ufp, pred = annual_train_test$cv_prediction)
  cv.eval$R2[cv.eval$pls_comp== i & cv.eval$dist_fract==dist_fract[j]] <-  r2_mse_based(obs = annual_train_test$log_ufp , pred = annual_train_test$cv_prediction) 
    
    }

}

```


```{r}
######################### select CV parameters #########################
min_rmse <- min(cv.eval$RMSE)
rmse_inx <- cv.eval$RMSE == min_rmse

cv_r2 <- cv.eval$R2[rmse_inx] 
cv_comp <- cv.eval$pls_comp[rmse_inx]
cv_dist_fract <- cv.eval$dist_fract[rmse_inx]
  
cv_comp_names <- names(scores_train)[1:cv_comp]
 
#trends
cv_cov_trend <-  as.formula(paste0("~ ", paste0(cv_comp_names, collapse = " + " )))

######################## Plot CV RMSE & R2 ################################## 
   
fit_info <- paste0("Min RMSE: ", round(min_rmse, 3),
                   "\nR2: ", round(cv_r2, 2),
                   "\nPLS Components: ", cv_comp,
                   "\nFraction of maximum distance plotted in variogram: ", cv_dist_fract
                   )

cv.eval %>%
  gather("variable", "value", RMSE:R2) %>%
  ggplot(aes(x=pls_comp, y=value, 
             col=factor(dist_fract)
             )) + 
  geom_point(alpha=0.7) + 
  geom_line(alpha=0.7) +
  geom_vline(xintercept = cv_comp, 
             linetype = "dashed",
             alpha=0.5) +
  facet_wrap(~variable, 
             scales = "free", 
             ncol=1
             ) +
  scale_x_continuous(breaks= scales::pretty_breaks(n = cv_comp)) +
  labs(
    x = "PLS Components",
    col = "Fract of max dist\nplotted in variogram",
    title = "10FCV Model Performance",
    subtitle = fit_info,
    caption = "MSE-based R2 = max(0, 1 – MSE / Var(Y))"
  ) 

```

Map CV UK Predictions

```{r, study area polygon, eval=F}

```

```{r, delete?, eval=F}
### --> delete? will plot to a grid instead

### --> WRONG: predictions are for last PLS & dist fraction used in CV
# use CV predictions for now
df_of_pred <- annual_train_test %>%
  select(site_id, 
         pred = cv_prediction) %>% 
   # get UFP in native scale
   mutate(pred = exp(pred)) %>%
   left_join(annual0[c("site_id", "latitude", "longitude")], by = "site_id")

#don't include AQS sites since these weren't used to build the model?
 #pls_df_test_aqs[c("site_id", "pred")]

df_of_pred %>%
  map_fn(color_by = "pred", 
         map_title = "CV UK Annual Average UFP Predictions")

```

# Fit final UK model to all stop data    
* exclude AQS co-location sites

```{r}
############################# PLS #############################
# fit model to all data
pls_train_test <- plsr(log_ufp ~.,
             data=annual_train_test[,c("log_ufp", cov_names)], 
             ncomp = cv_comp,
             scale=T
                )

# extract scores for UK
scores_train_test <- scores(pls_train_test)[,c(1:cv_comp)] %>%
  as.data.frame()

# take out spaces in names
names(scores_train_test) <- gsub(" ", "", names(scores_train_test))

```

Plot of PLS loadings. 

```{r, fig.height=15}
pls.loadings <- pls_train_test$loadings[] %>%
  as.data.frame() %>%
  rownames_to_column(var = "cov")

pls.loadings <- pls.loadings %>%
  # rename variables if buffers
  split_cov_name(cov = "cov")

my.alpha=0.3

pls.loadings.l  <- pls.loadings %>%
  #make long format for faceting
  gather(key = "Component", value = "Loading", contains("Comp")) %>%
  mutate(Component = as.numeric(substr(Component, 6, nchar(Component))))  
   
#first 4 components 
pls.loadings.l  %>%
  #drop these in first points
  drop_na(buffer) %>%
  ggplot(aes(x = Loading, y = cov)) +
  geom_point(aes(size=buffer),
             shape=1,
             alpha=my.alpha) +
  scale_size(breaks = c(min(pls.loadings$buffer, na.rm = T),
                        max(pls.loadings$buffer, na.rm = T)
                        )
             ) + #500, 5000, 10000,

  geom_point(data = pls.loadings.l[is.na(pls.loadings.l$buffer),],
           alpha=my.alpha,
           aes(shape="")) +
  geom_vline(xintercept=0,
             linetype="solid",
             alpha=my.alpha) +
    facet_wrap(~Component,
               labeller = "label_both",
               ncol = 2
               ) +
  labs(#x = paste0("Loading"),
       y = "Geocovariate",
       shape= "non-buffer", #"proximity,\nelevation",
       title = "PLS Geocovariate Component Loadings") +
  theme(legend.position = "bottom")


```

Geodataset.

```{r}

############################# UK #############################
# add site_id, ufp & lat/long
pls_df_train_test <- cbind(
  annual_train_test[c("site_id", "log_ufp")],
  scores_train_test
) %>%
  left_join(site_locations) 

# create geodata for test set
geo_train_test <- as.geodata(pls_df_train_test , 
                        coords.col = c("lambert_x", "lambert_y"), 
                        data.col = "log_ufp", 
                        covar.col = cv_comp_names)
# trend
trend_train_test <- trend.spatial(trend = cv_cov_trend,
                           geodata = geo_train_test)

# residual model

max.dist <- summary(geo_train_test)$distances.summary[["max"]]

```

Residual model.   

```{r, results="hide"} 
max.plot.dist <- max.dist*cv_dist_fract

##Empirical Variogram
variog_train_test <- variog(geo_train_test,
                       #uvec=seq(0, max.plot.dist, by = 1000),
                       max.dist=max.plot.dist,
                      #UK
                      trend = cv_cov_trend 
                      )
#plot(variog_train_test)

#estimate range & sill values. using WLS and an exponential fit
wls_ests <- variofit(vario = variog_train_test, cov.model = "exp")

# resid_model <- likfit(geodata = geo_train_test, 
#                            ini=wls_ests, 
#                            trend = cv_cov_trend,
#                            cov.model = "exp",
#                            #lik.method="ML") #ML
#                            lik.method="REML") # REML

# ols: equal; wls: npairs
resid_model <- variofit(vario = variog_train_test, 
                      ini=wls_ests, 
                      cov.model = "exp",
                      #weights="equal") #ols
                      weights = "npairs") #wls

# partial sill: sigma sq
# range: phi, 
# nugget: tau sq
resid_model.s <- summary(resid_model)
model_partial_sill <- resid_model.s$estimated.pars[["sigmasq"]]
model_range <- resid_model.s$estimated.pars[["phi"]]
model_nugget <- resid_model.s$estimated.pars[["tausq"]]

```

```{r}
#par(mfrow = c(1, 1))
plot(variog_train_test,
     main = paste0("Binned empirical and modeled variogram\nfor all stop data (excldues AQS sites). Using ", cv_comp, " PLS components"),
     xlab = "Distance (m)",
     #xlim=c(0,10000)
     )
lines(variog_train_test, lty=1)
lines(resid_model, lty=2, col=2)
legend("bottomright",
       legend = c("Empirical", paste0("Residual Model")),
       lty=c(1:2), col = c(1:2),
       #cex = 0.7
       )

```

Residual model parameters.

```{r}
data.frame(
  method = c("OLS"),
  Partial_Sill = c(model_partial_sill),
  Range_m = c(model_range),
  Nugget = c(model_nugget)
)%>% 
  kable(caption = "Residual model parameters for final model",  
        col.names = c("Method", "Partial Sill", "Range (m)", "Nugget"),
        digits = 4
        ) %>%
  kable_styling()

```

# Map of interpolated predictions    

### --->: need Puget Sound grid for predictions
#### ---> Error: df_of_pred not found

```{r, eval=F}
# Interpolate to a regularly spaced grid and store as a list
test_grid_interp <- with(df_of_pred, interp(x = longitude, y = latitude, z = pred ))  

# expand grid to dataframe
test_grid_dens_expand <- with (test_grid_interp, expand.grid(x=x, y=y)) %>%
  mutate(z = as.vector(test_grid_interp$z),
         z = ifelse(is.na(z), 0, z)) 

# blank map w/ outline - NEED INTERNET ACCESS
mymap <- test_grid_dens_expand %>%
  map_fn(latitude_name = "y", longitude_name = "x", 
         color_map = F, 
         include_study_area = TRUE,
         #default - though not using here
         color_by = "z", 
         map_title = "Interpolation of CV UK Annual Average UFP Predictions") 

#map2 <- mymap

# add concentration interpolations
mymap + 
  # draw 3D surface 
  stat_contour(aes(x = x, y = y, 
                   z = z, 
                   fill = ..level..), 
               alpha = 0.05,
               #need data for fn to work w/ "z"
               data = test_grid_dens_expand, 
               geom = "polygon", 
               bins = 50
               ) +
  scale_fill_gradient(name = "UFP (pt/cm3)", 
                      low = "yellow", high = "red")  +
    theme(legend.position = c(0.98, 0.01), legend.justification = c(1,0)) +
  labs()
 
```


# Out-of-sample RMSE and R2 for AQS sites 

### --> ? compare against true observations (not yhat estimates)?

```{r,results="hide"}
#AQS test set
#estimate scores for test set
scores_test_aqs <- predict(object = pls_train_test,
                       newdata = annual_aqs,
                       ncomp = 1:cv_comp, 
                       type = "score"
                       ) %>%
  as.data.frame()

#take out spaces 
names(scores_test_aqs) <- names(scores_train_test) 

#add site_id, ufp & lat/long
pls_df_test_aqs <- cbind(
  annual_aqs[c("site_id", "log_ufp")],
  scores_test_aqs
) %>%
  left_join(site_locations) 

#create geodata for test set
geo_test_aqs <- as.geodata(pls_df_test_aqs , 
                        coords.col = c("lambert_x", "lambert_y"), 
                        data.col = "log_ufp", 
                        covar.col = cv_comp_names)

trend_test_aqs <- trend.spatial(trend = cv_cov_trend,
                           geodata = geo_test_aqs)

uk_test_aqs <- krige.conv(coords = geo_train_test$coords,
                            data = geo_train_test$data,
                            locations = geo_test_aqs$coords,
                            krige=krige.control(type = "ok",
                                                obj.model = resid_model,
                                                trend.d= trend_train_test,
                                                trend.l= trend_test_aqs))

#save UK predictions
pls_df_test_aqs$pred  <- uk_test_aqs$predict

#calculate out-of-sample RMSE and R2
rmse_test_aqs <- rmse(obs =  pls_df_test_aqs$log_ufp, pred = pls_df_test_aqs$pred)
r2_test_aqs <- r2_mse_based(obs =  pls_df_test_aqs$log_ufp, pred = pls_df_test_aqs$pred)

```

Results table.

```{r}
data.frame(
  N = c(length(geo_test_aqs$data)),
  RMSE = c(rmse_test_aqs),
  R2 = c(r2_test_aqs)
) %>%
  kable(caption = paste0("Out-of sample RMSE and R2 for AQS co-location sites"),
        digits = 2) %>%
  kable_styling()

```
 
plot of Out-of-sample predictions vs measured UFP concentrations.

```{r}

pls_df_test_aqs %>%
  colo.plot(x.variable = "log_ufp", x.label = "Measure log UFP (log pt/cm3)",
            y.variable = "pred", y.label = "Predicted log UFP (log pt/cm3)", 
            rmse.digits = 2, 
            mytitle = paste0("Out-of-sample predictions vs measured UFP",
                                              "\nat AQS co-location stops"),
            mysubtitle = paste0("UK model wtih ", cv_comp, " PLS components"),
            mycaption = "MSE-based R2: max(0, 1 – MSE / Var(Y))"
            )

```


# Predict at participant homes

```{r, results="hide"}
#estimate PLS scores using ACT geocovariate dataset
scores_act <- predict(object = pls_train_test,
                       newdata = cov_act,
                       ncomp = 1:cv_comp, 
                       type = "score"
                       ) %>%
  as.data.frame()

#take out spaces 
names(scores_act) <- names(scores_train_test) 

#add site ientifier & lat/long
pls_df_act <- cbind(cov_act[c("site_id", "lambert_x", "lambert_y")],
                    scores_act)  

#create geodataset
### --> duplicate locations issue?
geo_act <- as.geodata(pls_df_act, 
                      coords.col = c("lambert_x", "lambert_y"), 
                      covar.col = names(scores_act))

#trend
trend_act <- trend.spatial(trend = cv_cov_trend,
                           geodata = geo_act)
#uk
uk_act <- krige.conv(coords = geo_train_test$coords,
                            data = geo_train_test$data,
                            locations = geo_act$coords,
                            krige=krige.control(type = "ok",
                                                obj.model = resid_model,
                                                trend.d= trend_train_test,
                                                trend.l= trend_act))
                           
#save UK predictions
cov_act$ufp_uk_pred  <- exp(uk_act$predict)
 
```

prediction boxplots overall and by important TRAP indicators

```{r}
# find covariaets predictive of UFP (for plotting/tabulation purposes)
act_cov_lasso <- lasso_fn(dt = cov_act, x_names = cov_names, y_name = "ufp_uk_pred", 
              lambda. = 530
              )

covs_to_desc_log <- act_cov_lasso$results$cov
  
covs_df <- cov_act %>%
  select(contains("ufp"), covs_to_desc_log) %>%
  # convert log distnaces to native scale
  mutate_at(vars(contains("log_")), ~exp(.)) %>%
  # take out "log" in name
  rename_at(vars(contains("log_")), ~sub("log_", "", .)) 
   
covs_to_desc_native <- sub("log_", "", covs_to_desc_log)

new_vars <- covs_df[,covs_to_desc_native] %>%
  rename_all(~paste0(., "_cat")) %>%
  #categorize variables into 3 groups, up to dig.lab digits before using sci notation
  mutate_all(~cut_interval(., n=3, dig.lab = 10))

new_var_names <- names(new_vars)

covs_df <- cbind(covs_df, new_vars)  

```

plots

```{r}
# overall
cov_act %>%
  ggplot(aes(x=ufp_uk_pred)) + 
  geom_density(aes(), fill = "black") + 
  labs(x = "UFP Prediction (pt/cm3)",
       title = "Distribution of UFP Predictions"
       )
  
# by geocovariates  
cov_act %>%
  select(contains("ufp"), covs_to_desc_log) %>%
  gather("cov", "value", -contains("ufp")) %>%
  ggplot(aes(x=value, y=ufp_uk_pred)) + 
  geom_point(aes(col=cov), alpha=0.1) +
  geom_smooth() +
  facet_wrap(~cov, scales = "free_x") + 
  theme(legend.position = "none") + 
  labs(x = "",
       y = "UFP Prediction (pt/cm3)",
       title = "UFP predictions for the ACT cohort by selected correlated covariates"
       )

```

Tables

```{r, results='asis'}
# table of overall predicted UFP distribution
covs_df %>%
  distribution.table(var.string = "ufp_uk_pred") %>%
  kable(caption = "Distribution of UFP Predictions") %>%
  kable_styling()

#new_var_names
 
# # delete?  too much info - plots above are clearer?
# #only works /w results = "asis" chunk setting
# for (i in seq_along(new_var_names)) {
#   #i=1
# covs_df %>%
#   group_by(get(new_var_names[i])) %>%
#     distribution.table(var.string = "ufp_uk_pred") %>%
#     rename_if(grepl("new_var_names", names(.)), ~new_var_names[i]) %>%
#     kable() %>%
#     kable_styling() %>%
#     print()
# }

```


# QC  

### --> 

# for Aim 3: How much does the regression vs kriging contribute to your esimate?
Paul Sampson suggestion: decompose the 2019 UK model to see how much of the prediction is generated by regression vs observed interpolations (kriging). If regression is most responsible, less worried about issues w/ smoothing back in time.

*kriging* + error = Yhat – xB 

```{r}

```

Amanda: "Compare distribution of PLS scores in monitoring vs cohort. See if combination of variables is unusual. Check predicted PLS values are not super large – suggests extrapolation. If have strange points, can look to see where they are on a map (e.g., a point is in middle of lake; or strange geographic area)."

### --> are scores diff b/c this is for entire cohort (outside of study area?)

```{r}
scores_train_test$site_type <- "mobile monitoring"
scores_act$site_type <- "ACT cohort"

scores_mm_act <- rbind(scores_train_test, scores_act)

scores_mm_act %>%
  gather("Component", "Score", Comp1:Comp3) %>%
  mutate(Component = substr(Component, 5, 5)) %>%
  ggplot(aes(x=Score, fill = site_type)) + 
  geom_density(alpha = 0.4) + 
  facet_wrap(~Component, labeller = "label_both") + 
  labs(fill = "Site Type"
       ) + 
  theme(legend.position = "bottom")
  

```

### --> ? variogram for something w/ more spatial structure, e.g., PM2.5

```{r}

```

### --> 

# Sensitivity Analyses

## Covariates used

Change TRAP indicator variable [? & UFP] transformations (native to log-transformed or vise versa)

```{r}


```


## Averaging of 1 sec data at each stop

??? Use averages

```{r}

```

## Annual averages 

Trim 10% observations

```{r}

```

Windosrize extreme stop readings 

```{r}
# #windosrized alternative 
# ufp <- ufp %>%
#   group_by(site_id) %>%
#   mutate(
#     ufp_wind = ifelse(ptrak_ct_cm3 > quantile(ptrak_ct_cm3, (1-trim_quantile), na.rm = T), quantile(ptrak_ct_cm3, (1-trim_quantile), na.rm = T), 
#                       ifelse(ptrak_ct_cm3 < quantile(ptrak_ct_cm3, (trim_quantile), na.rm = T), quantile(ptrak_ct_cm3, (trim_quantile), na.rm = T), 
#                              ptrak_ct_cm3)))


```

Use different annual averages
- UW ?

```{r}

```

## UK Model

Differeent number of PLS components

```{r}

```


? [time permitting] Normalize to the Roosevelt garage 

```{r}

```


