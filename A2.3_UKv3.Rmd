---
title: "Aim 2: UK"
author: "Magali Blanco"
date: '`r Sys.Date()`'
output:
  html_document:
    number_sections: yes
    df_print: paged
    toc: yes
    toc_depth: '3'
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: 3
editor_options:
  chunk_output_type: console
---

```{r}
# --> TO DO
# ? switch from geoR to ___ after general 
### -->[EDIT LATER] KEEPING pop_ FOR NOW UNTIL RECEIVE pop10_ from amanda

```

# Notes



# Approach    

1. Split up the data (~ 308 locations) into a validation (10%), training/test (90%) set.

2. Conduct cross-validation using the training/test set to select the number of PLS components, variogram plotting distance, and residual model for UK.

3. Fit a semi-final model to all of the training and test set data with the selected parameters. Use this model to estimate the out-of-sample model performance on the validation set.

4. Fit a final model to all of the data. 

5. Predict at ACT participant locations.

6. Repeate for various sensitivity analyses.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, 
                      cache=T, cache.comments = F, 
                      message = F, warning = F, 
                      tidy.opts=list(width.cutoff=60), tidy=TRUE,
                      fig.height = 6, fig.width = 8
                      )  

# Clear workspace of all objects and unload all extra (non-base) packages
# rm(list = ls(all = TRUE))
# if (!is.null(sessionInfo()$otherPkgs)) {
#   res <- suppressWarnings(
#     lapply(paste('package:', names(sessionInfo()$otherPkgs), sep=""),
#       detach, character.only=TRUE, unload=TRUE, force=TRUE))
# }

pacman::p_load(knitr, kableExtra, 
               #descriptive statistics
               Hmisc, EnvStats, 
                  #qwraps2, #mean_sd, median_iqr
               # modeling
               pls, geoR, #gstat - alternative for UK
               akima, # interp() - interpolate predictions on map
               ggpubr, tidyverse #dplyr, 
               )    
#Himsc: describe(); EnvStats: summaryFull(); ggpubr: ggarrange()

set.seed(1)
options(knitr.kable.NA = '')
source("0.Global_Fns.R")
source("A2.0.1_Var&Fns.R")

images_path <- file.path(images_path0, "3. UK")
tables_path <- file.path(tables_path0, "3. UK")

# act geocovariates
cov_act <- readRDS(file = file.path("Data", "Aim 2", "Geocovariates", "cov_act_log_scale_preprocessed.rda"))

# all annual estimates
all_annual0 <- readRDS(file.path("Data", "Aim 2", "Mobile Monitoring", "all_annual_estimates.rda"))

#covariates
### -->[EDIT LATER] KEEPING pop_ FOR NOW UNTIL RECEIVE pop10_ from amanda
cov_mm <- readRDS(file.path("Data", "Aim 2", "Geocovariates", "cov_mm_log_scale_preprocessed.rda")) %>%
  select(#-contains("pop_"), 
         -contains("pop90_"))

site_loc_vars <- cov_mm %>%select(site_id:lambert_y) %>% names()

#variable names in log and native scale
non_proximity_vars <- names(cov_mm)[!grepl("m_to_", names(cov_mm)) & !names(cov_mm) %in% site_loc_vars]
proximity_vars_log <- names(cov_mm)[grepl("m_to_", names(cov_mm))]
proximity_vars_native <- str_replace(string = proximity_vars_log, pattern = "log_", replacement = "")
cov_names_log <- append(non_proximity_vars, proximity_vars_log)
cov_names_native <- append(non_proximity_vars, proximity_vars_native)

proximity_vars_native <- cov_mm %>%
  select(site_id, proximity_vars_log) %>%
  mutate_at(proximity_vars_log, ~exp(.)) %>%
  rename_at(proximity_vars_log, ~sub(x = ., "log_", ""))

all_annual <- all_annual0 %>%
  #convert to log
  mutate_at(vars(contains("ufp")), ~log(.)) %>%
  mutate(native_scale_ufp = exp(primary_ufp)) %>%
  left_join(cov_mm) %>%
  # add native scale proximity variables
  left_join(proximity_vars_native)

ufp_names <- names(all_annual)[grepl("ufp", names(all_annual))]

analysis_names <- str_replace(ufp_names, "_ufp", "")

Analysis_description = c("Primary analysis: using stop medians; trimming 5%; regression to estimate season-, TOW2- and TOD5-adjusted UFP; log-transformed UFP and proximity covariates",
              "Native scale predictions and proximity variables (vs log-transformed)",
             "Using stop means (vs medians)",
             "Trim 10% of each site's highest and lowest stop readings (vs 5%)",
             "Windsorize each site's highest and lowest 5% stop readings (vs trimming)",
             "Use site-specific, unweighted annual averages (vs season-, TOW2-, TOD5-adjusted)"
             )

# add native proximity variables to act
proximity_vars_native_act <- cov_act %>%
  select(site_id, proximity_vars_log) %>%
  mutate_at(proximity_vars_log, ~exp(.)) %>%
  rename_at(proximity_vars_log, ~sub(x = ., "log_", ""))

cov_act_all <- left_join(cov_act, proximity_vars_native_act)

# divide ACT by area/location
monitoring_ids <- na.omit(cov_act_all[cov_act_all$site_location=="monitoring", "site_id"]) %>% as.vector()
study_ids <- append(monitoring_ids, 
                    na.omit(cov_act_all[cov_act_all$site_location=="study", "site_id"]) %>% as.vector())
st_ids <- append(study_ids, 
                    na.omit(cov_act_all[cov_act_all$site_location=="st", "site_id"]) %>% as.vector())

## ACT cov for primary analysis
cov_act <- cov_act_all %>% filter(site_id %in% study_ids)

## ACT cov for sensitivity analyses 
cov_act_monitoring <- cov_act_all %>% filter(site_id %in% monitoring_ids)
cov_act_st <- cov_act_all %>% filter(site_id %in% st_ids)

##########################
# create validation index
validation_idx <- sample(c(TRUE, FALSE), replace = T,  
                         size = nrow(all_annual), 
                         prob = c(.1, .9))

k <- 2#10
max_pls_comp. <- 4#6
dist_fract. <- c(#0.05, 
  seq(0.1, 0.3, by=0.2))

```
 

# All analyses

### --> add diff study/monitoring areas


## CV 

### --> warning msgs: In variofit(vario = variog_train, ini = wls_ests_train,  ... : unreasonable initial value for sigmasq (too high)

PLS components and variogram distance fraction selected via cross-validation on train/test set

```{r, results="hide"}
#cv_results <- data.frame()
cv_each_combo <- data.frame()

for (i in seq_along(ufp_names)) {
  #i=1
  # non-nataive scale analyses
  if(!grepl("native_scale", ufp_names[i])) {
    df_cv <-  pls_uk_cv_eval(dt2 = all_annual[!validation_idx,],
                   max_pls_comp = max_pls_comp.,
                   dist_fract = dist_fract., 
                   y_name.. = ufp_names[i],
                   k. = k,
                   # diff than if statement below
                   cov_names.. = cov_names_log, 
                   exponentiate_obs_and_pred = T)
      
    #df_cv_table <- df_cv$cv_table 
    #cv_results <- rbind(cv_results, df_cv_table)
    
    # save individual PLS-variogram results
    each_combo <- df_cv$cv_eval %>%
      mutate(Analysis = ufp_names[i])
    
    cv_each_combo <- rbind(cv_each_combo, each_combo)
  }
  
  #native scale analyses
  if(grepl("native_scale", ufp_names[i])) {
    df_cv <-  pls_uk_cv_eval(dt2 = all_annual[!validation_idx,],
                   max_pls_comp = max_pls_comp.,
                   dist_fract = dist_fract., 
                   y_name.. = ufp_names[i],
                   k. = k,
                   # diff than if statement above
                   cov_names.. = cov_names_native,
                   exponentiate_obs_and_pred = F)
    
    # df_cv_table <- df_cv$cv_table 
    # cv_results <- rbind(cv_results, df_cv_table)
    
    # save individual PLS-variogram results
    each_combo <- df_cv$cv_eval %>%
      mutate(Analysis = ufp_names[i])
    
    cv_each_combo <- rbind(cv_each_combo, each_combo)
    }
}

cv_results <- cv_each_combo %>% 
  group_by(Analysis) %>%
  filter(RMSE == min(RMSE)) %>%
  ungroup() %>%
  mutate(Description = Analysis_description) %>%
  select(Analysis, Description, everything()) %>%
  rename(PLS_Components = pls_comp,
         Variogram_Distance_Fraction = dist_fract
         )

```

```{r}
cv_results %>%
  kable(caption = "PLS components and variogram distance fraction selected via cross-validation for sensitivity analyses") %>%
  kable_styling()

```

Plots 

```{r}
cv_each_combo %>%
  filter(grepl("primary", Analysis)) %>%
  gather("variable", "value", RMSE:R2) %>%
  ggplot(aes(x=pls_comp, y=value, 
             col=factor(dist_fract))) + 
  geom_point() + geom_line() +
  facet_grid(variable~ Analysis,
             scales = "free",
             ) +
  scale_x_continuous(breaks= scales::pretty_breaks(n = max(cv_each_combo$pls_comp))) +
  labs(x = "PLS Components",
      col = "Fract of max dist\nplotted in variogram",
      title = paste0(k, "-fold CV Model Performance"), 
      caption = "MSE-based R2 = max(0, 1 â€“ MSE / Var(Y))"
    ) 

```

# Out of sample model performance (using validation set)

```{r, results = "hide"}
out_of_sample_valid <- data.frame(ufp_names,
                                  RMSE = NA,
                                  R2 = NA)

for (i in seq_along(ufp_names)) {
  #i=1
  # get validation results
  pls_components <- cv_results$PLS_Components[i]
  pls_variogram_dist <- cv_results$Variogram_Distance_Fraction[i]
  
  if(!grepl("native_scale", ufp_names[i])) {
    
    df <-  uk_predictions(dt = all_annual[!validation_idx,],
                     cov_loc_new = all_annual[validation_idx,],
                      y_name = ufp_names[i],
                      cov_names. = cov_names_log,
                      pls_comp = pls_components, 
                      variogram_dist_fract = pls_variogram_dist
                     )
    
    obs <- all_annual[validation_idx, ufp_names[i]] %>% exp()
    uk_pred <- df$dt$uk_pred %>% exp()
    
    out_of_sample_valid$RMSE[i] <- rmse(obs = obs, pred = uk_pred)
    out_of_sample_valid$R2[i] <- r2_mse_based(obs = obs, pred = uk_pred)
    }
  
  #native scale analyses
  #i=6
    if(grepl("native_scale", ufp_names[i])) {
      
      df <-  uk_predictions(dt = all_annual[!validation_idx,],
                     cov_loc_new = all_annual[validation_idx,],
                      y_name = ufp_names[i],
                      cov_names. = cov_names_native,
                      pls_comp = pls_components, 
                      variogram_dist_fract = pls_variogram_dist
                     )
    
    obs <- all_annual[validation_idx, ufp_names[i]]  
    uk_pred <- df$dt$uk_pred  
    
    out_of_sample_valid$RMSE[i] <- rmse(obs = obs, pred = uk_pred)
    out_of_sample_valid$R2[i] <- r2_mse_based(obs = obs, pred = uk_pred)
  
    }
}

```

```{r}
out_of_sample_valid %>%
  mutate(RMSE = round(RMSE)) %>%
  kable(caption = "Out-of-sample RMSE and R2 using model parameters from CV  (calculated on validation set).",
        digits = 2) %>%
  kable_styling()

```

# fit model to all mobile monitoring and ACT data 

```{r, results = "hide"}
set.seed(1)

uk_names <- paste0(analysis_names, "_uk")

# empty columns to save predictions
cov_act_all[,uk_names] <- NA
residual_model_param <- data.frame()  
pls_models <- list()
empirical_variograms <- list()
residual_models <- list()
uk_betas <- data.frame() #list()
# geodatasets <- list()
    
for (i in seq_along(ufp_names)) {
  #i=1
  # validation results
  pls_components <- cv_results$PLS_Components[i]
  pls_variogram_dist <- cv_results$Variogram_Distance_Fraction[i]
  
  if(!grepl("native_scale", ufp_names[i])) {
    
    df <-  uk_predictions(dt = all_annual,
                     cov_loc_new = cov_act_all,
                      y_name = ufp_names[i],
                      cov_names. = cov_names_log,
                      pls_comp = pls_components, 
                      variogram_dist_fract = pls_variogram_dist)
    
    # save predictions in native scale
    cov_act_all[uk_names[i]] <- exp(df$dt$uk_pred)
    }

  #native scale analyses
  #i=6
    if(grepl("native_scale", ufp_names[i])) {
      
      df <-  uk_predictions(dt = all_annual,
                     cov_loc_new = cov_act_all,
                      y_name = ufp_names[i],
                      cov_names. = cov_names_native,
                      pls_comp = pls_components, 
                      variogram_dist_fract = pls_variogram_dist)
      
      cov_act_all[uk_names[i]] <- df$dt$uk_pred
    }
  
    # save final residual model parameters
    residual_model_param <- rbind(residual_model_param, df$residual_model_table)
    
    # save geodataset
    # geodatasets[i] <- list(df$geo_dataset)
    # names(geodatasets)[i] <- uk_names[i]
    
    #pls_models
    pls_models[i] <- list(df$pls_model)
    names(pls_models)[i] <- uk_names[i]
    
    # variograms/residual models
    empirical_variograms[i] <- list(df$empirical_variogram)
    names(empirical_variograms)[i] <- uk_names[i]
    
    residual_models[i] <- list(df$residual_model)
    names(residual_models)[i] <- uk_names[i]
    
    # UK betas
    #uk_betas[i] <- df$betas
    uk_betas1 <- data.frame(df$betas) %>%
      rownames_to_column(var = "beta") %>% 
      rename(est = df.betas) %>%
      mutate(Analysis = uk_names[i])
    
    uk_betas <- rbind(uk_betas, uk_betas1)
   
}

```

Different monitoring regions 

```{r}
# pls_components <- cv_results$PLS_Components[cv_results$Analysis=="primary_ufp"]
# pls_variogram_dist <- cv_results$Variogram_Distance_Fraction[cv_results$Analysis=="primary_ufp"]
# 
# uk_monitoring_area <- uk_predictions(dt = all_annual,
#                                   # this changes
#                      cov_loc_new = cov_act_all,
#                       y_name = "primary_ufp",
#                       cov_names. = cov_names_log,
#                       pls_comp = pls_components, 
#                       variogram_dist_fract = pls_variogram_dist)
#     
#     # save predictions in native scale
#     cov_act_all$primary_uk <- exp(uk_monitoring_area$dt$uk_pred)

```

PLS component loadings for primary analysis

```{r}
my.alpha=0.3

pls_loadings <- pls_models[["primary_uk"]]$loadings[] %>%
  as.data.frame() %>%
  rownames_to_column(var = "cov") %>%
  # rename variables if buffers
  split_cov_name(cov = "cov") %>%
  #make long format for faceting
  gather(key = "Component", value = "Loading", contains("Comp")) %>%
  mutate(Component = as.numeric(substr(Component, 6, nchar(Component))))


pls_loadings  %>%
  #buffered covariates
  drop_na(buffer) %>%
  ggplot(aes(x = Loading, y = cov)) +
  geom_point(aes(size=buffer),
             shape=1,
             alpha=my.alpha) +
  scale_size(breaks = c(min(pls_loadings$buffer, na.rm = T),
                        max(pls_loadings$buffer, na.rm = T)
                        )) + #500, 5000, 10000,
  #non-buffered covariates
  geom_point(data = pls_loadings[is.na(pls_loadings$buffer),],
           alpha=my.alpha,
           aes(shape="")) +
  geom_vline(xintercept=0,
             linetype="solid",
             alpha=my.alpha) +
    facet_wrap(~Component,
               labeller = "label_both",
               ncol = 2
               ) +
  labs(y = "Geocovariate",
       shape= "non-buffer", #"proximity,\nelevation",
       title = "PLS Geocovariate Component Loadings") +
  theme(legend.position = "bottom")


```

PLS score distributions of modeling vs prediction sites

Check that predicted PLS values are not super large â€“ suggests extrapolation.  

```{r}

primary_scores <- compare_pls_scores(my_pls_model = pls_models[["primary_uk"]],
                        new_prediction_sites = cov_act,
                        new_site_label = "ACT Cohort",
                        my_title = "Distribution of PLS component scores for mobile monitoring stops and ACT locations within the study area"
                        )

#plot 
primary_scores$plot

primary_scores_monitoring_loc <- compare_pls_scores(my_pls_model = pls_models[["primary_uk"]],
                        new_prediction_sites = cov_act_monitoring,
                        new_site_label = "ACT Cohort",
                        my_title = "Distribution of PLS component scores for mobile monitoring stops and ACT locations within the monitoring area"
                        )

primary_scores_monitoring_loc$plot

primary_scores_st_loc <- compare_pls_scores(my_pls_model = pls_models[["primary_uk"]],
                        new_prediction_sites = cov_act_st,
                        new_site_label = "ACT Cohort",
                        my_title = "Distribution of PLS component scores for mobile monitoring stops and ACT locations within the spatiotemporal area"
                        )

primary_scores_st_loc$plot

primary_scores_all_loc <- compare_pls_scores(my_pls_model = pls_models[["primary_uk"]],
                        new_prediction_sites = cov_act_all,
                        new_site_label = "ACT Cohort",
                        my_title = "Distribution of PLS component scores for mobile monitoring stops and ACT locations within the spatiotemporal area"
                        )

primary_scores_all_loc$plot





# native_scale_scores <- compare_pls_scores(my_pls_model = pls_models[["native_scale_uk"]],
#                         new_prediction_sites = cov_act,
#                         new_site_label = "ACT Cohort",
#                         my_title = "Distribution of PLS component scores for the ACT cohort and mobile monitoring stops"
#                         )
# 
# native_scale_scores$plot

```

# Regression vs Kriging 

or Aim 3: How much does the regression vs kriging contribute to your esimate?    
Paul Sampson suggestion: decompose the 2019 UK model to see how much of the prediction is generated by regression vs observed interpolations (kriging). If regression is most responsible, less worried about issues w/ smoothing back in time.

UK Yhat = regression_prediction + kriging + error

* the regression prediction (yhat_regression) from UK excludes kriging and model error 
* the UK prediction (yhat_uk) includes the regression prediction, kriging and model error (final estimate used)

### --> use study area?

```{r}
#rm(uk_decompose)
my_analysis <- "primary_uk"

# UK betas
betas1 <- uk_betas %>% filter(Analysis == my_analysis)

beta_names <- betas1$beta
#                             subtract beta0
max_beta <- length(beta_names) - 1

#                 # --> chnage here for new datasets  
new_score_values <- primary_scores_all_loc$scores_new_locs 

# log-transform if not native scale analysis since predictions will be log values
if (!grepl("native_scale", my_analysis)) { 
  uk_decompose <- log(cov_act_all[my_analysis])
} else {
    uk_decompose <- cov_act_all[my_analysis]
  }

uk_decompose$regression_prediction <- betas1$est[betas1$beta=="beta0"] + betas1$est[betas1$beta=="beta1"]*new_score_values$Comp1

# update regression prediction if more than 1 component
if(max_beta > 1) {
  for(i in c(2:max_beta)) {
    #i=2
    beta_no <- paste0("beta", i)
    comp_no <- paste0("Comp", i)

      uk_decompose$regression_prediction <- uk_decompose$regression_prediction + betas1$est[betas1$beta==beta_no]*new_score_values[[comp_no]]
  }
}

uk_decompose <- uk_decompose %>% 
  mutate(krige_and_error = uk_decompose[[my_analysis]] - regression_prediction,
        reg_prediction_normalized =regression_prediction/uk_decompose[[my_analysis]])

#uk_decompose0 <- uk_decompose
# uk_decompose <- uk_decompose0

# add id info for plotting purposes
uk_decompose <- cbind(cov_act_all[c("site_id", "site_location", "latitude", "longitude")], 
                      uk_decompose) %>% 
  mutate(
    # add location columns
    monitoring_area = ifelse(site_id %in% monitoring_ids, TRUE, NA),
    study_area = ifelse(site_id %in% study_ids, TRUE, NA),
    st_area = ifelse(site_id %in% st_ids, TRUE, NA),
    all_area = TRUE
    )

uk_decompose_l <- uk_decompose %>%
  gather("area", "value", contains("area"), na.rm = T)

#plot limits to make square plots


#less code, but doesn't have fit info on plot
# p_range <- plot_range(vector1 = uk_decompose_l$primary_uk, vector2 = uk_decompose_l$regression_prediction)
# 
# uk_decompose_l %>% 
#   ggplot(aes(x = regression_prediction, y = primary_uk)) + 
#   geom_point(alpha = 0.2) + 
#   geom_abline(intercept = 0, slope = 1) + 
#   geom_smooth() +
#   xlim(p_range$min, p_range$max) +
#   ylim(p_range$min, p_range$max) +
#   labs(x = "Regression Prediction",
#        y = "UK Prediction",
#        title = "UFP predictions (pt/cm3) from different UK models") + 
#   facet_wrap(~area)

rmse_dig <- 2
coef_dig <- 2

p_monitoring <- uk_decompose %>%
    filter(monitoring_area) %>%
  colo.plot(y.variable = my_analysis, y.label = "UK Prediction",
          x.variable = "regression_prediction", x.label = "Regression Prediction",
          rmse.digits = rmse_dig, coef_digits = coef_dig, 
          mytitle = "Monitoring area" )  

p_study <- uk_decompose %>%
    filter(study_area) %>%
  colo.plot(y.variable = my_analysis, y.label = "UK Prediction",
          x.variable = "regression_prediction", x.label = "Regression Prediction",
          rmse.digits = rmse_dig, coef_digits = coef_dig,
          mytitle = "Study area" )  

p_st <- uk_decompose %>%
    filter(st_area) %>%
  colo.plot(y.variable = my_analysis, y.label = "UK Prediction",
          x.variable = "regression_prediction", x.label = "Regression Prediction",
          rmse.digits = rmse_dig, coef_digits = coef_dig,
          mytitle = "Spatiotemporal study area" )  

p_all <- uk_decompose %>%
  colo.plot(y.variable = my_analysis, y.label = "UK Prediction",
          x.variable = "regression_prediction", x.label = "Regression Prediction",
          rmse.digits = rmse_dig, coef_digits = coef_dig,
          mytitle = "All ACT locations")

ggarrange(p_monitoring, p_study, p_st, p_all, 
          common.legend = T, legend = "bottom" 
          #labels = c("monitoring", "study", "spatiotemporal model", "all" ), 
          ) %>%
  annotate_figure(top = "UK vs Regression UFP Predictions", 
                  # bottom = "regression",
                  # left = "UK"
                  )

```

Map of the difference between UK and regression predictionhs. High values indicate areas where UK has a greater influence.

```{r}
uk_decompose %>%
  filter(monitoring_area) %>%
  map_fn(color_by = "krige_and_error", 
         color_units = "log pt/cm3",
         map_title = "Difference between UK and regression prediction (kriging and error)", 
         include_study_area = T,
         include_monitoring_area = T
         )

uk_decompose %>%
  filter(st_area) %>%
  map_fn(color_by = "krige_and_error", 
         color_units = "log pt/cm3",
         map_title = "Difference between UK and regression prediction (kriging and error)", 
         include_spatiotemporal_area = T
         )


```


Proportion of UK prediction explained by regression

### --> did this right???

```{r, echo = T}
reg_values <- uk_decompose$regression_prediction
uk_values <-uk_decompose$primary_uk

var(reg_values) / var(uk_values)

# same thing?
var.test(reg_values, uk_values)

```

```{r}
# # not necessary?  
# uk_decompose %>%
#   ggplot(aes(x=reg_prediction_normalized)) + 
#   geom_histogram() + 
#   labs(title = "Distribution of regression predictions normalized to UK predictions",
#        subtitle = "(yhat_regression) / (yhat_uk)",
#        x = "Normalized regression prediction"
#       )
# 
# uk_decompose %>%
#   distribution.table(var.string = "reg_prediction_normalized", round.int = 2) %>%
#   mutate(Q5 = quantile(uk_decompose$reg_prediction_normalized, probs = 0.05),
#          Q95 = quantile(uk_decompose$reg_prediction_normalized, probs = 0.95)
#          ) %>%
#   kable(caption = "Distribution of regression predictions normalized to UK predictions (yhat_regression/yhat_uk)", 
#         digits = 2) %>%
#   add_footnote("the regression prediction (yhat_regression) from UK excludes kriging and model error ") %>%
#   add_footnote("the UK prediction (yhat_uk) includes the regression prediction, kriging and model error (final estimate used)") %>%
#   kable_styling()

```

Residual Model parameters.

```{r}
residual_model_param1 <- cbind(uk_names, residual_model_param) 

residual_model_param1 %>%   
  mutate(Range_m = round(Range_m)) %>%
  kable(caption = "Final UK residual model parameters for primary and sensitivity analyses", 
        digits = 3) %>%
  kable_styling()

```

Variogram for primary analysis

```{r}
plot_variogram <- empirical_variograms[["primary_uk"]]
plot_residual_model <- residual_models[["primary_uk"]]

#par(mfrow = c(1, 1))
plot(plot_variogram,
     main = paste0("Binned empirical and modeled variogram"),
     xlab = "Distance (m)"
     )

lines(plot_variogram, lty=1)
lines(plot_residual_model, lty=2, col=2)
legend("bottomright",
       legend = c("Empirical", paste0("Residual Model")),
       lty=c(1:2), col = c(1:2),
       #cex = 0.7
       )

```

# Predictions

## Annual UFP

combine all results 

```{r}
all_predictions <- cov_act %>%
  select(site_loc_vars, uk_names)

```

UFP prediction distribution 
 
```{r}
all_predictions_l <- all_predictions %>% gather("analysis", "prediction", contains("uk")) %>%
  mutate(analysis = str_replace(analysis, "_uk", ""),
         analysis = relevel(factor(analysis), ref = "primary")) 

# table 
all_predictions_l %>%
  group_by(analysis) %>%
  distribution.table(var.string = "prediction") %>%
  kable(caption = "Distribution of UFP predictions (pt/cm3) for primary and sensitivity analyses") %>%
  kable_styling()

# density plot
all_predictions_l %>%
  ggplot(aes(x=prediction, fill = analysis)) + 
  geom_density(alpha = 0.2) + 
  labs(fill = "Analysis",
       title = "Distribution of UFP predictions (pt/cm3) for primary and sensitivity analyses")
   
```

Scatterplots around 1-1 line

```{r}
max_plot <- max(all_predictions_l$prediction)
min_plot <- min(all_predictions_l$prediction)
  
all_predictions %>%
  gather("analysis", "prediction", contains("uk"), -contains("primary")) %>%
  mutate(analysis = str_replace(analysis, "_uk", "")) %>% 
  ggplot(aes(x = primary_uk, y = prediction, group = analysis, col = analysis)) + 
  geom_point(aes(col = analysis), alpha = 0.2) + 
  geom_abline(intercept = 0, slope = 1) + 
  geom_smooth() +
  xlim(min_plot, max_plot) +
  ylim(min_plot, max_plot) +
  labs(x = "Primary Analysis",
       y = "Sensitivity Analysis",
       title = "UFP predictions (pt/cm3) from different UK models") + 
  facet_wrap(~analysis)
  
```

Maps
 
```{r, fig.height=10}
# dot map
#list to store maps
dot_maps <-list()

for(i in seq_along(uk_names)) {
  #i=1
  mymap <- all_predictions %>% map_fn(color_by = uk_names[i], map_title = "")
  
  dot_maps[i] <- list(mymap)
  names(dot_maps)[i] <- uk_names[i]
}

ggarrange(plotlist = dot_maps,
          # dot_maps$primary_uk,
          # dot_maps$native_scale_uk, 
          # dot_maps$means_uk,
          # dot_maps$trim10_uk,
          # dot_maps$windsorize_uk,
          # dot_maps$uw_uk,
          ncol = 3, nrow = 2,
          common.legend = T, legend = "right",
          labels = uk_names
          ) %>%
  annotate_figure(top = "Annual average UFP predictions from different UK models" ) 
   
```

```{r, interpolation code exp}
### --> ?edit? - example of interpolation code

# # Interpolate to a regularly spaced grid and store as a list
# interpolation_grid <- with(act_predictions, interp(x = longitude, y = latitude, z = ufp_uk_pred))  
# 
# # expand grid to dataframe
# grid_dens_expand <- with (interpolation_grid, expand.grid(x=x, y=y)) %>%
#   mutate(z = as.vector(interpolation_grid$z),
#          z = ifelse(is.na(z), 0, z)) 
# 
# # base map 
# base_map_pred <- grid_dens_expand %>%
#   #mutate_all(as.numeric) %>%
#   map_base(latitude_name = "y", longitude_name = "x",
#            include_study_area = T,
#            map_title = "Interpolation of Annual Average UFP Predictions") 
# 
# base_map_pred + stat_contour(aes(x = x, y = y,
#                                  z = z, fill = ..level..,),
#                              alpha = 0.05,
#                             geom = "polygon", 
#                             bins = 50,
#                             data = grid_dens_expand) +
#   scale_fill_gradient(name = "UFP (pt/cm3)",
#                       low = "yellow", high = "red")

```

## Prediction differences 

```{r}
# calculate difference between sensitivity & primary estimates
pred_differences <- all_predictions %>%
  mutate_at(vars(contains("uk"), -contains("primary")), ~.-primary_uk) %>%
  select(-primary_uk)

#density plot
pred_differences %>% gather("analysis", "prediction_diff", contains("uk")) %>%
  ggplot(aes(x = prediction_diff, fill = analysis)) + 
  geom_density(alpha = 0.3) + 
  geom_vline(xintercept = 0, linetype = "dashed") +
  facet_wrap(~analysis, 
             #scales = "free"
             ) +
  labs(x = "UFP prediction differnce (pt/cm3)",
       title = "Difference in annual average UFP predictions relative to primary UK model"
       )

# distribution table
pred_differences %>% gather("analysis", "prediction_diff", contains("uk")) %>%
  group_by(analysis) %>%
  distribution.table(var.string = "prediction_diff") %>%
  kable(caption = "Difference in annual average UFP predictions relative to primary UK model") %>%
  kable_styling()

```

Maps

```{r, fig.height=10}

difference_maps <-list()

for(i in seq_along(uk_names[2:length(uk_names)])) {
  #i=2
  pred_differences$color_by <- pred_differences[[uk_names[i+1]]]
  
  mymap <- pred_differences %>% map_base() +
    geom_point(data = pred_differences,
               aes(x = longitude, y = latitude, col = color_by),
               alpha = 0.3, size = 2) +
    scale_color_gradient(name = "pt/cm3")  
  
  difference_maps[i] <- list(mymap)
  names(difference_maps)[i] <- uk_names[i+1]
}

ggarrange(plotlist = difference_maps,
          ncol = 3, nrow = 2,
          common.legend = T, legend = "right",
          labels = uk_names[2:length(uk_names)]
          ) %>%
  annotate_figure(top = "Difference in annual average UFP predictions relative to primary UK model" ) 
 
```


## Additional   
### Covariates most associated with UFP (Lasso)   

```{r}
# find covariaets predictive of UFP (for plotting/tabulation purposes)
act_cov_lasso <- lasso_fn(dt = cov_act, x_names = cov_names_log, y_name = "primary_uk", 
              lambda. = 530
              )

selected_cov <- act_cov_lasso$results$cov
  
# plot 
cov_act %>%
  gather("cov", "value", selected_cov) %>%
  ggplot(aes(x=value, y=primary_uk)) + 
  geom_point(aes(col=cov), alpha=0.1) +
  geom_smooth() +
  facet_wrap(~cov, scales = "free_x") + 
  theme(legend.position = "none") + 
  labs(x = "",
       y = "UFP Prediction (pt/cm3)",
       title = "UFP predictions for the ACT cohort by selected correlated covariates"
       )

```
