---
title: "Beacon Hill 2001 UFP Data"
author: "Magali Blanco"
date: ' `r Sys.Date()` '
output:
  html_document:
    number_sections: yes
    df_print: paged
    toc: yes
    toc_depth: '3'
editor_options:
  chunk_output_type: console
---

##--> calcluate medians instead since using all data? 

```{r, echo=F}
 # --> Rmarkdown doesn't print to html variables from source()? myquantile_lower/upper

```

# Beacon Hill 2001 Data 

```{r, echo=F}
# #Notes

# * If there is no "P" in the name, the particle diameter is 0.__ µm.
# V05 is the volume of .05 micrometer particles per cc. V1 through V7 (columns L-R) are for 0.1-0.7 µm particles   
# *  What are columns S:AH, which have about 5 numbers after the “V”, and some have additional letters (e.g., ‘V83546, V1P0368)? - 0.835 microns and 1.0368 microns  (the P stands for “point”- I didn’t make that one up!)   
# *  Particle size (e.g., 0.05 µm) is the lowest size limit of the bin 
# 
#missing data
## The midnight data is missing due to daily calibration procedures at the Beacon Hill site.  Not sure about the July/December missing data.
#
# Kim et al. 2004:
# * The sampling height was 4 m above ground (this is lower than our samples).
# * Range: 20 - ____ nm
# * ?? are bins counts within that bin (?Yes); or counts greater than that size?
# 
# Mobile Monitoring particle counter ranges
# * PTRAK: 20 nm -  1 µm, screened: 50 nm - 1 µm
# * DiscMini: 10-700 nm
# * Nanoscan: 10-420 nm

```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, cache=T, cache.comments = F, message = F, warning = F, tidy.opts=list(width.cutoff=60), tidy=TRUE, fig.height = 8)  

# Clear workspace of all objects and unload all extra (non-base) packages
rm(list = ls(all = TRUE))
if (!is.null(sessionInfo()$otherPkgs)) {
  res <- suppressWarnings(
    lapply(paste('package:', names(sessionInfo()$otherPkgs), sep=""),
      detach, character.only=TRUE, unload=TRUE, force=TRUE))
}

pacman::p_load(readxl, dplyr, tidyverse, lubridate, knitr, nlme, lme4, VCA)   

#set plot theme
theme_set(theme_linedraw() + theme(legend.position="bottom")) 

```

```{r}
#source common variables (e.g., percentil, time of day)
source("A2-3_Var&Fns.R")

## ??? ERROR in knit: myquantile_lower / _upper not recognized from source() file ????
myquantile_lower <- 0.00
myquantile_upper <- 1.00

bh <- read_excel("~/Everything/School/PhD_UW/Dissertation/Write Up/1. Proposal/Aim 3. Hx UFP/Hx Data/Tim Larson/Beacon Hill size data.xls") 
 
bh <- bh %>%
  #only keep 2001 data; too little 2000 and 2002 data to calculate annual avg
  filter(year(date) == "2001") %>%
  #format date for use in function
  mutate(
    date = ymd_h(paste(date, hour))
    ) %>%
  add.temporal.variables(data = ., 
                         date.var = "date") %>%
  mutate(
    #group night hours together (for plotting)
    hour_night = ifelse(hour < min(early_am), hour + 24, hour),     
    
    # assign temporal weights
    # #upweigh surrounding months of missing times (no Jul or Dec data)
    # month.wt = ifelse(month == 01 | month == 11 | month == 06 | month == 08, 1.5/12, 1/12),
    # #upweight hr 1 and 23 since no hour 24
    # hour.wt = ifelse(hour == 01 | hour == 23, 1.5/24, 1/24),
    # month.hour.wt = month.wt*hour.wt
    season.wt = 1/4,
    day_of_week.wt = ifelse(time_of_week == "weekday", 5/7, 2/7),
    time_of_day.wt = ifelse(time_of_day == "early_am", length(early_am)/24,
                            ifelse(time_of_day == "am", length(am)/24,
                                   ifelse(time_of_day == "noon", length(noon)/24,
                                          ifelse(time_of_day == "evening", length(evening)/24, length(night)/24)))),
    season_week_hour.wt = season.wt*day_of_week.wt*time_of_day.wt
    
    ) 

```

```{r}
# #check that sum of weights should be 1. # it is
# weights <- bh %>% group_by(season, time_of_week, time_of_day) %>%
#   select(season, time_of_week, time_of_day, season_week_hour.wt) %>%
#   unique() %>%
#   ungroup() %>%
#   summarize(
#     sum = sum(season_week_hour.wt)
#   )

# how many hourly values do we have? 
##only have data for 10 months & 23 hours
# unique(bh$month) #no July or December
# unique(bh$hour) #no midnight

##old: No data for July or December, or Midnight. Little data for months 1, 10, 11; more data for hour 23 than the rest. Thus, values need to be reweighted.

# bh %>%
#   ggplot(aes(x=hour, fill=time_of)) +  #fill= hour
#   geom_bar() + 
#   facet_wrap(~month, labeller = "label_both")

```

## Available Data

```{r}
bh %>%
  ggplot(aes(x=time_of_day, fill= time_of_week)) +  #fill= hour
  geom_bar() + 
  facet_wrap(~season, labeller = "label_both")

```


 
```{r}
# calculate total PNC per hourly 

##convert vol conc (µm3/cm3 air) to num conc (particles/cm3 air)
###make long format
bh.long <-bh %>%
  gather(V02:V2P4579, key = "diam_um_lower", value = "vol_conc_um3_cm3")

bins <- unique(bh.long$diam_um_lower) 

bh.long <- bh.long %>% 
            #rename bin column names
    mutate(diam_um_lower = as.numeric(ifelse(diam_um_lower %in% bins[1:19], 
                    paste0("0.", substr(diam_um_lower, 2,7)),
                    paste0(substr(diam_um_lower, 2,2), ".", substr(diam_um_lower, 4,7))))) 

##Calculate mean diameter for each bin (bin sizes are for lower size cut)
diam <- unique(bh.long$diam_um_lower)
bh.long$diam_um_upper <- NA 

for (i in 1:length(diam)) {
  bh.long$diam_um_upper[bh.long$diam_um_lower == diam[i]] = diam[i+1]
}

bh.long$diam_um_center <- rowMeans(bh.long[c("diam_um_lower", "diam_um_upper")])
 
 bh.long <- bh.long %>%
   mutate(
     #calculate particle volume per bin      
     particle_vol_um3_particle = 4/3*pi*(diam_um_upper/2)^3,
    # calculate number concentration
    num_conc_particles_cm3 = vol_conc_um3_cm3 / particle_vol_um3_particle
    ) 

### get a total particle conc for a specific size range (# counts are for particles ">" __ um) & date
bh.long <- bh.long %>%
    # only keep bin counts for particle sizes similar to what we are collecting in mobile monitoring (PTRAK: 20 nm - 1µm; DiscMini: 10-700 nm; Nanoscan: 10-420 nm). Note: few particles are large, so this doens't make a large difference in the annual avg estimate.
  ## counts similar to PTRAK
  filter(diam_um_lower < 1.03680) %>%
  #calculate avg total PNC for each hour
  group_by(date, hour, hour_night, time_of_day, day, time_of_week, month,  season, season_week_hour.wt) %>%
 dplyr::summarize(
            #total particle count for each day and hour
            pnc_particles_cm3 = sum(num_conc_particles_cm3, na.rm = T)) %>%
  ungroup()


##drop high hourly readings (e.g., new years). this is similar to what will be done with MM data.

bh.long <- bh.long %>%
  filter(pnc_particles_cm3 > as.numeric(quantile(pnc_particles_cm3, myquantile_lower)) &
           pnc_particles_cm3 < as.numeric(quantile(pnc_particles_cm3, myquantile_upper))
         )
  
```

Histogram of hourly UFP estimates used to estimate weighted annual average (trimmed lower and upper `r myquantile_lower*100`% of data).

```{r}
#plot w/o of extreme values
bh.long %>%
  ggplot(aes(x=pnc_particles_cm3)) +
  geom_histogram()

```


## Weighted Annual Average UFP (#/cm3)   
 
```{r}
#take season-week-hour avg's 

# #look to see if there are any extreme values. There are
# bh.long %>%
#   ggplot(aes(x=pnc_particles_cm3)) + 
#   geom_histogram()  



# take avg reading for each season-week-hour combination  
  pnc <- bh.long %>% 
  #group by hour, day & month first since some of these times are sampled more than others; later, take group averages (e.g., season, time_of_week, time_of_day)
  group_by(hour, time_of_day, day, time_of_week, month, season, season_week_hour.wt) %>%
    dplyr::summarize(
      # number of observations used to calculate each month-hour combination (all use 1-2 obs)
      unique_hour_samples = n(),
      # use na.rm=T b/c some values are missing when exclude extremely high values
      avg_pnc_particles_cm3 = mean(pnc_particles_cm3, na.rm = T)
      ) %>%
  #now take avg of larger temporal groups
  group_by(time_of_day, time_of_week, season, season_week_hour.wt) %>%
    dplyr::summarize(
      # number of observations per month-hour combination 
      N = n(),
      # use na.rm=T b/c some values are missing when exclude extremely high values
      avg_pnc_particles_cm3 = mean(avg_pnc_particles_cm3, na.rm = T)
      )

#check that weights add up to 1. They do
#sum(pnc$season_week_hour.wt)
  
# when there is only 1 value for each season-week-hour combination, multiply its respective weight; sum all values to estimate annual avg for total PNC
(pnc_annual_avg_particles_cm3 <- round(sum(pnc$avg_pnc_particles_cm3 * pnc$season_week_hour.wt))) 

#annaul estimates
## w/o lower/upper 5% values
### 69,259: using upper bin size 
### 91,876: using center bin size 
### 130,647: using lower bin size 

## 54,418: using center cut bin size and without extreme values > 90% 
## 96,069: using center cut bin size and without extreme values > 99% 
## ~102,534: using center cut bin size with all values included
## ~146,514: using lower cut bin size (vol/particle is smaller than using center cut --> vol conc is divided by smaller # --> larger PNC)

```

## Temporal UFP patterns   
### Plots    

### -->  add adjusted means from LS fits to boxplots? 

```{r}
#season/month
bh.long %>%
  ggplot(aes(x=season, y=pnc_particles_cm3)) + 
  geom_boxplot(aes(fill=month)) + 
  geom_smooth(aes(x=as.numeric(season)), se=F, formula = y ~ splines::bs(x, 4), method = lm) + 
  labs(title = "loess fit to month")
  
bh.long %>%
  ggplot(aes(x=month, y=pnc_particles_cm3)) + 
  geom_boxplot(aes(fill=season)) + 
  geom_smooth(aes(x=as.numeric(month)), se=F, formula = y ~ splines::bs(x, 4), method = lm) + 
  labs(title = "loess fit to month")
 
#week/time_of_week
bh.long %>%
  ggplot(aes(x=day, y=pnc_particles_cm3)) + 
  geom_boxplot(aes(fill=time_of_week)) + 
  geom_smooth(aes(x=as.numeric(day)), se=F, formula = y ~ splines::bs(x, 5), method = lm) 


#hour/time_of_day
bh.long %>%
  ggplot(aes(x=hour, y=pnc_particles_cm3)) + 
  geom_boxplot(aes(fill=time_of_day, group=hour)) + 
  geom_smooth(aes(x=as.numeric(hour)), se=F) 

bh.long %>%
  ggplot(aes(x=hour_night, y=pnc_particles_cm3)) + 
  geom_boxplot(aes(fill=time_of_day, group=hour_night)) + 
  geom_smooth(aes(x=as.numeric(hour_night)), se=F) + 
  labs(title = paste0("Hours < ", min(early_am), " renumbered to group 'night' hours"))

# bh.long %>%
#   ggplot(aes(x=time_of_day, y=pnc_particles_cm3)) + 
#   geom_boxplot(aes(fill=factor(hour_night))) + 
#   geom_smooth(aes(x=as.numeric(time_of_day)), se=F, formula = y ~ splines::bs(x, 4), method = lm)    

  
```

### Models by time of day, time of week and season

Similar to mobile monitoring approach.

#### LS linear Fit

Only spring is significant. 

```{r}
#only spring (vs winter) is significant
lm(avg_pnc_particles_cm3 ~ time_of_day + time_of_week + season, data=pnc) %>%
  summary()

```

#### ANOVA Variance Component Analyses for random models

Most of the variation is random; 1/3 is from season; practically none from time of day or time of week (?)

#### --> ? delete? can't calc variance w/ just 2 categores (e.g., time_of_week)

```{r}
# ? fits random intercepts for all covariates? 

anovaVCA(avg_pnc_particles_cm3 ~ time_of_day + time_of_week + season, 
         Data=as.data.frame(pnc))  

```

### Models by hour, day and month

#### LS linear fit 

Tue, Sat and Sun are lower than Monday; June is lower than January.

```{r}
#sign: hour, Sat, Sun, Apr, Jun, Nov
lm(pnc_particles_cm3 ~ factor(hour_night) + day + month, data=bh.long) %>%
  summary()

```

#### ANOVA Variance Component Analyses for random models

Most of the variation is random, a small amount from month and day.

Error in this model is larger than for time of day, time of week and season model. 

```{r}
anovaVCA(pnc_particles_cm3 ~ hour + day + month, 
         Data=as.data.frame(bh.long)) 

```


## Mobile Monitoring observations at Beacon Hill  

Mean UFP (#/cm3) from 2/22 - 8/06.

```{r}
mm.wide <- readRDS(file.path("Data", "MobileMonitoring", "mm.wide_190806.rda"))

bh.mm <- mm.wide %>%
  filter(aqs_location == "Beacon Hill") %>%
  select(arrival_time, date:season, ufp_pt_noscreen_ct_cm3) %>%
  drop_na(ufp_pt_noscreen_ct_cm3)

round(mean(bh.mm$ufp_pt_noscreen_ct_cm3)) 

# 4,929 estimate for Beacon Hill through 7/20/19 from PTRAK no screen

```

UFP observations 

```{r}
bh.mm %>%
  ggplot(aes(x=ufp_pt_noscreen_ct_cm3)) + 
  geom_histogram() 

```

# 5 Homes UFP Data
UFP data collected during the winters of 2000 2001 at 5 homes in the Seattle are (data unpublished).

```{r}
#particle column counts are in µm
# count concentration is for: #/cm3

```


```{r}
homes0 <- read.csv(file.path("..", "Write Up", "1. Proposal", "Aim 3. Hx UFP", "Hx Data", "Tim Larson", "Five Homes Study","five home combined_out_no.csv"))

homes <- homes0 %>%
  mutate(
    #format time
    time = ymd_hms(time, tz = "America/Los_Angeles"),
    #recalculate this. it's wrong/off in original dataset for some reason
    DOY = format(time, "%j"),
    #order locations by when sampling occurred 
    location = factor(location, levels = c("m14", "h05", "m37", "v05", "p63")),
    ) %>%
  #add temporal variables
  add.temporal.variables(data = .,
                         date.var = "time")

#calculate total UFP particles uner 1 µm
homes$ufp_67_950_nm_pct_cm3 <- homes %>% 
  select(X0.06745: X0.94868) %>%
  rowSums()

#how long were locations sampled? 
sampling_times <- homes %>%
  group_by(location) %>%
  dplyr::summarize(
    #mean_ufp = mean(ufp_67_0.95_nm),
    start_doy = as.numeric(min(DOY)),
    end_doy = as.numeric(max(DOY)),
    days_sampled = end_doy - start_doy,
    start_date = min(time),
    end_date = max(time)
    )  

#All but location v05 are sampled for full days 24
kable(sampling_times, caption = "Locations Sampled by Days of the Year. All but location v05 are sampled for full days 24", )

# drop times sampled at loc v05 after 9:55:04 AM on last sampling day to ensure all mean estimates are for full days
#v05.start.date <- format(sampling_times[sampling_times$location=="v05",]$start_date, "%Y-%m-%d" )
v05.start.time <- format(sampling_times[sampling_times$location=="v05",]$start_date, "%H:%M:%S" )
v05.end.date <- format(sampling_times[sampling_times$location=="v05",]$end_date, "%Y-%m-%d" )
v05.new.end <- as.POSIXct(paste(v05.end.date, v05.start.time))
                             
homes <- homes %>%
  mutate(
  time = as.POSIXct(ifelse(location %in% c("m14", "h05", "m37", "p63") | 
                      (location == "v05" & time < v05.new.end), 
                    time, NA),
                      origin = "1970-01-01"
                    )) %>%
  drop_na(time)

#calculate avg (total) UFP reading at each site 
ufp_mean <- homes %>%
  group_by(location) %>%
  dplyr::summarize(
    ufp_site_mean = round(mean(ufp_67_950_nm_pct_cm3))
  ) 

homes <- homes %>% 
  left_join(ufp_mean)

site_means <- ufp_mean %>%
  left_join(sampling_times[c("location", "start_date", "end_date", "days_sampled")])

kable(site_means, caption = "UFP PNC (#/cm3) by loation and date sampled")

kable(cbind(min_conc_pct_cm3=range(site_means$ufp_site_mean)[1],
            max_conc_pct_cm3=range(site_means$ufp_site_mean)[2]), 
      caption = "range of UFP PNCs (#/cm3) observed")

```

Plot excludes data from a few hours at loation v05 that went past the 24 hr mark (i.e., the data shown was used to calculate averages). 

```{r}
#plot data
homes %>%
  ggplot(aes(x=time, y=ufp_67_950_nm_pct_cm3)) + 
  geom_point() + 
  geom_smooth(formula = y ~ splines::bs(x, 15), method = lm) +
  geom_hline(aes(yintercept = ufp_site_mean)) +
  facet_wrap(~location, 
             scales="free_x"
             )

```

#### --> make plots by hour, day of week, location

```{r}

```

  
