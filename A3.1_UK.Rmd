---
title: 'Aim 3: UK PLS models and validation'
author: "Magali Blanco"
date: ' `r Sys.Date()` '
output:
  html_document: 
    number_sections: yes
    toc: yes
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
 knitr::opts_chunk$set(echo = F, 
                      cache=F, cache.comments = F, 
                      message = F, warning = F, 
                      tidy.opts=list(width.cutoff=60), tidy=TRUE,
                      fig.height = 5, fig.width = 8
                      )  

# Clear workspace of all objects and unload all extra (non-base) packages
rm(list = ls(all = TRUE))
if (!is.null(sessionInfo()$otherPkgs)) {
  res <- suppressWarnings(
    lapply(paste('package:', names(sessionInfo()$otherPkgs), sep=""),
      detach, character.only=TRUE, unload=TRUE, force=TRUE))
}

# Load key packages using pacman 
my_repo <- 'http://cran.r-project.org'
if (!require("pacman")) {install.packages("pacman", repos = my_repo)}

pacman::p_load(knitr, kableExtra, 
               #descriptive statistics
               Hmisc, EnvStats, 
               # modeling
               pls, geoR, #gstat - alternative for UK
               akima, # interp() - interpolate predictions on map
               ggpubr, tidyverse,
               # 3D mapping
               #rayshader,
               ggmap, sf, #mapping
               ggspatial #mapping, adding scales, N arrows...
               
               #parallel  # mclapply() for parallized processing;  detectCores()
               )    
#Himsc: describe(); EnvStats: summaryFull(); ggpubr: ggarrange()

set.seed(1)

options(knitr.kable.NA = '')
source("0.Global_Fns.R")
source("A2.0.1_Var&Fns.R")
source("A3.0.0_Var&Fns.R")

```


```{r, global fns}
#not sure why these fns are not read from "0.Global_Fns.R". Think it may be a naming issue fo rmse()

#returns MSE
mse <- function(obs, pred){
  mean((obs - pred)^2)
  }

rmse_fn <- function(obs, pred){
  sqrt(mean((obs - pred)^2))
  }

# rmse <- function(obs, pred){
#   sqrt(mean((obs - pred)^2))
# }

#returns MSE-based R2
r2_mse_based <- function(obs, pred) {
  mse.est <- mse(obs, pred)
  r2 <- 1- mse.est/mean((obs - mean(obs))^2)
  max(0, r2)
  }

#######
 
 
```

# Purpose & Approach

## UK Model

This script builds off of the Universal Kriging work completed for Aim 2 (in A2.3_UK_v4.Rmd) by incorporating time-varying covariates and using a temporal trend adjustment factor to predict historical BC and UFP levels back in time. Our primary prediction models are: 

The models separate space and space-time geocovarites in order to:

a) inter-/intra-polate a single linear combinations composed of related geocovariates (e.g., different buffers for the same covariate) rather than many individual covariate buffers each year, and    
b) estimate model coeffiecients for time-varying covariate


## Validation

Furthermore, this script conducts out-of-sample validation to check the model's historical BC (and UFP) predictions at AQS sites. Specifically, we compare model predictions to historical observations at AQS sites:   

* overall
* by site
* stratified by whether or not AQS sites are in study area
* by year/decade 
  

For quality control purposes, we:   

* Check that model predictions are not increasingly bias back in time
* Check whether some sites are more accurately predicted than others 

## Sensitivity Analyses 

```{r}
### --> ? diff ML approaches: lasso, random forests...with stacked ensemble?

cbind(
  Analysis = c("Primary: EC emissions, ratio temporal trend adjustment", 
               "NOx Emissions (vs EC)"
               #"Ratio temporal trend adjustment (vs additive)",
               #"No temporal trend adjustment"
               )
  ) %>%
  kable(., caption = "Description of primary and sensitivity analyses") %>%
  kable_styling()


```

MM stop estimates & covariates

```{r}
# upload datasets 

#MM 

## annual avgs
estimates_mm <- readRDS(file.path("Data", "Aim 2", "Mobile Monitoring", "annual_2020-10-02.rda")) %>%
  # drop sensitivity analysis estimaets for Aim 2
  select(site_id, contains("primary")) %>%
  rename_at(vars(contains("primary")), ~gsub("_primary", "", .))

## cov
cov_mm <- readRDS(file.path("Data", "Aim 2", "Geocovariates", "cov_mm_preprocessed.rda")) %>%
  #drop Roosevelt garage & stop w/ 1 obeservation that was replaced by MS0601
  filter(!site_id %in% c("MS0000", "MS0398")) %>% #MS0601 replaced MS0398
  #drop old 2006 NDVI
  select(-contains("ndvi"))

  
## emissions cov
emissions_mm <- read.csv(file.path("Data", "Aim 3", "Emissions", "mm_locs_trap_sum.csv")) %>%
  select(-gid) %>%
  rename(site_id = native_id) %>%
  #?? take out "_" before buffer - for fn later
  rename_if(grepl("_g_", names(.)), 
            ~str_replace(string = ., pattern =  "_g_",replacement =  "") ) 


## NDVI cov
ndvi_mm <- read.csv(file.path("Data", "Aim 3", "NDVI", "mm_locs_ndvi.csv")) %>%
  select(-gid) %>%
  rename(site_id = native_id) %>%
  #make wide format similar to pop TVC
  gather("ndvi", "value", contains("ndvi")) %>%
  mutate(ndvi =paste0("ndvi", substr(year, 3,4), substr(ndvi, 5, nchar(ndvi)) )) %>%
  select(-year) %>%
  spread(ndvi, value)


## combine all
mm <- estimates_mm %>%
  #convert to log
  mutate_at(vars(contains(c("ufp", "bc"))), ~log(.)) %>%
  left_join(cov_mm) %>%
  left_join(emissions_mm) %>%
  left_join(ndvi_mm) 

```



```{r}
### --> ? need? delete?


#AQS observations for validation? 
mm_aqs_2019 <- mm %>%
  filter(str_detect(site_id, "MC"))


#only keep non-AQS site stops
mm <- mm %>%
  filter(str_detect(site_id, "MS"))

```

Cohort covariates

```{r}
# cohort

##  cov
### ~cov_act_all in A2 code
cohort <- readRDS(file = file.path("Data", "Aim 2", "Geocovariates", "cov_act_preprocessed.rda")) %>%
  #drop old 2006 NDVI
  select(-contains("ndvi"))

## emissions cov      # participants in north WA
emissions_cohort <- read.csv(file.path("Data", "Aim 3", "Emissions", "wa_n_actb5_trap_sum.txt")) %>%
  # add participants in south WA
  rbind(read.csv(file.path("Data", "Aim 3", "Emissions", "wa_s_actb5_trap_sum.txt"))) %>%
  select(-gid) %>%
  rename(site_id = native_id) %>%
#?? take out "_" before buffer - for fn later
  rename_if(grepl("_g_", names(.)),
            ~str_replace(string = ., pattern =  "_g_",replacement =  "") )

## NDVI 

# this dataset has 402 fewer sites than the cohort dataset, but those 402 locations are outside of the study area and are dropped anyways, so it's fine
ndvi_cohort <- read.csv(file.path("Data", "Aim 3", "NDVI", "wa_n_actb5_gis_ndvi.csv")) %>%
 rbind(read.csv(file.path("Data", "Aim 3", "NDVI", "wa_s_actb5_gis_ndvi.csv"))) %>%
  select(-gid) %>%
  rename(site_id = native_id) %>%
  #make wide format similar to pop TVC
  gather("ndvi", "value", contains("ndvi")) %>%
  mutate(ndvi =paste0("ndvi", substr(year, 3,4), substr(ndvi, 5, nchar(ndvi)) )) %>%
  select(-year) %>%
  spread(ndvi, value)



# combine all
## ACT cov for primary analysis
cohort <- cohort %>%
  ##only keep sites in study area
  filter(site_location %in% c("monitoring", "study")) %>%
  # add emissions
  left_join(emissions_cohort) %>%
  left_join(ndvi_cohort)


  
```


AQS site covariates

```{r}
# aqs 

## cov
aqs_cov <- readRDS(file = file.path("Data", "Aim 2", "Geocovariates", "cov_agency_preprocessed.rda")) %>%
  select(site_id, native_id, Local.Site.Name:Longitude, everything(),
         #drop old 2006 NDVI
         -contains("ndvi")
         )

## emissions cov
aqs_emissions <- read.csv(file.path("Data", "Aim 3", "Emissions", "aqs_trap_sum.csv")) %>%
  select(-gid) %>%
  rename(site_id = native_id) %>%
  #?? take out "_" before buffer - for fn later
  rename_if(grepl("_g_", names(.)), 
            ~str_replace(string = ., pattern =  "_g_",replacement =  "") ) 

###crosswalk for emissions estimates & cov
cw <- read_sf(file.path("..", "GIS", "Shapefiles", "AQS Sites", "aqs_nearby.shp")) %>%
  st_drop_geometry() %>%
  select(site_id = native_id, Local.Site.Name = Local.Site, everything()) %>%
  mutate(Latitude = as.numeric(Latitude),
         Longitude = as.numeric(Longitude),
         )

aqs_emissions <- cw %>%
  left_join(aqs_emissions) %>%
  select(-site_id)  
  
  
## NDVI 
aqs_ndvi <- read.csv(file.path("Data", "Aim 3", "NDVI", "aqs_ndvi.csv")) %>%
  select(-gid) %>%
  rename(site_id = native_id) %>%
  #make wide format similar to pop TVC
  gather("ndvi", "value", contains("ndvi")) %>%
  mutate(ndvi =paste0("ndvi", substr(year, 3,4), substr(ndvi, 5, nchar(ndvi)) )) %>%
  select(-year) %>%
  spread(ndvi, value) %>%
  
  #IDs are different in other datasets
  left_join(cw[c("site_id", "Local.Site.Name")] ) %>%
  #alphabetize for joining later since IDs are different
  arrange(Local.Site.Name) %>%
  select(-Local.Site.Name) %>%
  #may need this later for a crosswalk
  rename(site_id2 = site_id)


#combine all
aqs <- aqs_cov %>%
  left_join(aqs_emissions) %>%
  
  #id's are different, arrange by name, then join
  arrange(Local.Site.Name) %>%

  cbind(aqs_ndvi) %>%
  select(site_id, site_id2, everything())
  

```

```{r}
# ? need for validation set??
aqs_ids <- aqs %>%
  select(native_id, Local.Site.Name, Longitude, Latitude)


```


Grid covariates

```{r}
# grid

## cov
grid_cov <- readRDS(file = file.path("Data", "Aim 2", "Geocovariates", "cov_grid_preprocessed.rda"))  %>% 
  select(site_id, location_id:msa, longitude, latitude, everything(),
         #drop 2006 NDVI
         -contains("ndvi")
         ) %>%
  #ps_a00001 is equal to grid_mm 0001, so we can join columns afterwards
  arrange(native_id)
 
 
## emissions cov
grid_emissions <- read.csv(file.path("Data", "Aim 3", "Emissions", 
                                     "grid_trap_sum.csv" #"grid_old_trap_sum.csv"
                                     )) %>%
  select(-gid) %>%
  #rename(site_id = native_id) %>%
  #?? take out "_" before buffer - for fn later
  rename_if(grepl("_g_", names(.)), 
            ~str_replace(string = ., pattern =  "_g_",replacement =  "") ) 


###crosswalk for emissions estimates & cov
grid_emissions <-  read_sf(file.path("..", "GIS", "Shapefiles", "Predictions", "grid", "new_grid.shp")) %>%
  st_drop_geometry() %>%
  select(native_id, longitude = long, latitude = lat) %>%
  left_join(grid_emissions) %>% #select(-site_id)  
  #ps_a00001 is equal to grid_mm 0001, so we can join columns afterwards
  arrange(native_id) %>%
  select(native_id_emissions = native_id,
         longitude_emissions = longitude,
         latitude_emissions = latitude,
         everything()
         )



# NDVI
grid_ndvi <- read.csv(file.path("Data", "Aim 3", "NDVI", "grid_ndvi.csv")) %>%
  select(-gid) %>%
  rename(site_id = native_id) %>%
  #make wide format similar to pop TVC
  gather("ndvi", "value", contains("ndvi")) %>%
  mutate(ndvi =paste0("ndvi", substr(year, 3,4), substr(ndvi, 5, nchar(ndvi)) )) %>%
  select(-year) %>%
  spread(ndvi, value) %>%
  #match IDs from above (IDs are diff)
  rename(native_id_emissions = site_id)



# combine all
grid <- grid_cov %>% #left_join(
  #datasets are arranged in same order even though IDs are diff
  cbind(grid_emissions) %>%
  #NDVI has same IDs as grid_emissions
  left_join(grid_ndvi) %>%
  # check that lat/long is same for 2 dfs. #looks good
  select(contains(c("site_id", "native_id", "longitude", "latitude")), everything() )
  
 

```

```{r}

### only keep grid points in study area & not on water
project_crs <- 4326

#### study area
study_area_shp <- read_sf(file.path("..", "GIS", "Shapefiles", "Study area", "oval_around_monitoring_area.shp")) %>%
  st_transform(project_crs)

#### water
water_shp <- read_sf(file.path("..", "GIS", "Shapefiles", "Other features", "Water", "DNR_Hydrography__Water_Bodies", "DNR_Hydrography__Water_Bodies.shp")) %>%
  st_transform(project_crs)


####create sf object from grid
grid_shp <- grid %>%
  st_as_sf(., coords=c("longitude","latitude"), remove=F,
           crs=project_crs)

#### intersection
grid_shp$in_study_area <- st_intersects(grid_shp, study_area_shp, sparse = F) %>%
  apply(., 1, any)

#### points not in water
grid_shp$not_in_water <- !st_intersects(grid_shp, water_shp, sparse = F) %>%
  apply(., 1, any)


#### convert back to df
grid <- grid_shp %>%
  st_drop_geometry() %>% 
  #only need these points
  filter(in_study_area == TRUE,
         not_in_water == TRUE
         ) %>%
  select(-c(in_study_area, not_in_water, #site_id
            )) %>%
  select(longitude, latitude, everything())

```

Checking raw NDVI buffer estimates over time.

* range is from -1 to 1. Higher values are indicative of more "green." Water has the lowest values.

* note, 1990 & 2010 came from Landsat 5, while 2019 came from Landsat 8. The wavelength ranges used to calculate NDVI are slightly different for these two satellites.

https://www.usgs.gov/core-science-systems/nli/landsat/landsat-normalized-difference-vegetation-index?qt-science_support_page_related_con=0#qt-science_support_page_related_con 

https://www.usgs.gov/faqs/what-are-best-landsat-spectral-bands-use-my-research?qt-news_science_products=0#qt-news_science_products

```{r, fig.height=10}

raw_ndvi <- grid_shp %>%
  select(native_id, longitude, latitude, 
         #look small buffer
         matches("_a00250"), #matches("_a00500"), 
         #only show median estimates
         #-matches(c( "_q25", "_q75")),
         ) %>% 
  gather("key", "value", contains("ndvi")) %>%  
  
  separate(key, into = c("year", "type", "buffer"), sep = "_") %>%
  mutate(year = ifelse(test =  grepl("90", year), yes =  1990, 
                        no = ifelse(grepl("10", year), 2010, 2019)
                       )
         ) 

raw_ndvi %>%  
  filter(!type %in% c("summer", "winter")) %>%
  {
              ggplot(data=.) + 
              geom_sf(aes(col=value)) +
             scale_color_gradient(low = "blue", high = "green") +
             
            facet_grid(type~year) +
            
            labs(title = "Raw NDVI buffer estimates for the grid",
                 subtitle = paste0("buffer: ", unique(.$buffer))
                 )
           }

```

```{r}
# one quantile & no summer/winter
raw_ndvi %>%
  filter(type=="q50") %>%  {
              ggplot(data=.) + 
              geom_sf(aes(col=value)) +
             scale_color_gradient(low = "blue", high = "green") +
             
            facet_grid(type~year) +
            
            labs(title = "Raw NDVI buffer estimates for the grid",
                 subtitle = paste0("buffer: ", unique(.$buffer)),
                 col = "NDVI"
                 )
           }



```



```{r}

# delete??

# group ACT locations by location
# monitoring_ids <- cohort$site_id[grepl("monitoring", cohort$site_location)] 
# 
# outside_monitoring_in_study_ids <- cohort$site_id[grepl("study", cohort$site_location)] 
# outside_monitoring_in_st_ids <- cohort$site_id[grepl("study|st", cohort$site_location)]
# 
# study_ids <- append(monitoring_ids, outside_monitoring_in_study_ids)
# st_ids <- append(monitoring_ids, outside_monitoring_in_st_ids)


```


```{r}

# main datasets

## mm 
## cohort 
## grid 

```

```{r}
# CV results from A2
a2_cv_results <- readRDS(file.path("Output", "Aim 2", "Tables", "3. UK", "cv_results.rda")) %>%
label_analysis(var = "Analysis") %>%
    filter(grepl("primary", Analysis, ignore.case = T)) %>%
  select(Pollutant, PLS_Components, Variogram_Distance_Fraction, RMSE, R2) %>%
  mutate(Aim = 2)
 
```

```{r}
# trend adjustment
## convert ug/m3 (10^-6) to ng/m3 (10^-9)

conversion_factor <- 1e3 
trend_adjustment <- read_rds(file.path("Data", "Aim 3", "Hx BC at AQS Sites", "trend_adjustment.rda")) %>%
  #mutate(difference = difference*conversion_factor) %>%
  select(Year, ratio_adjustment = ratio)

# Hx BC validation - Hx AQS readings 
bc_validation <- read_rds(file.path("Data", "Aim 3", "Hx BC at AQS Sites", "aqs_avgs_for_validation.rda")) %>%
  mutate(mean_ng_m3 = mean*conversion_factor,
         #site_id = NA,
         Pollutant = "bc"
         ) %>%
  select(-c(mean, no_zero_readings)) %>%
  mutate(
    #match other datasets
    Local.Site.Name = recode_factor(factor(Site),
                         "Seattle Beacon Hill" = "Seattle - Beacon Hill",
                         "Seattle Duwamish Vly" = "Seattle - Duwamish",
                         "Puyallup South Hill" = "Puyallup",
                         "Seattle 10th & Weller" = "Seattle-10th & Weller"
                         )
    ) %>%
  left_join(aqs[c("site_id", "Local.Site.Name")])



# ## add site_id #s for merging later since these sites have slightly different site names
# bc_validation$site_id[str_detect(string = bc_validation$Site, pattern = "Weller")] <- "MC0120"
# bc_validation$site_id[str_detect(string = bc_validation$Site, pattern = "Beacon")] <- "MC0003"
# bc_validation$site_id[str_detect(string = bc_validation$Site, pattern = "Duwamish")] <- "MC0126"
# bc_validation$site_id[str_detect(string = bc_validation$Site, pattern = "Kent")] <- "MC0406"
# bc_validation$site_id[str_detect(string = bc_validation$Site, pattern = "Allentown")] <- "MC0002"


# UFP validation - BH 2001
ufp_validation <- readRDS(file.path("Data", "Aim 3", "Hx UFP at BH", "BH_2001_UFP.rda")) %>%
  mutate(site_id = unique(aqs$site_id[grepl("Beacon", aqs$Local.Site.Name)]), #"MC0003",
         Pollutant = "ufp",
         Year = 2001
         )

```



```{r}

# # ??? what is this?? delete??
# 
# Add geocovariates to validation AQS sites so we can make predictions here later
# 
# #add geocovariates to validation AQS sites so we can make predictions here later 
# aqs <- bc_validation %>%
#   select(Site) %>%
#   unique() 
# 
# aqs$site_id <- NA
# aqs$site_id[str_detect(string = aqs$Site, pattern = "Weller")] <- "MC0120"
# aqs$site_id[str_detect(string = aqs$Site, pattern = "Beacon")] <- "MC0003"
# aqs$site_id[str_detect(string = aqs$Site, pattern = "Duwamish")] <- "MC0126"
# aqs$site_id[str_detect(string = aqs$Site, pattern = "Kent")] <- "MC0406"
# aqs$site_id[str_detect(string = aqs$Site, pattern = "Allentown")] <- "MC0002"
# 
# aqs <- aqs %>%
#   #add geocovariates
#   left_join(mm_aqs_2019) %>%
#   #won't be using these annual avg estimates from the MM campaign
#   select(-c(bc, ufp))

```


ACS survey: Pop density 2010+

```{r}
# pop density 2010+ for Seattle-Tacoma-Bellevue, WA Metro Area from annual Census ACS surveys  

# place to save total estimates for all files

pop_folder <- file.path("Data", "Aim 3", "Population", "Census", "ACS-yearly")

# files w/ data...end w/ csv
pop_files <- list.files(pop_folder) %>% 
  str_subset("data_with_overlays.*csv$")


acs <- data.frame(
  # get years from file names
  Year = as.numeric(str_extract(pop_files, "[0-9]{4}")),
  Estimate = NA)


for (i in seq_along(acs$Year)) {
  #i=1
  #pop_file <- paste0("ACSDP1Y", acs$Year[i], ".DP05_data_with_overlays_2020-06-15T124502.csv")
  
                                                                  #total pop estimate
  acs$Estimate[i] <- read.csv(file.path(pop_folder, pop_files[i]))[2, "DP05_0001E"] %>% 
    #convert factor to number
    as.character() %>% as.numeric()

}

#estimate avg annual change (proportion) relative to 2010 for the entire area
pop_2010_2018_prop <-  (acs$Estimate[nrow(acs)] - acs$Estimate[1])/acs$Estimate[1]/(acs$Year[nrow(acs)] - acs$Year[1] )
 
```


```{r}
acs%>%
  ggplot(aes(x=Year, y=Estimate)) +
  geom_point() +
  labs(title = "ACS survey total annual population estimates",
       subtitle = paste0("Seattle-Tacoma_Bellevue metropolitan area:\n ~", round(pop_2010_2018_prop*100, 1), "% annual increase" )
       )

```

```{r}
# shapefiles for maps

#background map slightly larger than the study area  
study_area <- readRDS(file.path("Data", "GIS", "study_area_df.rda"))
monitoring_area <- readRDS(file.path("Data", "GIS", "monitoring_area_df.rda")) 
map0 <- map_base(dt=study_area, latitude_name = "lat", longitude_name = "long")
aviation_area <- readRDS(file.path("Data", "GIS", "zoning_df.rda")) 
airports_area <- readRDS(file.path("Data", "GIS", "airports_df.rda")) 

#markers
markers <- readRDS(file = file.path("Data", "GIS", "markers.rda"))

```

###--> keep variog dist fract at 10%? since UFP and BC were differnet in A2

```{r}
# common variables

site_loc_vars <- cov_mm %>% select(site_id, longitude, latitude, lambert_x, lambert_y) %>% names()

space_vars <- cov_mm %>%
  select(-site_loc_vars, -native_id,
         -contains(c("pop", "ndvi"#, 
                    #"ec"
                    ))) %>%
  names()

# modeling parameters from Aim 2

n_components <-  min(a2_cv_results$PLS_Components) #3 
variog_dist_frac <- 0.10
  
# n_components_ufp <- a2_cv_results$PLS_Components[str_detect(a2_cv_results$Pollutant, "UFP")]
# n_components_bc <- a2_cv_results$PLS_Components[str_detect(a2_cv_results$Pollutant, "BC")]
variog_dist_frac_ufp <- a2_cv_results$Variogram_Distance_Fraction[str_detect(a2_cv_results$Pollutant, "UFP")]
variog_dist_frac_bc <- a2_cv_results$Variogram_Distance_Fraction[str_detect(a2_cv_results$Pollutant, "BC")]


# years obtained for TVCs
census_data_yrs <- c(1990, 2000, 2010)
last_acs_yr <- max(acs$Year)
ndvi_data_yrs <- c(1990, 2010, 2019) #c(2006) 
emissions_data_yrs <- c(seq(1990, 2015, by = 5), 2019)

#PLS modeling year used
pop_model_yr <- 2010
ndvi_model_yr <- 2019
emissions_model_yr <- 2019

# first prediction year
first_p_yr <- 1995
last_p_yr <- 2019

# pollutant names
pollutants <- c("bc", "ufp")

#mapping years
mapping_yrs <- c(seq(first_p_yr, last_p_yr-1, 15), 2019)

```

# Available time-varying covariates (TVCs)

Each of these TVCs has several buffers for each year.

* Decenial US Census surveys (1990, 2000 and 2010) 
* NDVI for 1990-1993,2006 and 2010/2019
* Emissions every 5 years from 1990-2015 and for 2019 from MOVES emission factors for King County (1995-2019) and WADOT AADT (1995-2015) estimates 


# PLS - Create dimension-reduced versions of time-varying covariates

Transforming space-varying covariates (for M=3 components), population density (M=1), NDVI (M=1) and EC emissions (M=1) into linear combinations (scores) using PLS regression and 2019 TRAP observations (Y).

Fit PLS models using 2019 TRAP (BC, UFP) and:

* space-varying covariates
* population: Census 2010   
* NDVI __2006__ [for now]
* emissions 2019 [not yet]   

```{r}
pop_model_vars <- "pop10_"
ndvi_model_vars <- "ndvi19_"
ec_model_vars <- "ec2019_"
# sensitivity
nox_model_vars <- "nox2019_"

space_models <- fit_pls(dt = mm, x = space_vars, .ncomp = n_components)
pop2010_models <- fit_pls(dt = mm, x = pop_model_vars, .ncomp = 1)
ndvi2019_models <- fit_pls(dt = mm, x = ndvi_model_vars, .ncomp = 1)
ec2019_models <- fit_pls(dt = mm, x = ec_model_vars, .ncomp = 1)
# sensitivity
nox2019_models <- fit_pls(dt = mm, x = nox_model_vars, .ncomp = 1)

```


Variability explained by each model.

```{r}
variance_explained <- data.frame()

for(i in seq_along(pollutants)) {
  #i=1
  space_var <- explvar(space_models[[pollutants[i]]]) %>% as.vector()
                                                    
  pop_var <- explvar(pop2010_models[[pollutants[i]]]) %>% as.vector() %>% 
    # NAs for 2nd & 3rd components 
    append(rep(NA, n_components-1))
  ndvi_var <- explvar(ndvi2019_models[[pollutants[i]]]) %>% as.vector() %>% 
    append(rep(NA, n_components-1))
   
  ec_var <- explvar(ec2019_models[[pollutants[i]]]) %>% as.vector() %>%
    append(rep(NA, n_components-1))
  
  var0 <- rbind(space_var,
        pop_var,
        ndvi_var,
        ec_var
        ) %>%
    as.data.frame() %>%
    rename_all( ~ names(explvar(space_models$bc))) %>%
    rownames_to_column(var = "Model") %>%
    mutate(
      Model = recode_factor(factor(Model),
                            space_var = "Space",
                            pop_var = "Population Density",
                            ndvi_var = "NDVI",
                            ec_var = "EC"
                            ),
      Pollutant = toupper(pollutants[i])
      ) %>%
    select(Pollutant, everything())
  
  variance_explained <- rbind(variance_explained, var0)
   
}

variance_explained %>% kable(caption = 
                               paste0("Percent of variability explained by each PLS model for annual average ", 
                                      toupper(pollutants[i] )),
                             digits = 1
                             ) %>%
  kable_styling() 

```


**Geocovariate loadings from models fit to 2019 MM observations**

Space models

```{r, fig.height=10}

pls_loadings <- data.frame()

for(i in seq_along(pollutants)) {
  #i=1
  pls_loadings0 <- as.data.frame(space_models[[pollutants[i]]]$loadings[]) %>%
    rownames_to_column(var = "cov") %>%
    # rename variables if buffers
    split_cov_name(cov = "cov") %>%
    #make long format for faceting
    gather(key = "Component", value = "Loading", contains("Comp")) %>%
    mutate(Component = as.numeric(substr(Component, 6, nchar(Component))),
           pollutant = toupper(pollutants[i])
           )   
  
  pls_loadings <- rbind(pls_loadings, pls_loadings0)
  
}

pls_loadings  %>%
  #buffered covariates
  drop_na(buffer) %>%
  ggplot(aes(x = Loading, y = cov)) +
  geom_point(aes(size=buffer, col=buffer), shape=1) +
  scale_size(breaks = c(min(pls_loadings$buffer, na.rm = T),
                        max(pls_loadings$buffer, na.rm = T))) +  
  #non-buffered covariates
  geom_point(data = pls_loadings[is.na(pls_loadings$buffer),],
           aes(shape="")) +
  geom_vline(xintercept=0,
             linetype="solid") +
  facet_grid(pollutant~Component, labeller = "label_both") +
  labs(y = "Geocovariate",
       shape= "non-buffer", 
       size = "buffer size (m)",
       col = "buffer size (m)",
       title = paste0(toupper(pollutants[i]), " PLS geocovariate component loadings for space-varyign covariates")) +
  theme(legend.position = "bottom") 

```

TVC models 

```{r}

########################################################################################################
# returns PLS loading plots for TVCs and each pollutant

loading_plots_tvcs <- function(dt, tvc_name) {
  
  #plot_list <- list()
  pls_loadings <- data.frame()
  
  for(i in seq_along(pollutants)) {
    #i=1
    pls_loadings0 <- as.data.frame(dt[[pollutants[i]]]$loadings[]) %>%
      rownames_to_column(var = "cov") %>%
      # rename variables if buffers
      split_cov_name(cov = "cov") %>%
      #make long format for faceting
      gather(key = "Component", value = "Loading", contains("Comp")) %>%
      mutate(Component = as.numeric(substr(Component, 6, nchar(Component))),
             Pollutant = toupper(pollutants[i])
             )   
  
  pls_loadings <- rbind(pls_loadings, pls_loadings0)
  
  }
  
  plot <- pls_loadings  %>%
    mutate(buffer = factor(buffer)) %>%
    ggplot(aes(x = Loading, y = cov, size=buffer, col=buffer)) +
    geom_point(shape=1) +
    geom_vline(xintercept=0,
               linetype="solid") +
    facet_grid(Pollutant~Component, labeller = "label_both") +
    labs(y = "Geocovariate",
         title = paste0(tvc_name, " component loadings for PLS model")
         ) +
    theme(legend.position = "bottom") 
    
  #   plot_list[i] <- list(plot)
  #   names(plot_list)[i] <- pollutants[i]
  # }
  
  return(plot)
  
  }

########################################################################################################

```

```{r}
loading_plots_tvcs(dt = pop2010_models, tvc_name = paste0("population density (", pop_model_yr, ")"))
  
loading_plots_tvcs(dt = ndvi2019_models, tvc_name = paste0("NDVI (", ndvi_model_yr, ")"))  

loading_plots_tvcs(dt = ec2019_models, paste0("EC (", emissions_model_yr, ")"))   

# sensitivity
loading_plots_tvcs(dt = nox2019_models, paste0("NOx (", emissions_model_yr, ")"))   
                                                      
                      
```

```{r}

#list of different datasets where we want PLS scores
## these already have NOx covariates
dt_list <- list(mm = mm, 
                cohort = cohort, 
                grid = grid,
                aqs = aqs)

```

**Use 2019 mobile monitoring fitted models to estimate PLS scores for other times and/or locations.** 

space-varying covariates

```{r}

# place to save PLS scores 
bc_scores_space <- list()
ufp_scores_space <- list()

for(i in seq_along(dt_list)) {
  #i=1
  #get scores for each dataset in list using BC PLS model
  bc_scores_space[i] <- list(get_scores(dt = dt_list[[i]], 
                              rename_vars = FALSE, 
                              pls_model = space_models$bc, 
                              rename_components = paste0("space", c(1:n_components)))
                             )
  
  ufp_scores_space[i] <- list(get_scores(dt = dt_list[[i]], 
                              rename_vars = FALSE, 
                              pls_model = space_models$ufp, 
                              rename_components = paste0("space", c(1:n_components)))
                             )
  
  names(bc_scores_space)[i] <- names(dt_list)[i]
  names(ufp_scores_space)[i] <- names(dt_list)[i]

}


```

Population density

```{r}
 
####################################################################################################

# for POP scores. repeates get_scores() for different datasests (mm, cohort, grid), pollutants (BC, UFP) and population years (1990, 2000, 2010)

get_scores_pop_models <- function( 
  #diff datasets
  .dt,
  # diff pollutant models
  .pls_model#,
  # variables used to build models (e.g., for renaming )
  #.pop_model_vars = pop_model_vars
  ) {
  
  # scores for Census 2010
  pop2010_scores <- get_scores(dt = .dt, 
                               # no need to change var names here
                               rename_vars = FALSE,
                               pls_model = .pls_model,
                               rename_components = "pop2010") 
  
  # scores for Census 2000
  pop2000_scores <- get_scores(dt = .dt, 
                               #var names to change to match model variables (for estimating/predicting scores)
                               rename_vars = TRUE, 
                               change_var_name = "pop_",
                               pls_model = .pls_model, 
                               rename_components = "pop2000") 

  # scores for Census 1990 
  pop1990_scores <- get_scores(dt = .dt, 
                               rename_vars = TRUE, 
                               change_var_name = "pop90_",
                               pls_model = .pls_model, 
                               rename_components = "pop1990")
  
  #combine scores for diff years
  pop_scores <- cbind(pop1990_scores, pop2000_scores, pop2010_scores) 
  
  return(pop_scores)
  
  }

####################################################################################################

```

```{r}

# place to save PLS scores 
bc_scores_pop <- list()
ufp_scores_pop <- list()

for (i in seq_along(dt_list)) {
  #i=1
  bc_scores_pop[i] <- list(get_scores_pop_models(.dt = dt_list[[i]], .pls_model = pop2010_models$bc) )
  ufp_scores_pop[i] <- list(get_scores_pop_models(.dt = dt_list[[i]], .pls_model = pop2010_models$ufp) )
  
  names(bc_scores_pop)[i] <- names(dt_list)[i]
  names(ufp_scores_pop)[i] <- names(dt_list)[i]
}


```

NDVI

```{r}
####################################################################################################

# for NDVI scores. repeates get_scores() for different datasests (mm, cohort, grid), pollutants (BC, UFP) and NDVI years (1990-93, 2010, 2019)

get_scores_ndvi_models <- function( 
  #diff datasets
  .dt,
  # diff pollutant models
  .pls_model
  ) {
  
  # scores for NDVI modeling years
  ndvi2019_scores <- get_scores(dt = .dt, 
                               # no need to change var names here
                               rename_vars = FALSE,
                               pls_model = .pls_model,
                               rename_components = "ndvi2019") 
  
  # # scores for other years
  ndvi1990_scores <- get_scores(dt = .dt,
                               #var names to change to match model variables (for estimating/predicting scores)
                               rename_vars = TRUE,
                               change_var_name = "ndvi90_",
                               pls_model = .pls_model,
                               rename_components = "ndvi1990")

  ndvi2010_scores <- get_scores(dt = .dt,
                               rename_vars = TRUE,
                               change_var_name = "ndvi10_",
                               pls_model = .pls_model,
                               rename_components = "ndvi2010")
  
  #combine scores for diff years
  ndvi_scores <- cbind(
    ndvi1990_scores, 
    ndvi2019_scores, 
    ndvi2010_scores
    )  
  
  return(ndvi_scores)
  
  }

####################################################################################################

```


```{r}
 # place to save PLS scores 
bc_scores_ndvi <- list()
ufp_scores_ndvi <- list()

for (i in seq_along(dt_list)) {
  #i=1
  bc_scores_ndvi[i] <- list(get_scores_ndvi_models(.dt = dt_list[[i]], .pls_model =ndvi2019_models$bc) )
  ufp_scores_ndvi[i] <- list(get_scores_ndvi_models(.dt = dt_list[[i]], .pls_model = ndvi2019_models$ufp) )
  
  names(bc_scores_ndvi)[i] <- names(dt_list)[i]
  names(ufp_scores_ndvi)[i] <- names(dt_list)[i]
}


 
```
 

emissions

```{r}
get_scores_ec_models <- function( 
  #diff datasets
  .dt,
  # diff pollutant models
  .pls_model,
  #emissions covariates to use/look for. for sensitivity analyses, change this to "nox"
  emissions = "ec"
  ) {
  
  
  pattern1 <- paste0("^", emissions, "[0-9]{4}")
  # ec names
  df_names <- str_match(names(.dt), pattern = pattern1 #"^ec[0-9]{4}"
                        ) %>% 
    unique() %>% as.vector()
  
  
  # drop "NA"
  df_names <- df_names[!is.na(df_names)]

  #get scores for other, non-modeled years
  df <- as.data.frame(matrix(nrow=nrow(.dt)))
  
  for (i in seq_along(df_names)) {
    #i=1
    temp <- get_scores(dt = .dt, 
                               #var names to change to match model variables (for estimating/predicting scores)
                               rename_vars = TRUE, 
                              # vars to change names to model vars for score estimation        
                               change_var_name = paste0(df_names[i], "_"),  
                              # model to use for score estimateion        
                              pls_model = .pls_model, 
                              # new component name
                              rename_components = df_names[i]) 
    df <- cbind(df, temp)

  }
  
  #drop NA column
  df <- df %>% select(df_names)
  
  return(df)
  
  }


```

```{r}
# place to save PLS scores 
bc_scores_ec <- list()
ufp_scores_ec <- list()

### --> change c(1,3,4) to seq_along(dt_list) when get cohort estimates

#for (i in c(1,3,4) ) {   
for (i in seq_along(dt_list)) {   
  #i=2 #cohort
  bc_scores_ec[i] <- list(get_scores_ec_models(.dt = dt_list[[i]], .pls_model = ec2019_models$bc) )
  ufp_scores_ec[i] <- list(get_scores_ec_models(.dt = dt_list[[i]], .pls_model = ec2019_models$ufp) )
  
  names(bc_scores_ec)[i] <- names(dt_list)[i]
  names(ufp_scores_ec)[i] <- names(dt_list)[i]
}

```

```{r}
# nox 

# place to save PLS scores 
bc_scores_nox <- list()
ufp_scores_nox <- list()

### --> change c(1,3,4) to seq_along(dt_list) when get cohort estimates

#for (i in c(1,3,4) ) {   
for (i in seq_along(dt_list)) {   
  #i=1
  bc_scores_nox[i] <- list(get_scores_ec_models(.dt = dt_list[[i]], .pls_model = nox2019_models$bc,
                                                emissions = "nox"
                                                ))
  ufp_scores_nox[i] <- list(get_scores_ec_models(.dt = dt_list[[i]], .pls_model = nox2019_models$ufp,
                                                emissions = "nox"
                                                ))
  
  names(bc_scores_nox)[i] <- names(dt_list)[i]
  names(ufp_scores_nox)[i] <- names(dt_list)[i]
}


```


**Combine all PLS scores**

Datasets of linear combinations of geocovariates (space, pop, ndvi and emissions)

```{r}

# BC
bc_scores_mm0 <- cbind(mm[site_loc_vars],
                   bc_scores_space$mm,
                   bc_scores_pop$mm,
                   bc_scores_ndvi$mm, 
                   bc_scores_ec$mm,
                   
                   # sensitivity
                   bc_scores_nox$mm
                   
                   ) %>%
  mutate(Pollutant = "bc")

bc_scores_cohort0 <- cbind(cohort[site_loc_vars],
                   bc_scores_space$cohort,
                   bc_scores_pop$cohort,
                   
                   bc_scores_ndvi$cohort, 
                   
                   bc_scores_ec$cohort,
                   # sensitivity
                   bc_scores_nox$cohort
                   
                   ) %>% 
  mutate(Pollutant = "bc")

bc_scores_grid0 <- cbind(grid[site_loc_vars],
                   bc_scores_space$grid,
                   bc_scores_pop$grid,
                   
                   bc_scores_ndvi$grid, 
                   bc_scores_ec$grid,
                   
                   #sensitivity
                   bc_scores_nox$grid
                   ) %>% 
  mutate(Pollutant = "bc")

bc_scores_aqs0 <- cbind(aqs[site_loc_vars],
                   bc_scores_space$aqs,
                   bc_scores_pop$aqs,

                   bc_scores_ndvi$aqs,
                   bc_scores_ec$aqs,
                   bc_scores_nox$aqs
                   )%>%
  mutate(Pollutant = "bc")

 
# UFP
ufp_scores_mm0 <- cbind(mm[site_loc_vars],
                   ufp_scores_space$mm,
                   ufp_scores_pop$mm,
                   
                   ufp_scores_ndvi$mm, 
                   ufp_scores_ec$mm,
                   
                   #sensitivity
                   ufp_scores_nox$mm
                   ) %>% 
  mutate(Pollutant = "ufp")

ufp_scores_cohort0 <- cbind(cohort[site_loc_vars],
                   ufp_scores_space$cohort,
                   ufp_scores_pop$cohort,
                   
                   ufp_scores_ndvi$cohort, 
                   ufp_scores_ec$cohort,
                   ufp_scores_nox$cohort
                   ) %>% 
  mutate(Pollutant = "ufp")

ufp_scores_grid0 <- cbind(grid[site_loc_vars],
                   ufp_scores_space$grid,
                   ufp_scores_pop$grid,
                   
                   ufp_scores_ndvi$grid, 
                   ufp_scores_ec$grid,
                   
                   #sensitivity
                   ufp_scores_nox$grid
                   ) %>% 
  mutate(Pollutant = "ufp")

ufp_scores_aqs0 <- cbind(aqs[site_loc_vars],
                   ufp_scores_space$aqs,
                   ufp_scores_pop$aqs,

                   ufp_scores_ndvi$aqs,
                   ufp_scores_ec$aqs,
                   
                   #sensitivity
                   ufp_scores_nox$aqs
                   ) %>%
  mutate(Pollutant = "ufp")




# combine before doing interpollation

mm_scores0 <- rbind(bc_scores_mm0, ufp_scores_mm0)
cohort_scores0 <- rbind(bc_scores_cohort0, ufp_scores_cohort0)
grid_scores0 <- rbind(bc_scores_grid0, ufp_scores_grid0)
aqs_scores0 <- rbind(bc_scores_aqs0, ufp_scores_aqs0)

```


Plots of TVCs over time.

* Population PLS scores generally increase over time (increasing population density?)

  - the shape of the density curves is very similar. Using a simple trend could generally capture changes over time?   
  - The largest difference could be in 2010 where we see a few locations with very high values.

* NDVI ______    
* EC emissions ______


Mobile monitorig PLS scores

```{r}
mm_scores0 %>%
  # select(
  #   Pollutant,
  #   space1, space2, pop2010, 
  #   ### --> update
  #   ndvi2006,
  #   ec2019
  #   ) %>%
   
  gather("TVC", "value", #space1:ec2019 #c(
    # space1, space2, pop2010, 
    # ### --> update
    # ndvi2006,
    # ec2019
    
   paste0("pop", census_data_yrs),
   paste0("ndvi", ndvi_data_yrs),
   paste0("ec", emissions_data_yrs),
   contains("space")
         ) %>%
  separate(col = TVC, 
           into = c("TVC", "Year"), 
           sep = "(?<=[A-Za-z])(?=[0-9])" 
           ) %>%
  mutate(TVC = toupper(TVC)) %>%
  label_pollutant(var = "Pollutant", label = "pollutant") %>%
  group_by(Pollutant, Covariates=TVC, Year) %>%
  distribution.table(var.string = "value", round.int = 1) %>%
  kable(caption = "Distribution of PLS scores for the mobile monitoring locations, by pollutant and TVC") %>%
  kable_styling()

```

```{r}
cohort_scores0 %>%
  gather("TVC", "value", c(paste0("pop", census_data_yrs), 
                           paste0("ndvi", ndvi_data_yrs),
                           paste0("ec", emissions_data_yrs)
                           ) 
         ) %>%
  separate(col = TVC, 
           into = c("TVC", "Year"), 
           sep = "(?<=[A-Za-z])(?=[0-9])" 
           ) %>%
  mutate(TVC = toupper(TVC)) %>%
  label_pollutant(var = "Pollutant", label = "pollutant") %>%
  group_by(Pollutant, TVC, Year) %>%
  distribution.table(var.string = "value", round.int = 1) %>%
  kable(caption = "Distribution of PLS scores for the cohort, by pollutant and TVC") %>%
  kable_styling()
```

```{r}

# grid_scores0 %>%
#   #            --> update when get new emission covariates
#   gather("tvc", "value", starts_with(c("pop" #, "ndvi", "ec"
#                                        ))) %>%
#   mutate(
#     Year = substr(tvc, nchar(tvc)-3, nchar(tvc)),
#     tvc = substr(tvc, 1, nchar(tvc)-4),
#     
#     #rename TVCs - for plotting
#     tvc = recode_factor(factor(tvc),
#                         "pop" = "Population Density",
#                         "ndvi" = "NDVI"
#                         ),
#     Pollutant = toupper(Pollutant)
#   ) %>%
# 
#   ggplot(aes(x=value, fill=Year)) + 
#   geom_density(alpha=0.3) + 
#   facet_grid(Pollutant~tvc, scales="free") + 
#   labs(
#     title = "PLS scores for TVCs at grid locations",
#     x = "PLS score"
#     #subtitle = ""  
#   ) +
#   scale_fill_brewer(palette = "Spectral")  


```


Compare the distribution of the PLS scores at MM locations used to fit & transform all other geocovariates at cohort locations to linear combinations


```{r}
mm_og_scores <- mm_scores0 %>%
  select(
    Pollutant,
    #space1, space2, 
    pop2010, 
    ndvi2019,
    ec2019,
    
    #sensitivity
    nox2019
    ) %>%
  gather("TVC", "value", c(pop2010, ndvi2019, ec2019,
                           # sensitivity
                           nox2019
                           )) %>%
  separate(col = TVC, 
           into = c("TVC", "Year"), 
           sep = "(?<=[A-Za-z])(?=[0-9])" 
           )  %>%
  mutate(Location = "Mobile Monitoring",
         #Alpha=0.7,
         #Pollutant = toupper(Pollutant)
         )  

score_comp <- cohort_scores0 %>%  
  select(
    Pollutant,
    #space1, space2, 
    #all years prior to interpolation
    starts_with(c("pop", "ndvi", "ec",
                  # sensitivity
                  "nox"
                  ))
    ) %>%  
  
  gather("TVC", "value", starts_with(c( "pop", "ndvi", "ec",
                                        # sensitivity
                                        "nox"
                                        ))
         ) %>%
     separate(col = TVC, 
           into = c("TVC", "Year"), 
           sep = "(?<=[A-Za-z])(?=[0-9])" 
           )  %>%
  mutate(
    Location = "Cohort",  
    #Alpha=0.3,
    
  ) %>% 
  rbind(mm_og_scores) %>%
  mutate(
    Pollutant = toupper(Pollutant),
    TVC = toupper(TVC),
    Location = factor(Location, levels = c("Mobile Monitoring", 
                                           "Cohort" #"Grid"
                                           ))
    
  )

```

```{r, fig.height=10}

score_comp %>%
  filter(!grepl("nox", TVC, ignore.case = T),
    TVC == "POP" & Year >= 1990 |
      TVC == "NDVI" & Year >= 1990 |
      Year >=1995
         ) %>%
    #mutate(Year = as.numeric(Year)) %>%
  #View()
  ggplot(aes(y=value,
             x=Year, #group=Year,
             #fill=Location
             )
         ) +
  geom_boxplot(aes(
    #fill=Location, 
    col = Location
  )) +
  geom_smooth() +
  facet_grid(TVC~Pollutant,
              scales="free", 
             labeller = "label_both"
              ) +
  labs(y = "PLS Score",
       title = "Distribution of PLS scores"
       )

```

NOx and EC emission covariates are almost identical 

```{r}
# nox
score_comp %>%
  filter(grepl("nox|ec", TVC, ignore.case = T),
      Year >=1995
         ) %>%
  ggplot(aes(y=value,
             x=Year, #group=Year,
             #fill=Location
             )
         ) +
  geom_boxplot(aes(
    col = Location
  )) +
  geom_smooth() +
  facet_grid(TVC~Pollutant,
              scales="free", 
             labeller = "label_both"
              ) +
  labs(y = "PLS Score",
       title = "Distribution of TVC emission PLS scores"
       )


```


* the best fit line is not parallel to the 1-1 line. In general, higher density areas had a greater increase in population density over time than lower density areas.
* Some sites deviate from the general trend more than others.    


TVCs will characterize temporal site changes better than our trend adjustment at sites where the change in population density between 1990 and 2010 differs from what would be our trend adjustment (i.e., a parallel line above the 1-1 line)

```{r}
cohort_scores0 %>% #grid_scores0 %>%
  gather(key = "Census", value = "Value", contains("pop"), -contains("90")) %>%
  mutate(
    Census = str_extract(Census, "\\d{4}"),
    Pollutant = toupper(Pollutant)
  ) %>%
  ggplot(aes(x=pop1990, y=Value, col=Census)) + 
  geom_point(alpha=0.02) + 
  #geom_smooth(se=F) + 
  geom_smooth(method = "lm", aes(linetype="lm") ) +  #
  geom_abline(aes(slope = 1, intercept = 0, linetype= "1-1")) +
  facet_grid(Census~Pollutant) + 
  labs(
    title = "Cohort population density PLS scores over time",
    x = "1990 Census",
    y = "PLS Score",
    linetype = ""
  )


```

 
## Inter- and intra-polation of PLS scores over time

Use linear inter- and intra-pollation to estimate PLS scores for otherwise unavailable years: 

* Population density: 

  - at the individual level, using decenial US Census block group information: interpolate 1991-1999 (using 1990 and 2000 linear fit) and 2001-2009 (using 2000 and 2010 linear fit)    
  - at the metropolitan area level, using annual US Census ACS survey (since 2020 decenail Census survey is not yet available): inter- and intral-polate 2011-2019 (using 2010-2018 linear fit).    
    - Note: unlike adjusting at the individual level, this approach assumes that all places increased in population density the same degree

* NDVI: use the same NDVI for 1990-1993 (NDVI estimated using images from 1990-1993), 1994-2005 (using 1990-1993 and 2006 linear fit), 2007-2009 (using 2006-2010 linear fit), and ??? __the same NDVI for 2010-2019 (since we are unsure of the temporal trend post 2010)__  

* emissions: use linear interpolation from 5 yr estimates to estimate missing years (e.g., use 1990-1995 fit to estimate 1991-1994).
 

- Population density

### --> ? issue: adding ~ 2%/yr increase after 2010, but PLS scores close to 0 are not changing much 

```{r}
# returns pop interpollated values for unavailable years 

interpolate_pop <- function(dt) {
  
  dt <- dt %>%
    mutate(
      # calculate yearly change
      pop1990_2000_slope = (pop2000 - pop1990)/10,
      pop2000_2010_slope = (pop2010 - pop2000)/10
    ) #%>% cbind(interpolated_df0)
  
  # use 1991-1999 individual-level fit to interpollate 
  for(i in 1:9){
    #i=1
    yr <- 1990+i
    
    var <- paste0("pop", yr)
    dt[var] <- dt$pop1990 + dt$pop1990_2000_slope*i
  }
  
  # use 2001-2009 individual-level fit to interpollate  
  for(i in c(1:9)){
    #i=1
    yr <- 2000+i
    
    var <- paste0("pop", yr)
    dt[var] <- dt$pop2000 + dt$pop2000_2010_slope*i
  }
  
  # use 2001-2019 metro area-level fit to inter- and intra-pollate 
  for(i in c(1:9)){
    #i=1
    yr <- 2010+i
    
    var <- paste0("pop", yr)
    # add the same percent annual increase as the Sea-tac-bell area
                            # convert from proportion change/yr to linear combination units. since the proportion slope is positive, make sure the linear combination units being added also are
    dt[var] <- dt$pop2010 + (abs(dt$pop2010* pop_2010_2018_prop*i))
  }
  
  return(dt)
  
}
```


```{r}
# BC
bc_mm_scores <- interpolate_pop(dt = bc_scores_mm0) %>%
  mutate(Pollutant = "bc",
         # add observed concentration
         value = mm[["bc"]]) %>%
  select(Pollutant, value, everything())
  
bc_cohort_scores <- interpolate_pop(dt = bc_scores_cohort0)%>%
  mutate(Pollutant = "bc",
         #no obsrved concentration, but need this column for modeling later
         value = NA) %>%
  select(Pollutant, value, everything())

bc_grid_scores <- interpolate_pop(dt = bc_scores_grid0)%>%
  mutate(Pollutant = "bc",
         value = NA) %>%
  select(Pollutant, value, everything())

bc_aqs_scores <- interpolate_pop(dt = bc_scores_aqs0)%>%
  mutate(Pollutant = "bc",
         value = NA) %>%
  select(Pollutant, value, everything())

# UFP
ufp_mm_scores <- interpolate_pop(dt = ufp_scores_mm0) %>%
  # add observed pollutant concentration
  mutate(Pollutant = "ufp",
         value = mm[["ufp"]]) %>%
  select(Pollutant,value, everything())

ufp_cohort_scores <- interpolate_pop(dt = ufp_scores_cohort0)%>%
  mutate(Pollutant = "ufp",
         value = NA) %>%
  select(Pollutant, value, everything())

ufp_grid_scores <- interpolate_pop(dt = ufp_scores_grid0) %>%
  mutate(Pollutant = "ufp",
         value = NA) %>%
  select(Pollutant, value, everything())

ufp_aqs_scores <- interpolate_pop(dt = ufp_scores_aqs0) %>%
  mutate(Pollutant = "ufp",
         value = NA) %>%
  select(Pollutant, value, everything())

# ## combine pollutants
# mm_scores <- rbind(bc_mm_scores, ufp_mm_scores)
# cohort_scores <- rbind(bc_cohort_scores, ufp_cohort_scores)
# grid_scores <- rbind(bc_grid_scores, ufp_grid_scores)
# aqs_scores <- rbind(bc_aqs_scores, ufp_aqs_scores) %>%
#   #drop aqs sites w/o geocovariats. no aqs sites have "value" estimates
#   drop_na(-value) 

```

- Emissions

```{r}
# fn returns interpollated values every 5 yrs for EC

#dt = bc_mm_scores

interpolate_ec <- function(dt) {
  
  dt <- dt %>%
    mutate(
      # calculate yearly change
      ec1990_1995_slope = (ec1995 - ec1990)/5,
      ec1995_2000_slope = (ec2000 - ec1995)/5,
      ec2000_2005_slope = (ec2005 - ec2000)/5,
      ec2005_2010_slope = (ec2010 - ec2005)/5,
      ec2010_2015_slope = (ec2015 - ec2010)/5,
      ec2015_2019_slope = (ec2019 - ec2015)/4
      )  
  
  #interpolate every 5 yrs between 1990-2015
  emissions_data_yrs1 <- seq(1990, 2010, 5)
  
  for (j in seq_along(emissions_data_yrs1)) {
    #j=5
    yr_start <- emissions_data_yrs1[j]
    yr_start_var <- paste0("ec", yr_start)
    slope_var <- paste0(yr_start_var, "_", yr_start+5, "_slope")
  
    for(i in 1:4){
      #i=4
      yr <- yr_start+i
      var <- paste0("ec", yr)
      dt[var] <- dt[yr_start_var] + dt[slope_var]*i
    }
  }
  
  #last years have a shorter interval
  for(i in 1:3){
    yr <- 2015+i
    var <- paste0("ec", yr)
    dt[var] <- dt$ec2015 + dt$ec2015_2019_slope*i
  }
  
  return(dt)
  
}

```

```{r}
# BC
bc_mm_scores1 <- interpolate_ec(dt = bc_mm_scores)  

bc_cohort_scores1 <- interpolate_ec(dt = bc_cohort_scores)  # 

bc_grid_scores1 <- interpolate_ec(dt = bc_grid_scores)  
bc_aqs_scores1 <- interpolate_ec(dt = bc_aqs_scores) 

# UFP
ufp_mm_scores1 <- interpolate_ec(dt = ufp_mm_scores)  
ufp_cohort_scores1 <- interpolate_ec(dt = ufp_cohort_scores) #ufp_cohort_scores1 <- ufp_cohort_scores

ufp_grid_scores1 <- interpolate_ec(dt = ufp_grid_scores) 
ufp_aqs_scores1 <- interpolate_ec(dt = ufp_aqs_scores)  

```

```{r}
# NOX

## rename "nox" as "ec" so we can use the same fn

# BC
bc_mm_scores1 <- bc_mm_scores1 %>%
  select(-contains("ec")) %>%
  #rename "nox" to "ec" so fn works
  rename_all(~gsub("nox", "ec", .) ) %>%
  ##also works
  #rename_at(vars(contains("nox")), ~gsub("nox", "ec", .) ) %>%
  interpolate_ec(dt = .) %>%
  #rename "ec" back to nox
  rename_at(vars(starts_with("ec")), ~gsub("ec", "nox", .) ) %>%
  # add to other interpolated scores
  right_join(bc_mm_scores1)  

  

    ### --> DONE. # UPDATE when get cohort covars. replace next line w/ one below it
#bc_cohort_scores1 <- bc_cohort_scores
bc_cohort_scores1 <- bc_cohort_scores1  %>%
  select(-contains("ec")) %>%
  #rename "nox" to "ec" so fn works
  rename_all(~gsub("nox", "ec", .) ) %>%
  interpolate_ec(dt = .) %>%
  #rename "ec" back to nox
  rename_at(vars(starts_with("ec")), ~gsub("ec", "nox", .) ) %>%
  # add to other interpolated scores
  right_join(bc_cohort_scores1)


bc_grid_scores1 <- bc_grid_scores1 %>%
  select(-contains("ec")) %>%
  #rename "nox" to "ec" so fn works
  rename_all(~gsub("nox", "ec", .) ) %>%
  interpolate_ec(dt = .) %>%
  #rename "ec" back to nox
  rename_at(vars(starts_with("ec")), ~gsub("ec", "nox", .) ) %>%
  right_join(bc_grid_scores1)

bc_aqs_scores1 <- bc_aqs_scores1 %>%
  select(-contains("ec")) %>%
  #rename "nox" to "ec" so fn works
  rename_all(~gsub("nox", "ec", .) ) %>%
  interpolate_ec(dt = .) %>%
  #rename "ec" back to nox
  rename_at(vars(starts_with("ec")), ~gsub("ec", "nox", .) )  %>%
  right_join(bc_aqs_scores1)

# UFP
ufp_mm_scores1 <- ufp_mm_scores1 %>%
  select(-contains("ec")) %>%
  #rename "nox" to "ec" so fn works
  rename_all(~gsub("nox", "ec", .) ) %>%
  interpolate_ec(dt = .) %>%
  #rename "ec" back to nox
  rename_at(vars(starts_with("ec")), ~gsub("ec", "nox", .) )  %>%
  right_join(ufp_mm_scores1)

### --> DONE. # UPDATE when get cohort covars. replace next line w/ one below it

# bc_cohort_scores1 <- bc_cohort_scores1  %>%
#   select(-contains("ec")) %>%
#   #rename "nox" to "ec" so fn works
#   rename_all(~gsub("nox", "ec", .) ) %>%
#   interpolate_ec(dt = .) %>%
#   #rename "ec" back to nox
#   rename_at(vars(starts_with("ec")), ~gsub("ec", "nox", .) ) %>%
#   # add to other interpolated scores
#   right_join(bc_cohort_scores1)

#ufp_cohort_scores1 <- ufp_cohort_scores
ufp_cohort_scores1 <- ufp_cohort_scores1 %>%
  select(-contains("ec")) %>%
  #rename "nox" to "ec" so fn works
  rename_all(~gsub("nox", "ec", .) ) %>%
  interpolate_ec(dt = .) %>%
  #rename "ec" back to nox
  rename_at(vars(starts_with("ec")), ~gsub("ec", "nox", .) ) %>%
  right_join(ufp_cohort_scores1)

ufp_grid_scores1 <- ufp_grid_scores1 %>%
  select(-contains("ec")) %>%
  #rename "nox" to "ec" so fn works
  rename_all(~gsub("nox", "ec", .) ) %>%
  interpolate_ec(dt = .) %>%
  #rename "ec" back to nox
  rename_at(vars(starts_with("ec")), ~gsub("ec", "nox", .) )  %>%
  right_join(ufp_grid_scores1)

ufp_aqs_scores1 <- ufp_aqs_scores1 %>%
  select(-contains("ec")) %>%
  #rename "nox" to "ec" so fn works
  rename_all(~gsub("nox", "ec", .) ) %>%
  interpolate_ec(dt = .) %>%
  #rename "ec" back to nox
  rename_at(vars(starts_with("ec")), ~gsub("ec", "nox", .) ) %>%
  right_join(ufp_aqs_scores1)


```


- NDVI

```{r}
# returns dataset w/ new variables for interpolated NDVI 

interpolate_ndvi <- function(dt) {
  
  dt <- dt %>%
    mutate(
      # calculate yearly change
      ndvi1990_2010_slope = (ndvi2010 - ndvi1990)/20,
      ndvi2010_2019_slope = (ndvi2019 - ndvi2010)/9,
      )  
  
  
  # use 1990-2010 individual-level fit to interpollate 
  for(i in 1:19){
    #i=1
    yr <- 1990+i
    
    var <- paste0("ndvi", yr)
    dt[var] <- dt$ndvi1990 + dt$ndvi1990_2010_slope*i
  }
  
  # use 2010-2019 individual-level fit to interpollate  
  for(i in c(1:8)){
    #i=1
    yr <- 2010+i
    
    var <- paste0("ndvi", yr)
    dt[var] <- dt$ndvi2010 + dt$ndvi2010_2019_slope*i
  }
  
  
  return(dt)
  
}

```

```{r}
# BC
bc_mm_scores1 <- interpolate_ndvi(dt = bc_mm_scores1)  

bc_cohort_scores1 <- interpolate_ndvi(dt = bc_cohort_scores1)  # 

bc_grid_scores1 <- interpolate_ndvi(dt = bc_grid_scores1)  
bc_aqs_scores1 <- interpolate_ndvi(dt = bc_aqs_scores1) 

# UFP
ufp_mm_scores1 <- interpolate_ndvi(dt = ufp_mm_scores1)  
ufp_cohort_scores1 <- interpolate_ndvi(dt = ufp_cohort_scores1)  

ufp_grid_scores1 <- interpolate_ndvi(dt = ufp_grid_scores1) 
ufp_aqs_scores1 <- interpolate_ndvi(dt = ufp_aqs_scores1)  


```


```{r}
## combine pollutants
mm_scores <- rbind(bc_mm_scores1, ufp_mm_scores1)
cohort_scores <- rbind(bc_cohort_scores1, ufp_cohort_scores1)
grid_scores <- rbind(bc_grid_scores1, ufp_grid_scores1)
aqs_scores <- rbind(bc_aqs_scores1, ufp_aqs_scores1) %>%
  #drop aqs sites w/o geocovariats. no aqs sites have "value" estimates
  drop_na(-value) 

```


Plots show a steady change in TVC values over time. 

for **population density**, values generally increase from 1990-2000. Values are more dispersed in 2010 than 2000, hence why we see increases in value over time.


PLS distribution:

-table 

```{r, eval=F}
#           sep = "(?<=[A-Za-z])(?=[0-9])"  

cohort_scores %>%  
  gather("TVC", "value", contains(c(paste0("pop", c(census_data_yrs, last_acs_yr)), 
                           paste0("ndvi", ndvi_data_yrs),
                           paste0("ec", emissions_data_yrs)
                           ),
                           -contains("slope")
                           ) 
         ) %>%  
  separate(col = TVC, 
           into = c("TVC", "Year"), 
           sep = "(?<=[A-Za-z])(?=[0-9])" 
           ) %>%
  mutate(TVC = toupper(TVC)) %>%
  label_pollutant(var = "Pollutant", label = "pollutant") %>%
  group_by(Pollutant, TVC, Year) %>%
  distribution.table(var.string = "value", round.int = 1) %>%
  kable(caption = "Distribution of PLS scores for the cohort, by pollutant and TVC") %>%
  kable_styling()
  
```

```{r}
# nox

cohort_scores %>%
  gather("TVC", "value", contains(c(paste0("ec", emissions_data_yrs),
                           paste0("nox", emissions_data_yrs)
                           )
                           ),
         -contains(c("slope"))
         
         ) %>%
  separate(col = TVC, 
           into = c("TVC", "Year"), 
           sep = "(?<=[A-Za-z])(?=[0-9])" 
           ) %>%
  mutate(TVC = toupper(TVC)) %>%
  label_pollutant(var = "Pollutant", label = "pollutant") %>%
  group_by(Pollutant, TVC, Year) %>%
  distribution.table(var.string = "value", round.int = 1) %>%
  kable(caption = "Distribution of emission PLS scores for the cohort, by pollutant and TVC") %>%
  kable_styling()

```

-plot 

### --> —> why does NDVI increase from 1990-2010 but decrease from 2010-2019? 

```{r}
cohort_scores %>% #grid_scores %>%
  gather("TVC", "value", contains(c("pop", "ndvi", "ec")), -contains("slope")) %>%
  separate(col = TVC, 
           into = c("TVC", "Year"), 
           #separate text from numbers: position following letters; 
           sep = "(?<=[A-Za-z])(?=[0-9])" ) %>%
  mutate(
    Year = as.numeric(Year),
    Interpolated = ifelse((Year %in% c(census_data_yrs, last_acs_yr) & grepl("pop", TVC) ) |
      (Year %in% ndvi_data_yrs & grepl("ndvi", TVC)) | 
        (Year %in% emissions_data_yrs & grepl("ec", TVC)),
      #"observed", "interpolated"
      "FALSE", "TRUE"
      ),
    Pollutant = toupper(Pollutant),
    TVC = toupper(TVC)
  ) %>% #View()
ggplot(aes(x=Year, y=value, )) + 
  geom_boxplot(aes(#fill=Interpolated,
    col=Interpolated,
    group=Year), alpha=0.3) + 
  geom_smooth(se=F) +
  facet_grid(TVC~Pollutant, scales="free", labeller = "label_both") + 
  labs(
    title = "Distribution of observed and interpolated TVC PLS scores for the Cohort"
    #fill = "Estimate\nType"
  )  
  
 
 
```

```{r}
#nox
nox_p_interpolated <- cohort_scores %>% #grid_scores %>%
  gather("TVC", "value", contains(c("nox", "ec")), -contains("slope")) %>%
  separate(col = TVC, 
           into = c("TVC", "Year"), 
           #separate text from numbers: position following letters; 
           sep = "(?<=[A-Za-z])(?=[0-9])" ) %>%
  mutate(
    Year = as.numeric(Year),
    Interpolated = ifelse((Year %in% c(census_data_yrs, last_acs_yr) & grepl("pop", TVC) ) |
      (Year %in% ndvi_data_yrs & grepl("ndvi", TVC)) | 
        (Year %in% emissions_data_yrs & grepl("ec|nox", TVC)),
      #"observed", "interpolated"
      "FALSE", "TRUE"
      ),
    Pollutant = toupper(Pollutant),
    TVC = toupper(TVC)
  ) %>%
ggplot(aes(x=Year, y=value, )) + 
  geom_boxplot(aes(#fill=Interpolated, 
    col=Interpolated,
    group=Year), alpha=0.3) + 
  geom_smooth(se=F) +
  facet_grid(TVC~Pollutant, scales="free", labeller = "label_both") +
  
  labs(
    title = "Distribution of observed and interpolated emission TVC PLS scores for the cohort"
    #fill = "Estimate\nType"
  )  

nox_p_interpolated

# same as above but on log scale - warning: drops values < 0
nox_p_interpolated + 
  scale_y_log10()  + 
  labs(subtitle = "y axis is on log10 scale; values < 0 dropped")

```


```{r, eval=F}
# VERY detailed table of pollutant-yr-score

# table
cohort_scores %>% #grid_scores %>%
  gather("tvc", "value", contains(c("pop", "ndvi", "ec")), -contains("slope")) %>%
  separate(col = tvc, 
           into = c("tvc", "Year"), 
           #separate text from numbers: position following letters; 
           sep = "(?<=[A-Za-z])(?=[0-9])" ) %>%
  mutate(
    Pollutant = toupper(Pollutant),
    tvc = toupper(tvc)
  ) %>%
  group_by(Pollutant, tvc, Year) %>%
  summarize(
    N = n(),
    Min = min(value),
    Q25 = quantile(value, 0.25),
    Median = median(value),
    Q75 = quantile(value, 0.75),
    Max = max(value)
  ) %>%
  
  #distribution.table(var.string = "value", round.int = 1) %>%
  kable(caption = "Distribution of TVC PLS scores for the cohort", 
        digits = 2
        ) %>%
  kable_styling()
  
```


```{r, eval=F}
# table - nox
cohort_scores %>% #grid_scores %>%
  gather("tvc", "value", contains(c("nox" #"ec"
                                    )), -contains("slope")) %>%
  separate(col = tvc, 
           into = c("tvc", "Year"), 
           #separate text from numbers: position following letters; 
           sep = "(?<=[A-Za-z])(?=[0-9])" ) %>%
  mutate(
    Pollutant = toupper(Pollutant),
    tvc = toupper(tvc)
  ) %>%
  group_by(Pollutant, tvc, Year) %>%
  summarize(
    N = n(),
    Min = min(value),
    Q25 = quantile(value, 0.25),
    Median = median(value),
    Q75 = quantile(value, 0.75),
    Max = max(value)
  ) %>%
  
  #distribution.table(var.string = "value", round.int = 1) %>%
  kable(caption = "Distribution of emission TVC PLS scores for the cohort", 
        digits = 2
        ) %>%
  kable_styling()

```


* TVC values change over time to different degrees at different locations 

```{r}
site_sample_n <- 150

#cohort_scores
cohort_scores %>%
  gather("Year", "Value", contains(c(paste0("pop", c(census_data_yrs, last_acs_yr)),
                            paste0("ec", c(seq(1995, 2015, 5), 2019)),
                            paste0("ndvi", ndvi_data_yrs)
                            )
                            ),
         -contains("slope")
         
         ) %>%
  mutate(
    TVC = str_extract(Year, "[a-z]{0,}"),
    TVC = toupper(TVC),
    Year = as.numeric(str_extract(Year, "\\d{4}")),
    Pollutant = toupper(Pollutant)
  ) %>%
  #only plot a subsample of sites
  filter(site_id %in% sample(site_id, size = site_sample_n, replace = F)) %>%
  #View()
  
  ggplot(aes(x=Year, y= Value, group=site_id)) + 
  geom_point(alpha=0.2) + 
  geom_line(alpha=0.2) + 
  
  facet_grid(TVC~Pollutant, scales= "free",  labeller = "label_both") + 
  scale_x_continuous(breaks = c(census_data_yrs, last_acs_yr)) +
  labs(
    title = paste0("Population density PLS scores over time \nfor a random sample of cohort locations ", "(n = ",site_sample_n, ")"),
    y = "PLS Score"
  )  

```


Map TVC Surfaces

```{r}
# map visualization ideas
# https://gist.github.com/jebyrnes/f3f626aa24f565d78003f05aab4d0372 

```


```{r}
# FN returns list of mapped PLS scores for particular TVC variables & years

# i=3
# dt= scores_first_last_yr %>%
#     filter(Pollutant == "BC",
#            TVC == tvcs[i]
#            ) %>%
# mutate(diff2019 = y2019 - y1995)
# yrs = "2019"
# vars = "diff"
# legend_title = "PLS\nScore\nDifference"
# map_title = paste0("Change in ", tvcs[i], " TVC (BC model) from 1995-2019")
# low_col = "darkblue"
# high_col = "lightblue"
              

map_scores <- function(
  #data
  dt,
  # years to plot
  yrs,
  #PLS variables to plot
  vars, 
  legend_title = "",
  map_title = "",
  low_col = "yellow",
  high_col = "red"
  ) {
  
  vars <- paste0(vars,  yrs)
  
  #if no label is given, automatically add this label
  if(legend_title == "") {
    legend_title <- "PLS\nScore"
  }
  
  
  #range of the years we're plotting
  plot_range <- data.frame(
    min = min(dt[vars]),
    max = max(dt[vars])
  )
  
  plots_list <- list()
  
  for(i in seq_along(yrs)) {
    #i=3
    
    map_title1 <- map_title
  
    if(map_title1 == "") {
    map_title1 <- paste0(yrs[i])
    }
    
    df <- dt %>%  
    #select that yrs variable
    rename(new_var = vars[i])  
    
    p <- map0 + 
      geom_point(data = df, aes(x=longitude, y=latitude, col = new_var),
                 alpha=0.5
                 ) +
      scale_color_gradient(low = low_col, high = high_col,
                          #make all legends have the same range
                          limits = c(plot_range$min, plot_range$max)) +
    labs(title = map_title1,
         col = legend_title
         ) 
   
    plots_list[i] <- list(p)
  
  }
  
  return(plots_list)

}

```

### --> quantify chaning TVCs over time. e.g. (EC/Pop 2019-1995) ~ geocovariates. what are the drivers of change over time?    
 
Scatterplot comparing first & last perdiction year scores 

```{r}
 scores_first_last_yr <- cohort_scores %>%
  select_at(vars(Pollutant, site_id, latitude, longitude,
                 ends_with(c("1995", "2019"))
                 )   
            ) %>%
  gather("TVC", "value", ends_with(c("1995", "2019"))) %>%
  mutate(
    Year = str_extract(TVC, "\\d{4}" ),
    Year = as.numeric(Year),
    TVC =  str_extract(TVC, "\\D{0,}" ),
    TVC = toupper(TVC),
    Pollutant = toupper(Pollutant)
    
  ) %>%
  spread("Year", "value") %>%
  rename(y1995 = "1995",
         y2019 = "2019"
         )  
  #View()

range1 <- scores_first_last_yr %>%
  select(y1995, y2019) %>%
  summarize(
    min = min(.),
    max = max(.)
  )
  
scores_first_last_yr %>%
  ggplot(aes(x=y1995, y=y2019,
           #group=site_id
           )) + 
  geom_point(alpha=0.3) + 
  # lims(x=c(range1$min, range1$max),
  #      y=c(range1$min, range1$max)
  #      ) +
  geom_abline(slope = 1, intercept = 0) +
  facet_grid(Pollutant~TVC, scales = "free") + 
  labs(
    x = 1995,
    y = 2019,
    title = "First and last prediction year scores for TVCs and each cohort location"
  )


```

Quantify TVCs over time

```{r}
scores_first_last_yr %>%
  mutate(
    diff = y2019 - y1995
  ) %>%
  ggplot(aes(y = diff,
             x = TVC
             )) + 
  geom_boxplot() + 
  facet_wrap(~Pollutant, scales="free") + 
  labs(title = "Change in PLS score from 1995-2019 at cohort locations",
       y = "Change in PLS Score"
       )
  
```


```{r}

scores_first_last_yr %>%
  # ? don't include NOX for now?
  filter(TVC != "NOX") %>%
  group_by(Pollutant, TVC) %>%
  summarize(
    sd_1995 = sd(y1995),
    sd_2019 = sd(y2019),
    sd_ratio = sd_2019/sd_1995
  )  %>%
  kable(caption = "Change in variability of time-varying covariates between 1995 and 2019 at cohort locations.", 
        col.names = c("Pollutant Model", "TVC", "SD 1995", "SD 2019", "SD Ratio"), 
        digits = 2
        ) %>%
  kable_styling()

  
```

change map 

```{r}
# for mapping, redo code above for grid
scores_first_last_yr <- grid_scores %>%
  select_at(vars(Pollutant, site_id, latitude, longitude,
                 ends_with(c("1995", "2019"))
                 )
            ) %>%
  gather("TVC", "value", ends_with(c("1995", "2019"))) %>%
  mutate(
    Year = str_extract(TVC, "\\d{4}" ),
    Year = as.numeric(Year),
    TVC =  str_extract(TVC, "\\D{0,}" ),
    TVC = toupper(TVC),
    Pollutant = toupper(Pollutant)

  ) %>%
  spread("Year", "value") %>%
  rename(y1995 = "1995",
         y2019 = "2019"
         )
```

 
```{r}
tvcs <- unique(scores_first_last_yr$TVC)  

for(i in seq_along(tvcs)) {
  #i=2

p <- scores_first_last_yr %>%
    filter(Pollutant == "BC",
           TVC == tvcs[i]
           ) %>%
mutate(diff2019 = y2019 - y1995) %>%
  map_scores(yrs = "2019", vars = "diff", 
             legend_title = "PLS\nScore\nDifference", 
             map_title = paste0("Change in ", tvcs[i], " TVC (BC model) from 1995-2019")   ,
             low_col = "darkblue", high_col = "lightblue"
             )

  print(p)
}
  
```


PLS score maps.


Note: PLS scores are composed of both small and large population buffers, thus these are indicative of population nearby. 

Not plotting these maps for smaller buffers alone since these are only available for specific years (e.g., 1990, 2000, 2010 population Census)

Higher PLS scores are indicative of higher population densities (population density is positively associated with TRAP).


```{r}
# Pop
map_scores(dt = grid_scores, yrs = mapping_yrs, vars = "pop") %>%
  ggarrange(plotlist = .,
          nrow = 1,
          common.legend = T, legend = "right") %>%
  annotate_figure(top = "Population density" ) 

```

Higher PLS scores are indicative of higher EC concentrations.

```{r}

# EC
map_scores(dt = grid_scores, 
           yrs = mapping_yrs, 
           vars = "ec") %>%
  ggarrange(plotlist = .,
          nrow = 1,
          common.legend = T, legend = "right") %>%
  annotate_figure(top = "EC" ) 


# same but w/o earliest year, for legend color difference
map_scores(dt = grid_scores, 
           #dont include 1995, where concentrations are very high
           yrs = c(2010, 2019), 
           vars = "ec") %>%
  ggarrange(plotlist = .,
          nrow = 1,
          common.legend = T, legend = "right") %>%
  annotate_figure(top = "EC - later years only" ) 


```


```{r}
# nox
map_scores(dt = grid_scores, 
           yrs = mapping_yrs, 
           vars = "nox") %>%
  ggarrange(plotlist = .,
          nrow = 1,
          common.legend = T, legend = "right") %>%
  annotate_figure(top = "NOx" ) 

# later years only so we can see spatial contrast better
map_scores(dt = grid_scores, 
           yrs = c(2010, 2019), 
           vars = "nox") %>%
  ggarrange(plotlist = .,
          nrow = 1,
          common.legend = T, legend = "right") %>%
  annotate_figure(top = "NOx - later years only" ) 

```


Higher NDVI PLS scores are indicative of lower raw NDVI values (NDVI is negatively associated with TRAP). We see the highest NDVI PLS scores (lowest raw NDVI values) near seattle, and lower PLS scores east of Seattle and I-5.

* Individuals residing at locations with very high EC in earlier years (e.g., 1995) would have had very diff exposures relative to individuals residing away from major roads where changes have beem more subtle. Individuals residing near major roads could thus have higher degrees of exposure misclassification.    
  - AKA - spatial patterns have changed over time

```{r}

# NDVI
map_scores(dt = grid_scores, yrs = mapping_yrs, 
           vars = c("ndvi")) %>%
  ggarrange(plotlist = .,
          nrow = 1,
          common.legend = T, legend = "right"
          ) %>%
  annotate_figure(top = "NDVI" ) 

# # limited years
# map_scores(dt = grid_scores, 
#            #yrs = c(1995, 2010),
#            yrs = c(1995,2019),
#            vars = c("ndvi")) %>%
#   ggarrange(plotlist = .,
#           nrow = 1,
#           common.legend = T, legend = "right"
#           ) %>%
#   annotate_figure(top = "NDVI" ) 


```

PLS score differences maps.

* most areas have seen a similar change in population density over time.   
* Some areas have seen greater changes in population density than others. The Seattle area, for example, has seen larger increases in population density than other surrounding places. These are the places that will benefit from using time-varying covariates rather than just using a trend adjustment.   
```{r}
# pop
grid_scores %>%
  mutate(pop_diff1995_2019 = pop2019 - pop1995) %>%
  map_scores(dt = ., yrs = c("1995_2019"), vars = "pop_diff", 
             legend_title = paste0("PLS Score\nDifference"), 
             low_col = "lightblue", high_col = "black"
             ) %>%
  ggarrange(plotlist = .,
          nrow = 1,
          common.legend = T, legend = "right") %>%
  annotate_figure(top = "Change in population density over time") 

```


* We see a space-time interaction occurring when we look at the change in EC  emissions over time:   
  - Over time, locations near major highways have seen the most drastic decrease in EC concentrations    
  - Locations away from hwys have seen the least change over time, since many had 0 or low EC estimates to begin with 
    + The 0s away from major roads is OK since we will never fit a model with EC alone


```{r}
# EC
grid_scores %>%
  mutate(ec_diff1995_2019 = ec2019 - ec1995) %>%
  map_scores(dt = ., yrs = c("1995_2019"), vars = "ec_diff", 
             legend_title = paste0("PLS Score\nDifference"),
             low_col = "lightblue", high_col = "black"
             ) %>%
  ggarrange(plotlist = .,
          nrow = 1,
          common.legend = T, legend = "right") %>%
  annotate_figure(top = "Change in EC over time") 

```

```{r}
# nox
grid_scores %>%
  mutate(nox_diff1995_2019 = nox2019 - nox1995) %>%
  map_scores(dt = ., yrs = c("1995_2019"), vars = "nox_diff", 
             legend_title = paste0("PLS Score\nDifference"),
             low_col = "lightblue", high_col = "black"
             ) %>%
  ggarrange(plotlist = .,
          nrow = 1,
          common.legend = T, legend = "right") %>%
  annotate_figure(top = "Change in NOx over time") 

```

```{r}
# NDVI
# pop
grid_scores %>%
  mutate(ndvi_diff1995_2019 = ndvi2019 - ndvi1995) %>%
  map_scores(dt = ., yrs = c("1995_2019"), vars = "ndvi_diff", 
             legend_title = paste0("PLS Score\nDifference"), 
             low_col = "lightblue", high_col = "black"
             ) %>%
  ggarrange(plotlist = ., nrow = 1,
            common.legend = T, legend = "right") %>%
  annotate_figure(top = "Change in NDVI over time") 

```

# Linear regression to explore the variability explained by each PLS component 

### --> in lm, should ec2019 be positive & ndvi2006 be negative?

* all covariate predictors are centered and scaled such that each coefficient estimate is for 1 SD difference.

* space-varying covariates are msot predi 

* in univariate BC models, space-varying covariates explain most of the variability (R2) in BC, with each time-varying covariate (pop, ec, ndvi) explaining a smaller proportion

* multilinear models that build on the space-varying covariates add very little to the model in terms of how much additional variability in BC they are able to explain   
  - but EC and NDVI are still significant predictors
  - Pop is not a significant predictor

* new TV geocovariates may not add much value for 2019, but their spatial structure does change over time & they are significant, such that they will influence historical predictions
  

* results are similar for UFP

**BC**

```{r}

lm_df <- mm %>%
  select(site_id, bc) %>%
  left_join(bc_scores_mm0) %>%
  mutate_at(str_subset(names(.), "space|pop|ec|ndvi"), ~ scale(.))

r2_df <- data.frame(
  Predictors = c("space",
            "pop",
            "ec",
            "ndvi",
            "space + pop", 
            "space + pop + ec", 
            "space + pop + ec + ndvi"),
  R2 = NA
)

# linear regression
(r2_space <- lm(data = lm_df, bc ~ space1 + space2) %>% 
  summary() 
  )

(r2_pop <- lm(data = lm_df, bc ~ pop2010) %>% 
  summary()
)

(r2_ndvi <- lm(data = lm_df, bc ~ ndvi2019) %>% 
  summary()
  )

(r2_ec <- lm(data = lm_df, bc ~ ec2019) %>% 
  summary() 
  )


(r2_m2 <- update(object = r2_space, ~ . + pop2010) %>% 
  summary()  
)


(r2_m3 <- update(object = r2_m2, ~ . + ec2019) %>% 
  summary()  
) 

(r2_m4 <- update(object = r2_m3, ~ . + ndvi2019) %>% 
  summary() 
)


r2_df$R2[r2_df$Predictors == "space"] <- r2_space$r.squared
r2_df$R2[r2_df$Predictors == "pop"] <- r2_pop$r.squared
r2_df$R2[r2_df$Predictors == "ec"] <- r2_ec$r.squared
r2_df$R2[r2_df$Predictors == "ndvi"] <- r2_ndvi$r.squared

r2_df$R2[r2_df$Predictors == "space + pop"] <- r2_m2$r.squared
r2_df$R2[r2_df$Predictors == "space + pop + ec"] <- r2_m3$r.squared
r2_df$R2[r2_df$Predictors == "space + pop + ec + ndvi"] <- r2_m4$r.squared

r2_df %>%
  kable(caption = "Variability explained by BC models with an increasing number of PLS component predictors",
        digits = 2
        ) %>%
  kable_styling()

 
```

**UFP**

```{r}
lm_df <- mm %>%
  select(site_id, ufp) %>%
  left_join(ufp_scores_mm0) %>%
  mutate_at(str_subset(names(.), "space|pop|ec|ndvi"), ~ scale(.))

r2_df <- data.frame(
  Predictors = c("space",
            "pop",
            "ec",
            "ndvi",
            "space + pop", 
            "space + pop + ec", 
            "space + pop + ec + ndvi"),
  R2 = NA
)

# linear regression
(r2_space <- lm(data = lm_df, ufp ~ space1 + space2) %>% 
  summary() 
  )

(r2_pop <- lm(data = lm_df, ufp ~ pop2010) %>% 
  summary()
)

(r2_ndvi <- lm(data = lm_df, ufp ~ ndvi2019) %>% 
  summary()
  )

(r2_ec <- lm(data = lm_df, ufp ~ ec2019) %>% 
  summary() 
  )


(r2_m2 <- update(object = r2_space, ~ . + pop2010) %>% 
  summary()  
)


(r2_m3 <- update(object = r2_m2, ~ . + ec2019) %>% 
  summary()  
) 

(r2_m4 <- update(object = r2_m3, ~ . + ndvi2019) %>% 
  summary() 
)


r2_df$R2[r2_df$Predictors == "space"] <- r2_space$r.squared
r2_df$R2[r2_df$Predictors == "pop"] <- r2_pop$r.squared
r2_df$R2[r2_df$Predictors == "ec"] <- r2_ec$r.squared
r2_df$R2[r2_df$Predictors == "ndvi"] <- r2_ndvi$r.squared

r2_df$R2[r2_df$Predictors == "space + pop"] <- r2_m2$r.squared
r2_df$R2[r2_df$Predictors == "space + pop + ec"] <- r2_m3$r.squared
r2_df$R2[r2_df$Predictors == "space + pop + ec + ndvi"] <- r2_m4$r.squared

r2_df %>%
  kable(caption = "Variability explained by UFP models with an increasing number of PLS component predictors",
        digits = 2
        ) %>%
  kable_styling()

```


# UK Models

**Correlation between UFP and BC**

Since we will be using BC emission for UFP model. 

```{r}
high_quant <- 0.97
```

```{r}
model1 <- "ufp~bc"
ufp_bc <- lm(as.formula(model1), data = mm)
round_digits <- 2
lm_fit <- paste0("y = ", round(ufp_bc$coefficients[1], round_digits), " + ", round(ufp_bc$coefficients[2], round_digits), "X" )
lm_r2 <-  round(summary(ufp_bc)$r.squared, round_digits)
rmse <- rmse_fn(obs = exp(mm$ufp), pred = exp(predict(ufp_bc))) %>% round()

fit.info <- paste0(lm_fit,
                   "\nR2 = ", lm_r2,  
                   "\nRMSE = ", rmse,
                   "\nNo. Pairs = ", nrow(mm))


```

```{r, results="asis"}
# show equation in Rmd render

## --> error installing/using package
#install.packages("texPreview", dependencies = T)
#library(texPreview) #%>% texPreview::preview()

equatiomatic::extract_eq(ufp_bc) #%>% texPreview::preview()

#equatiomatic::extract_eq(ufp_bc, use_coefs=T)

 
```

Basic plot

* UFP and BC appear to be moderately positively correlated. This suggests taht BC may serve as a decent surrogate for UFP at many locations, though less well at locations furthest away from the best fit line.

* Edmund: the bulk of the correlation near the 1-1 line is likely due to general/heavy traffic

```{r}
mm %>%
  ggplot(aes(x=bc, y=ufp)) + 
  geom_point(alpha=0.5) + 
  geom_smooth(method = "lm", se=F, aes(linetype = "lm")) + 
  labs(
    x= "Ln BC (Ln ng/m3)",
    y = "Ln UFP (Ln pt/cm3)",
    title = "Comparison of annual average BC and UFP in mobile monitoring campaign",
    caption = paste0("R2 and RMSE are based on OLS for ", model1, " model. RMSE is calculated on the native scale."),
    linetype = ""
  ) +
  #annotate("text", -Inf, Inf, label = fit.info, hjust = 0, vjust = 1) + 
  annotate("text", -Inf, Inf, label = fit.info, hjust = 0, vjust = 1) + 
  
  scale_color_gradient2()
```

Plot highlighting residuals.

```{r}
mm %>%
  mutate(
    lm_residual = residuals(ufp_bc),
    high_residual = lm_residual <= quantile(lm_residual, 1-high_quant) |
           lm_residual >= quantile(lm_residual, high_quant)
  ) %>%
  ggplot(aes(x=bc, y=ufp)) + 
  geom_point(alpha=0.7, aes(col=lm_residual, shape=high_residual)) + 
  #geom_smooth(se=F) + 
  geom_smooth(method = "lm", se=F, aes(linetype = "lm")) + 
  labs(
    x= "Ln BC (Ln ng/m3)",
    y = "Ln UFP (Ln pt/cm3)",
    title = "Comparison of annual average BC and UFP in mobile monitoring campaign",
    subtitle = paste0("high residual: value <=", 1-high_quant, " or > ", high_quant, " quantile"),
    caption = paste0("R2 and RMSE are based on OLS for ", model1, "model. RMSE is calculated on the native scale."),
    shape = "high residual",
    col = "lm residual",
    linetype = ""
  ) +
  annotate("text", -Inf, Inf, label = fit.info, hjust = 0, vjust = 1) + 
  scale_color_gradient2()
  

```

Map of where high residuals are found. 

* When we map the high residuals, we see that many of these are near airports.   
  - this is similar to what the MOV-UP study found. These UFPs are likely in the smaller size range & indicative of airway emissions

* When we map the low residuals (higher BC levels than expected), most are near rialroad tracks 
  - Tim Gould: some are near rail yards

```{r}
#### --> ? add railroad (only include lines in study area before uploading to R?)

```

```{r}
ufp_bc_lm_high_residuals <- mm %>%
  mutate(
    lm_residual = residuals(ufp_bc)
  ) %>%
  filter(lm_residual <= quantile(lm_residual, 1-high_quant) |
           lm_residual >= quantile(lm_residual, high_quant)
           )

```

```{r, fig.height=8}
map0 +
  geom_polygon(data = study_area,
                aes(x = long, y = lat, group = group, fill = "Study Area"),
                alpha = 0.1) +
  geom_polygon(data = monitoring_area,
                aes(x = long, y = lat, group = group, fill = "Monitoring Area"),
                alpha = 0.1) +
  #airport area
  geom_polygon(data = airports_area, #aviation_area, #airports_area
                aes(x = long, y = lat, group = group, 
                    fill = "Airport"
                    #fill = "Aviation zones"
                    ),
                #alpha = 0.3
               ) +
      geom_point(data = ufp_bc_lm_high_residuals,
                 aes(x = longitude, y = latitude, col = lm_residual),
                 #alpha=0.3
                 ) + 
  scale_color_gradient2(name = "lm residual") +
  labs(
    title = "Cohort sites with large UFP~BC model residuals",
    fill = "",
    col = ""
    ) 

```


```{r}
# # datasets for modeling
# bc_mm_scores  #mm_scores  
# bc_cohort_scores 
# bc_grid_scores 
# bc_aqs_scores
# 
# # UFP
# ufp_mm_scores 
# ufp_cohort_scores 
# ufp_grid_scores 
# ufp_aqs_scores
```



## CV RMSE and R2 for the 2019 surface

```{r}

uk_predictors. <- c(paste0("space", c(1:n_components)), "pop", "ndvi", "ec"
                                                  )

uk_predictors_nox <- gsub("ec", "nox", uk_predictors.)

# FN returns CV predictions from fitting a UK model. This fn is similar to pls_uk_cv_predictions() in A2.0.1_Var&Fns.R, but it does not fit PLS since we have already determined these parameters in Aim 2. It can be used to predict within the same dataset (10 FCV - generates training/test sets) or to predict at new locations (uses MM stops IDs to find the training dataset).

# #performing CV 
# #dt = mm_scores %>% filter(grepl("BC", Pollutant))
# #perform_cv = TRUE
# 
# # predict at other locations
# dt = mm_cohort_scores %>% filter(grepl("BC", Pollutant))
# perform_cv = FALSE
# 
# y_name <- "value"
# uk_predictors = c(paste0("space", c(1:3)), "pop2019", "ndvi2006")
# dist_fract. = 0.1
 
uk_predictions <- function(dt,
                              y_name = "value",
                              #whether to perform CV within the same dataset. If false, identifies MM stops based on id and makes those the training set 
                              perform_cv = TRUE,
                              uk_predictors = uk_predictors.
                                                            ) {  
  
  dt <- dt %>% rename(y_name = y_name)  
  
  # variogram maximum distance fraction to model, from Aim 2
  dist_fract. <- ifelse(grepl("bc", unique(dt$Pollutant)), 
                        a2_cv_results$Variogram_Distance_Fraction[str_detect(a2_cv_results$Pollutant, "BC")],
                        a2_cv_results$Variogram_Distance_Fraction[str_detect(a2_cv_results$Pollutant, "UFP")]
                        
                        )
  
  
  if(perform_cv == TRUE) {
    # 10 FCV
    k <- 10
    
    dt <- dt %>%
    #create training/test set within a dataset
    mutate(set = sample(c(1:k), size = nrow(.), replace = T),
           # to save predictions
           uk_prediction = NA)
  }
  
  if(perform_cv == FALSE) {
    # only need to build a model w/ training data once
    k <-1
  }
  
  for(f in seq_len(k)) {
    #f=1
    
    ################################ create training/test sets ################################
    if(perform_cv == TRUE) { 
      #create test/training sets if conducting CV
      train_grp <- dt$set != f
    }
    
    if(perform_cv == FALSE) { 
      #ID non-aqs site MM stops as the training data
      train_grp <- grepl("MS", dt$site_id) #"MS|MC"
    }
    
    dt_train <- dt %>% filter(train_grp)  
    dt_test <- dt %>% filter(!train_grp) %>%
      #drop here, otherwise rows will be dropped when using as.geodata() if there are NAs
      select(-y_name)
    
    ################################ geodatasets ################################
    geo_train <- as.geodata(dt_train, 
                            coords.col = c("lambert_x", "lambert_y"), 
                            data.col = "y_name", 
                            covar.col = uk_predictors)
    geo_test <- as.geodata(dt_test, 
                           coords.col = c("lambert_x", "lambert_y"), 
                           covar.col = uk_predictors)
    
    ##trend
    cov_trend <-  as.formula(paste0("~ ", paste0(uk_predictors,  collapse = " + " )))
    
    max.dist <- summary(geo_train)$distances.summary[["max"]]
    
    # --> ? select this variogram parameter through CV??
    max.plot.dist <- max.dist*dist_fract. #[dist_fract_index] 
    
    ############################ model residuals ###################################### 
    ##Empirical Variogram
    brk_pt <- 1000
    by1_pt <- 300
    by2_pt <- 1000
    
    variog_train <- variog(geo_train,
                           #plotting breakpoints 
                           uvec=c(seq(0, brk_pt, by = by1_pt), seq((brk_pt + by2_pt), max.plot.dist, by= by2_pt)),
                           #UK
                           trend = cov_trend, 
                           messages = F)
    
    #use geoR to try to estimate intitial range & sill values. using WLS and an exponential fit
    wls_ests_train <- variofit(variog_train, cov.model = "exp", 
                               messages = F)
    
    # --> ? select this variogram parameter through CV??
    #don't need initial values above since estimates seem to be the same w/ or w/o ini = wls_ests_train (based on small sample)?
    resid_model_train <- variofit(vario = variog_train, 
                                  ini = wls_ests_train, 
                                  cov.model = "exp",
                                  weights = "npairs",#wls
                                  ) 
    
    
    # if fitting a model to the entire MM dataset (i.e., not performing CV), save residual model parameters
    if(perform_cv == FALSE) {
      
      # partial sill: sigma sq # range: phi, # nugget: tau sq
      resid_model.s <- summary(resid_model_train)
      
      ## residual model parameters
      residual_model_table <- data.frame(
        Partial_Sill = resid_model.s$estimated.pars[["sigmasq"]],
        Range_m = resid_model.s$estimated.pars[["phi"]],
        Nugget = resid_model.s$estimated.pars[["tausq"]])
      } 
    
    #trend
    train_trend <- trend.spatial(trend = cov_trend, geo_train)
    test_trend <- trend.spatial(trend = cov_trend, geo_test)
    
    ############################# Use UK to predict #############################
    kc_cv <- krige.conv(geo_train,
                        # where you want to predict
                        locations = geo_test$coords,
                        krige = krige.control(type = "ok",
                                                          # range, nugget, partial sill
                                              obj.model = resid_model_train, 
                                              trend.d = train_trend,
                                              trend.l = test_trend))
    
    #save CV predictions
    dt$uk_prediction[!train_grp] <- kc_cv$predict
    
  }

  
  ############################ return results ############################
  if(perform_cv == TRUE) {
    result <- list(data=dt)
    }
  
  if(perform_cv == FALSE) {
    result <- list(data=dt,
                   # include additional information on residual model
                   residual_model = resid_model_train,
                   residual_model_table = residual_model_table,
                   variogram = variog_train
                   )
  }
  
  return(result)
  
  }

```

```{r, results="hide"}
# UK model will be built using MM stops & most recent TVCs
mm_scores_2019 <- mm_scores %>%
  select(Pollutant:lambert_y,
         contains("space"),
         #select TVCs for modeling
         pop = pop2019,
         ndvi = ndvi2019,
         ec = ec2019,
         
         #sensitivity
         nox = nox2019
         )

cv_results <- data.frame(
  Pollutant = pollutants,
  RMSE = NA,
  R2 = NA
)

# not sure why have to repeate this here. otherwise, doesn't recognize rmse()
#source("0.Global_Fns.R")


for (i in seq_along(pollutants)) {
  #i=1
  mm_cv <- mm_scores_2019 %>% 
    filter(grepl(pollutants[i], Pollutant)) %>%
    uk_predictions(dt = ., 
                   y_name = "value",  
                   uk_predictors = uk_predictors.,
                   perform_cv = TRUE)
  
  cv_results$RMSE[cv_results$Pollutant == pollutants[i]] <- rmse_fn(obs = exp(mm_cv$data$y_name), pred = exp(mm_cv$data$uk_prediction)) %>% round()
  cv_results$R2[cv_results$Pollutant == pollutants[i]] <- r2_mse_based(obs = exp(mm_cv$data$y_name), pred = exp(mm_cv$data$uk_prediction)) %>% round(2)
  
  }

```


```{r}
a2_cv_results1 <- a2_cv_results %>%
  mutate(
    Pollutant = ifelse(grepl("bc", Pollutant, ignore.case = T), "bc", "ufp")
  ) %>%
  select(Pollutant, A2_RMSE = RMSE, A2_R2 = R2)

cv_results %>%
  left_join(a2_cv_results1) %>%
  label_pollutant(var = "Pollutant", label = "pollutant") %>%
  kable(caption = "Cross-validated RMSE and MSE-based R2 estimates based on the 1-1 line. Using 2019 surfaces. Estimates are compared to those obtained in the UK model from Aim 2") %>%
  kable_styling()
```

## UK model fitting & prediction at cohort & grid locations  

```{r}
# FN returns UK predictions for new locations (e.g., cohort, grid) and times after fitting a UK model to MM observations in 2019 

# dt_for_prediction = aqs_scores
# dt_mm2019 = mm_scores_2019
# #only need to predict when AQS sites start measuring BC (2003+)
# prediction_yrs = sort(unique(bc_validation$Year))
# pollutants. = pollutants
# uk_predictors.. = uk_predictors.

predict_over_time <- function(
  dt_for_prediction,  
  dt_mm2019 = mm_scores_2019,
  prediction_yrs = c(seq(first_p_yr, last_acs_yr, by=5), 2019),  
  pollutants. = pollutants,
  uk_predictors.. = uk_predictors.
  ) {
  
  dt <- dt_for_prediction %>%
    select(Pollutant, site_id, latitude, longitude) %>%
    #for merging later since AQS sites are strings
    mutate(site_id = as.character(site_id))
    
    
  for(y in seq_along(prediction_yrs)) {
  #y=1   
  
  dt0 <- dt_for_prediction %>%
    #select & rename TVCs for prediction
    select(Pollutant:lambert_y,
           contains("space"),
           #rename TVCs
           pop = paste0("pop", prediction_yrs[y]),
           ec = paste0("ec", prediction_yrs[y]),
           
           #sensitivity
           nox = paste0("nox", prediction_yrs[y]),
           
           ndvi = paste0("ndvi", prediction_yrs[y])
           
           ) %>%
    #attach modeling data 
    rbind(dt_mm2019)
    
  
  predictions_df <- data.frame()
  
  for(i in seq_along(pollutants.)) {
    #i=1
    
    # list w/ UK results
    dt_list <- dt0 %>% 
      filter(grepl(pollutants.[i], Pollutant)) %>%
      uk_predictions(dt = ., y_name = "value", perform_cv = FALSE, 
                     uk_predictors = uk_predictors..)
     
    # pull the df out of the list
    predictions_df0 <- dt_list$data %>% 
      select(Pollutant, site_id, uk_prediction) %>%
      #drop MM stops, which don't have predictions & to match dt_for_prediction rows
      filter(!str_detect(site_id, "MS")) %>%
      mutate(#site_id = as.numeric(site_id),
             #convert prediction back to the native scale
             uk_prediction = exp(uk_prediction)
             )
    
    predictions_df <- rbind(predictions_df, predictions_df0)
    
    # include the residual table & variogram from the last models fit to each pollutant. They should all be very similar since all models are fit to the same MM data alone, but they predict at different locations/times
    if(pollutants.[i] == "bc" & y==length(prediction_yrs)) {
      
      bc_residual_model <- dt_list$residual_model
      bc_residual_model_table <- dt_list$residual_model_table %>%
        mutate(Pollutant = "BC")
      
      bc_variogram  <- dt_list$variogram 
      
    }
    
    if(pollutants.[i] == "ufp" & y==length(prediction_yrs)) {
      
      ufp_residual_model <- dt_list$residual_model
      ufp_residual_model_table <- dt_list$residual_model_table %>%
        mutate(Pollutant = "UFP")
      
      ufp_variogram <- dt_list$variogram
      
    }
     
  }
  
  names(predictions_df)[names(predictions_df) == "uk_prediction"] <- paste0("uk_", prediction_yrs[y])
  
  #save predictions 
  dt <- dt %>% left_join(predictions_df)
  
  }

  # combine residual model parameters for pollutants
  residual_model_table <- rbind(bc_residual_model_table,
                                   ufp_residual_model_table) %>%
    select(Pollutant, everything())
  
  result <- list(
    #predictions for all location-years
    dt = dt,
    
    #UK residual model parameters
    bc_residual_model = bc_residual_model,
    ufp_residual_model = ufp_residual_model,
    residual_model_table = residual_model_table,
    #variograms for each pollutant
    bc_variogram = bc_variogram,
    ufp_variogram = ufp_variogram
    )
  
  #return(dt)
  return(result)

}

```


```{r, results="hide"}
# use UK to make perdictions

### --> DONE. # uncomment cohort_predictions... once get new covariates 

cohort_predictions <- predict_over_time(dt_for_prediction = cohort_scores,
                                        dt_mm2019 = mm_scores_2019,
                                        prediction_yrs = seq(first_p_yr, last_p_yr),  
                                        )
cohort_predictions_nox <- predict_over_time(dt_for_prediction = cohort_scores,
                                        dt_mm2019 = mm_scores_2019,
                                      prediction_yrs = seq(first_p_yr, last_p_yr),
                                      uk_predictors.. =  uk_predictors_nox
                                        )

grid_predictions <- predict_over_time(dt_for_prediction = grid_scores,
                                        dt_mm2019 = mm_scores_2019,
                                      prediction_yrs = seq(first_p_yr, last_p_yr) #mapping_yrs 
                                        )

## sensitivity - nox
grid_predictions_nox <- predict_over_time(dt_for_prediction = grid_scores,
                                        dt_mm2019 = mm_scores_2019,
                                      prediction_yrs = seq(first_p_yr, last_p_yr), 
                                      uk_predictors.. =  uk_predictors_nox
                                        )

#for validation
aqs_predictions <- predict_over_time(dt_for_prediction = aqs_scores, 
                    dt_mm2019 = mm_scores_2019,
                    #only need to predict when AQS sites start measuring UFP, BC (2001, 2003+)
                    prediction_yrs = sort(unique(c(ufp_validation$Year, bc_validation$Year)))
                    )

## nox
aqs_predictions_nox <- predict_over_time(dt_for_prediction = aqs_scores, 
                    dt_mm2019 = mm_scores_2019, 
                    uk_predictors.. = uk_predictors_nox,
                    #only need to predict when AQS sites start measuring UFP, BC (2001, 2003+)
                    prediction_yrs = sort(unique(c(ufp_validation$Year, bc_validation$Year)))
                    )
```

Variogram plots

```{r}
# --> DONE. # changed several of the following code chunks from eval=F to eval=T once I received cohort predictions

```

```{r, eval=T}
# BC
plot(cohort_predictions$bc_variogram,
     main = paste0("Binned empirical and modeled variogram for BC model"),
     xlab = "Distance (m)")

lines(cohort_predictions$bc_variogram, lty=1)
lines(cohort_predictions$bc_residual_model, lty=2, col=2)
legend("bottomright",
       legend = c("Empirical", paste0("Residual Model")),
       lty=c(1:2), col = c(1:2))

```


```{r, eval=T}
# UFP
plot(cohort_predictions$ufp_variogram,
     main = paste0("Binned empirical and modeled variogram for UFP model"),
     xlab = "Distance (m)")

lines(cohort_predictions$ufp_variogram, lty=1)
lines(cohort_predictions$ufp_residual_model, lty=2, col=2)
legend("bottomright",
       legend = c("Empirical", paste0("Residual Model")),
       lty=c(1:2), col = c(1:2))

```

Residual model parameters 

```{r, eval=T}
cohort_predictions$residual_model_table  %>%
  kable(digits = 3, 
        col.names = c("Pollutant", "Partial Sill", "Range (m)", "Nugget"),
        caption = "UK residual model parameters for each pollutant"
        ) %>%
  kable_styling()

```


# Temporal trend adjustment

Conducting a multiplicative (rather than an additive) trend adjustment since EC, BC and UFP scales and/or units are different. 

```{r}
#trend_adjustment
#trend_yrs <- unique(trend_adjustment$Year)

#cohort
cohort_uk_trend_predictions <- cohort_predictions$dt %>%
  #rename_at(vars(contains("uk_")), ~str_replace(., "uk_", "uk_trend_")) %>%
  gather(Year, uk_p, contains("uk")) %>%
  mutate(Year = as.numeric(str_extract(Year, "\\d{4}"))) %>%
  #merge w/ trend adjustment
  left_join(trend_adjustment) %>%
  mutate(
    #adjust UK predictions using trend
    uk_trend_p = uk_p*ratio_adjustment
  )

cohort_uk_trend_predictions_nox <- cohort_predictions_nox$dt %>%
  #rename_at(vars(contains("uk_")), ~str_replace(., "uk_", "uk_trend_")) %>%
  gather(Year, uk_p, contains("uk")) %>%
  mutate(Year = as.numeric(str_extract(Year, "\\d{4}"))) %>%
  #merge w/ trend adjustment
  left_join(trend_adjustment) %>%
  mutate(
    #adjust UK predictions using trend
    uk_trend_p = uk_p*ratio_adjustment
  ) %>%
  #for differentiting later
  rename(uk_trend_p_nox = uk_trend_p)  %>%
  #don't need this
  select(-uk_p)



#grid
grid_uk_trend_predictions <- grid_predictions$dt %>%
  gather(Year, uk_p, contains("uk")) %>%
  mutate(Year = as.numeric(str_extract(Year, "\\d{4}"))) %>%
  #merge w/ trend adjustment
  left_join(trend_adjustment) %>%
  mutate(
    #adjust UK predictions using trend
    uk_trend_p = uk_p*ratio_adjustment
  )

## nox
grid_uk_trend_predictions_nox <- grid_predictions_nox$dt %>%
  gather(Year, uk_p, contains("uk")) %>%
  mutate(Year = as.numeric(str_extract(Year, "\\d{4}"))) %>%
  #merge w/ trend adjustment
  left_join(trend_adjustment) %>%
  mutate(
    #adjust UK predictions using trend
    uk_trend_p = uk_p*ratio_adjustment
  ) %>%
  #for differentiting later
  rename(uk_trend_p_nox = uk_trend_p) %>%
  #don't need this
  select(-uk_p)

#aqs sites
aqs_uk_trend_predictions <- aqs_predictions$dt %>%
  gather(Year, uk_p, contains("uk")) %>%
  mutate(Year = as.numeric(str_extract(Year, "\\d{4}"))) %>%
  #merge w/ trend adjustment
  left_join(trend_adjustment) %>%
  mutate(
    #adjust UK predictions using trend
    uk_trend_p = uk_p*ratio_adjustment
  )

aqs_uk_trend_predictions_nox <- aqs_predictions_nox$dt %>%
  gather(Year, uk_p, contains("uk")) %>%
  mutate(Year = as.numeric(str_extract(Year, "\\d{4}"))) %>%
  #merge w/ trend adjustment
  left_join(trend_adjustment) %>%
  mutate(
    #adjust UK predictions using trend
    uk_trend_p = uk_p*ratio_adjustment
  ) %>%
  #for differentiting later
  rename(uk_trend_p_nox = uk_trend_p) %>%
  select(-uk_p)



```

Plots comparing predictions before and after the trend adjustment.

-density plot

TRAP predictions are very similar across years unless a trend adjustment is applied. 

The trend adjustment makes TRAP predictions more variable back in time.

```{r}
cohort_uk_trend_predictions %>% #grid_uk_trend_predictions %>%
  gather("trend", "prediction", contains("uk")) %>%
  mutate(Trend = ifelse(str_detect(trend, "trend"), "Trend", "No Trend")) %>%
    label_pollutant(var = "Pollutant", label = "pollutant") %>%
  ggplot(aes(x=prediction, fill=Year, group=Year)) + 
  geom_density(alpha=0.1) + 
  facet_grid(Trend~Pollutant, scales="free") +
  labs(
    title = "UFP and BC predictions before and after applying a trend adjustment",
    subtitle = "Cohort"
  )
```

just the trend predictions (primary analysis)

```{r}
cohort_uk_trend_predictions %>% #grid_uk_trend_predictions %>%
  label_pollutant(var = "Pollutant", label = "pollutant") %>%
  #present BC first
  mutate(Pollutant =relevel(factor(Pollutant), ref = "BC (ng/m3)"))%>%
  ggplot(aes(y=uk_trend_p, 
             #fill=Year, 
             x=Year,
             group=Year
             )) + 
  geom_boxplot() +
  #geom_density(alpha=0.5) + 
  facet_wrap(~Pollutant, scales="free") +
  labs(
    title = "UFP and BC predictions after applying a trend adjustment",
    y = "Predicted Exposure",
    subtitle = "Cohort"
  )

```

-table

```{r}
#stratified by yr
predictions_t1 <- cohort_uk_trend_predictions %>% #grid_uk_trend_predictions %>%
  drop_na(uk_trend_p) %>%
  # gather("Trend", "prediction", contains("uk")) %>%
  # mutate(Trend = ifelse(str_detect(Trend, "trend"), "Yes", "No")) %>%
  # mutate(Year = as.factor(Year)) %>%
  
  #only keep some years to summarize
  filter(Year %in% c(seq(first_p_yr, last_p_yr, 5), 2019)) %>%
  # don't want commas between 1,000's
  mutate(Year = as.factor(Year)) %>%
  
  label_pollutant(var = "Pollutant", label = "pollutant") %>%
  group_by(#Trend, 
           Pollutant, Year) %>%
  distribution.table(var.string = "uk_trend_p")
  
# add overall
predictions_t2 <- cohort_uk_trend_predictions %>% #grid_uk_trend_predictions %>%
  drop_na(uk_trend_p) %>%
  label_pollutant(var = "Pollutant", label = "pollutant") %>%
  group_by(Pollutant, Year = "Overall") %>%
  distribution.table(var.string = "uk_trend_p")  %>%
  
  rbind(predictions_t1) %>%
  mutate(Year = relevel(factor(Year), ref = "Overall"  )) %>%
  
  ungroup() %>%
  arrange(Pollutant, Year) 

#one pollutant at a time

## BC
predictions_t2 %>%
  filter(grepl("BC", Pollutant)) %>%
  select(-Pollutant) %>%
  kable(caption = "BC predictions over time at cohort locations, adjusted for a trend") %>%
  kable_styling()

## UFP
predictions_t2 %>%
  filter(grepl("UFP", Pollutant)) %>%
  select(-Pollutant) %>%
  kable(caption = "UFP predictions over time at cohort locations, adjusted for a trend") %>%
  kable_styling()


```

**NOx vs EC model predictions**

Predictions from the primary and NOx predictor model for the cohort 

```{r}
#bc
cohort_uk_trend_predictions %>% #grid_uk_trend_predictions %>%
  left_join(cohort_uk_trend_predictions_nox) %>% #grid_uk_trend_predictions %>%
  filter(Pollutant == "bc") %>%
   colo.plot(x.variable = "uk_trend_p", x.label = "EC Predictors (Primary Model)", 
             y.variable = "uk_trend_p_nox", y.label = "NOx Predictors", 
             mytitle = "BC predictions from the \nprimary and NOx predictor model", mysubtitle = "BC Pollutant Model",
             int_digits = 1, slope_digits = 2,
             r2.digits = 2, #rmse.digits = 1
               )

#ufp
cohort_uk_trend_predictions %>% #grid_uk_trend_predictions %>%
  left_join(cohort_uk_trend_predictions_nox) %>%
  filter(Pollutant == "ufp") %>%
   colo.plot(x.variable = "uk_trend_p", x.label = "EC Predictors (Primary)", 
             y.variable = "uk_trend_p_nox", y.label = "NOx Predictors", 
             mytitle = "UFP predictions from the \nprimary and NOx predictor model", mysubtitle = "UFP Pollutant Model",
             int_digits = 1, slope_digits = 2, 
             r2.digits = 2, #rmse.digits = 1
               )

```



scatterplot.

for any given year, the sites most impacted by the multiplicataive trend adjustment are those with higher concentrations (these are further away from the 1-1 line).

```{r}
cohort_uk_trend_predictions %>%
  label_pollutant(var = "Pollutant", label = "pollutant") %>%
  ggplot(aes(x=uk_p, y=uk_trend_p, col=Year)) + 
  geom_point(alpha=0.1) + 
  geom_abline(aes(slope = 1, intercept = 0, linetype = "1-1")) +
  facet_wrap(~Pollutant, scales="free") + 
  labs(
    title = "UFP and BC predictions before and after applying a trend adjustment",
    subtitle = "Cohort",
    x = "No Trend",
    y = "Trend",
    linetype = ""
    
  )

```



# Validation - out of sample  


* We will include BH in this validation since BC is a different measurement than EC, which was used to set the trend. The model performance at this location, however, could be more optimistic since measurements are from the same site.   
* We will not include Duwamish Valley since this site is fundamentally different than other sites. Compared to other locations it tends to have:   
  - more heavy duty diesel trucks, including a lot of idling traffic
  - heavy boat emissions
  - heavy train traffic
  - a temporal trend that is different from other sites. Thus our models will "appear" to perform worse.

```{r}
#combine observation & prediction datasets for AQS sites
bc_valid_p <- bc_validation %>%
  #make site_id type match other dfs
  mutate(site_id = as.character(site_id)) %>%
  #only keep predictions for site-years w/ AQS estimates
  left_join(aqs_uk_trend_predictions) %>%
  left_join(aqs_uk_trend_predictions_nox) %>%
  
  #drop sites w/o geocovariates...or UK predictions
  drop_na() %>%
  filter(!grepl("Duwamish", Site))

ufp_valid_p <- ufp_validation %>%
  #make site_id type match other dfs
  mutate(site_id = as.character(site_id)) %>%
  left_join(aqs_uk_trend_predictions) %>%
  left_join(aqs_uk_trend_predictions_nox)

```

## BC 

Site-years available for BC validation

```{r}
bc_valid_p %>%
  ggplot(aes(x=Year, y=Site, fill=Site)) + 
  stat_bin2d() + 
  theme(legend.position = "none") +
  labs(
    title = "Site-years available to validate BC model"
  )

```

Scatter plots 

-by site and year

* The BC model produces underpredicts observed BC concentrations   
* $R^2=0$ since it is based on the 1-1 line, and none of the observation-prediction pairs are on the 1-1 line
  - Duwamish performs the worst since it is the furthest from the 1-1 line

```{r}
#calculate plot range
plot_range <- bc_valid_p %>%
  dplyr::summarize(
    min_val = min(c(mean_ng_m3, uk_trend_p)),
    max_val = max(c(mean_ng_m3, uk_trend_p))
    )

```

```{r}

bc_valid_p %>%
  #filter(str_detect(Site, "Duwamish", negate = T)) %>%
  ggplot(aes(x = mean_ng_m3, y = uk_trend_p, 
             col=Year,
             shape=Site,
             )) + 
  geom_point() + 
  geom_smooth(se=F, span=1,
              #if want to draw lowess lines for years instead of sites
              #aes(group=Year)
              ) +
  geom_abline(aes(slope = 1, intercept = 0 #linetype="1-1"
                  ),
              linetype="dashed",
              alpha=0.5
              ) + 
  
  scale_shape_manual(values = 1:length(unique((bc_valid_p$site_id)))) + 
  #make square plot
  lims(x=c(plot_range$min_val, plot_range$max_val),
       y=c(plot_range$min_val, plot_range$max_val)) +
  labs(x= "Observed", 
       y= "Predicted",
       title = "Scatterplot of observed vs predicted BC concentration (ng/m3)",
       caption = "Dashed diagonal line is the 1-1 line"
       )

  
```

* BC model model wtih NOx predictor looks almost identical to EC model predictions

```{r}
#nox
bc_valid_p %>%
  ggplot(aes(x = mean_ng_m3, y = uk_trend_p_nox, 
             col=Year,
             shape=Site,
             )) + 
  geom_point() + 
  geom_smooth(se=F, span=1) +
  geom_abline(aes(slope = 1, intercept = 0, linetype="1-1")) + 
  scale_shape_manual(values = 1:length(unique((bc_valid_p$site_id)))) + 
  #make square plot
  lims(x=c(plot_range$min_val, plot_range$max_val),
       y=c(plot_range$min_val, plot_range$max_val)) +
  labs(x= "Observed", 
       y= "Predicted",
       title = "Scatterplot of observed vs predicted BC concentration (ng/m3)",
       subtitle = "NOx predictor"
       )
```


-overall 

* The BC model generally has poor precision (R2) and tends to be bias (high RMSE)

```{r}

bc_valid_overall <- bc_valid_p %>%
   #filter(str_detect(Site, "Duwamish", negate = T)) %>%
  dplyr::summarize(
    N = n(), #all site-years
    Years = paste(unique(Year), collapse = ", ")      ,
    Sites = paste(unique(Site), collapse = ", ")      ,
    RMSE = round(rmse_fn(obs = mean_ng_m3, pred = uk_trend_p)),
    R2 = r2_mse_based(obs = mean_ng_m3, pred = uk_trend_p)
  ) 

#nox
bc_valid_overall_nox <- bc_valid_p %>%
   #filter(str_detect(Site, "Duwamish", negate = T)) %>%
  dplyr::summarize(
    N = n(), #all site-years
    Years = paste(unique(Year), collapse = ", ")      ,
    Sites = paste(unique(Site), collapse = ", ")      ,
    RMSE = round(rmse_fn(obs = mean_ng_m3, pred = uk_trend_p_nox)),
    R2 = r2_mse_based(obs = mean_ng_m3, pred = uk_trend_p_nox)
  ) 
```

```{r}
 
bc_valid_overall %>%
  kable(digits = 2, 
        caption = "Out-of-sample validation for all AQS sites and years with BC observations (ng/m3). RMSE and R2 (MSE-based) are calculated around the 1-1 line.", 
          ) %>%
  kable_styling()

```

* NOx results look the same 

```{r}
bc_valid_overall_nox %>%
  kable(digits = 2, 
        caption = "NOx predictor model. Out-of-sample validation for all AQS sites and years with BC observations (ng/m3). RMSE and R2 (MSE-based) are calculated around the 1-1 line.", 
          ) %>%
  kable_styling()
```


**How much lower are predictions than observations?**

Predicted vs observed proporiton

```{r}
bc_valid_p %>%
  mutate(prop = uk_trend_p/mean_ng_m3,
         mean_prop = mean(prop)
         ) %>%
  {
  ggplot(., aes(y=prop, x=Year,
             shape = Site,
             col = Site,
             group=Site
             )) + 
  geom_point() + 
  geom_hline(yintercept = .$mean_prop[1], linetype=2,
             aes(fill = "Mean Proportion")
             ) + 
      labs(y = "Predicted/Observed BC Conc",
           title = "Predictions are lower than site observations"
           )
  }
  
```

```{r}
# table
 
## by site
prop_t1 <- bc_valid_p %>%
  mutate(prop = uk_trend_p/mean_ng_m3) %>%
  group_by(Site) %>%
  distribution.table(var.string = "prop", round.int = 2) 

## by year 
prop_t2 <- bc_valid_p %>%
  mutate(Year = as.factor(Year),
         prop = uk_trend_p/mean_ng_m3) %>%
  group_by(Year) %>%
  distribution.table(var.string = "prop", round.int = 2)  

## by site
bc_valid_p %>%
  mutate(prop = uk_trend_p/mean_ng_m3) %>%
  group_by(Site = "All") %>%
  distribution.table(var.string = "prop", round.int = 2) %>%
  rbind(prop_t1) %>%
  kable(caption = "Predictions as proportions of observed concentrations, by site. N = number of years.") %>%
  kable_styling()

## by year
bc_valid_p %>%
  mutate(prop = uk_trend_p/mean_ng_m3) %>%
  group_by(Year = "All") %>%
  distribution.table(var.string = "prop", round.int = 2) %>%
  rbind(prop_t2) %>%
  kable(caption = "Predictions as proportions of observed concentrations, by year N = number of sites") %>%
  kable_styling()


```


-by site

* The BC model performed better (lower RMSE) at some sites than others
  - Duwamish performs the worst when looking at the RMSE

```{r}
bc_valid_by_site <- bc_valid_p %>%
  group_by(Site) %>%
  dplyr::summarize(
    N = n(),  
    Years = paste(unique(Year), collapse = ", ")      ,
    RMSE = round(rmse_fn(obs = mean_ng_m3, pred = uk_trend_p)),
    R2 = r2_mse_based(obs = mean_ng_m3, pred = uk_trend_p)
  ) 

```

```{r}
# nox
bc_valid_by_site_nox <- bc_valid_p %>%
  group_by(Site) %>%
  dplyr::summarize(
    N = n(),  
    Years = paste(unique(Year), collapse = ", ")      ,
    RMSE = round(rmse_fn(obs = mean_ng_m3, pred = uk_trend_p_nox)),
    R2 = r2_mse_based(obs = mean_ng_m3, pred = uk_trend_p_nox)
  ) 
```


```{r}
scale_val <- 600

bc_valid_by_site %>%
  mutate(R2 = R2*scale_val,
         #sorten site names
         Site = str_remove(Site, "Seattle ")
         ) %>%
  gather(key = Measure, value = value, RMSE, R2) %>%
  ggplot(aes(x=Site, y=value, shape=Measure, col=N )) + 
  geom_point() +
  geom_hline(aes(yintercept = bc_valid_overall$RMSE, linetype="RMSE")) +
  geom_hline(aes(yintercept = bc_valid_overall$R2*scale_val, linetype="R2")) +
  # avoid overlapping x-axis labels
  scale_x_discrete(guide = guide_axis(n.dodge=2))+

  scale_y_continuous(sec.axis = sec_axis(~./scale_val, name = "R2")) + 
  labs(y = "RMSE (ng/m3)",
       title = "Out-of-sample validation results for the BC model by site",
       linetype = "All Site-Years",
       shape="Site-Specific",
       col = "No. Years"
       )
  
 
```

```{r}

bc_valid_by_site %>%
  kable(digits = 2, 
        caption = "Out-of-sample validation results for the BC model (ng/m3) by AQS site. N is the number of years avaialble for that site. RMSE and R2 (MSE-based) are calculated around the 1-1 line.", 
          ) %>%
  kable_styling()

```

```{r}
#nox

bc_valid_by_site_nox %>%
  kable(digits = 2, 
        caption = "NOx predictor model. Out-of-sample validation results for the BC model (ng/m3) by AQS site. N is the number of years avaialble for that site. RMSE and R2 (MSE-based) are calculated around the 1-1 line.", 
          ) %>%
  kable_styling()

```

-by year

* the BC model generally performs better (lower RMSE) during more recent years 

```{r}

bc_valid_by_yr <- bc_valid_p %>%
  filter(str_detect(Site, "Duwamish", negate = T)) %>%
  group_by(Year) %>%
  dplyr::summarize(
    N = n(),  
    Sites = paste(unique(Site), collapse = ", ")      ,
    RMSE = round(rmse_fn(obs = mean_ng_m3, pred = uk_trend_p)),
    R2 = r2_mse_based(obs = mean_ng_m3, pred = uk_trend_p)
  ) 
```

```{r}
#nox
bc_valid_by_yr_nox <- bc_valid_p %>%
  filter(str_detect(Site, "Duwamish", negate = T)) %>%
  group_by(Year) %>%
  dplyr::summarize(
    N = n(),  
    Sites = paste(unique(Site), collapse = ", ")      ,
    RMSE = round(rmse_fn(obs = mean_ng_m3, pred = uk_trend_p_nox)),
    R2 = r2_mse_based(obs = mean_ng_m3, pred = uk_trend_p_nox)
  ) 
```

* note, RMSE pattern is the opposite over time (RMSE increases) when Duwamish is dropped 

```{r}
scale_val <- 1e3

bc_valid_by_yr %>%
  mutate(R2 = R2*scale_val,
         ) %>%
  gather(key = Measure, value = value, RMSE, R2) %>%
  ggplot(aes(x=Year, y=value, shape=Measure, col=N)) + 
  geom_point() + 
  geom_smooth(se=F) +
  geom_hline(aes(yintercept = bc_valid_overall$RMSE, linetype="RMSE")) +
  geom_hline(aes(yintercept = bc_valid_overall$R2*scale_val, linetype="R2")) +
  scale_y_continuous(sec.axis = sec_axis(~./scale_val, name = "R2")) + 
  
  #only include whole #s in legend. setting 20 as the upper limit, which is never reached
  scale_color_gradient(breaks=c(seq(1,20))) +
  
  labs(y = "RMSE (ng/m3)",
       title = "Out-of-sample validation results for the BC model by year",
       linetype = "All Site-Years", #"Overall",
       shape="Year-Specific",
       col = "No. Sites"
       )
```

```{r}

bc_valid_by_yr %>%
  kable(digits = 2, 
        caption = "Out-of-sample validation by year. N is the number of sites avaialble for that year. RMSE and R2 (MSE-based) are calculated around the 1-1 line.", 
          ) %>%
  kable_styling()

```

```{r}
#NOX
bc_valid_by_yr_nox %>%
  kable(digits = 2, 
        caption = "NOx predictor model. Out-of-sample validation by year. N is the number of sites avaialble for that year. RMSE and R2 (MSE-based) are calculated around the 1-1 line.", 
          ) %>%
  kable_styling()

```


## UFP

```{r}
#plot 
ufp_valid_p %>%
  ggplot(aes()) + 
  geom_point(aes(x=Year, y=uk_trend_p,
                 col = "Predicted"
                 )) + 
  geom_rect(aes(
    #xmin = 0, xmax=4000,
    xmin = 2000, xmax=2002,
    ymin = ufp_valid_p$pt_cm3_upper_bin,
    ymax = ufp_valid_p$pt_cm3_lower_bin,
   fill = "Observed" #"Kim et al. (2004)" 
  ),
  alpha=0.3
  ) +
  scale_y_continuous(sec.axis = sec_axis(~./ufp_valid_p$pt_cm3_upper_bin,
                                         name = "Proportion of Lower Observed", 
                                         breaks = c(#seq(0.4, 2, .2),
                                                    round(ufp_valid_p$uk_trend_p/ufp_valid_p$pt_cm3_upper_bin,2),
                                                    1,
                                                    round(ufp_valid_p$pt_cm3_lower_bin/ufp_valid_p$pt_cm3_upper_bin, 2)
                                                    )
                                         ),
                     labels = scales::comma, #function(x) format(x, scientific = FALSE),
                     breaks = c(#seq(4e4,3e5, 2e4),
                                ufp_valid_p$uk_trend_p,
                                ufp_valid_p$pt_cm3_upper_bin,
                                ufp_valid_p$pt_cm3_lower_bin
                                )
                     ) +
  scale_x_continuous(breaks = 2001) +
  labs(y = "UFP (pt/cm3)",
       fill = "",
       col = "",
       title = "Observed (Kim et al., 2004) and predicted UFP concentration for Beacon Hill"
       )
 
```

```{r}
ufp_valid_p %>%
  ggplot(aes()) + 
  geom_point(aes(x=Year, y=uk_trend_p_nox,
                 col = "Predicted"
                 )) + 
  geom_rect(aes(
    #xmin = 0, xmax=4000,
    xmin = 2000, xmax=2002,
    ymin = ufp_valid_p$pt_cm3_upper_bin,
    ymax = ufp_valid_p$pt_cm3_lower_bin,
   fill = "Observed" #"Kim et al. (2004)" 
  ),
  alpha=0.3
  ) +
  scale_y_continuous(sec.axis = sec_axis(~./ufp_valid_p$pt_cm3_upper_bin,
                                         name = "Proportion of Lower Observed", 
                                         breaks = c(#seq(0.4, 2, .2),
                                                    round(ufp_valid_p$uk_trend_p/ufp_valid_p$pt_cm3_upper_bin,2),
                                                    1,
                                                    round(ufp_valid_p$pt_cm3_lower_bin/ufp_valid_p$pt_cm3_upper_bin, 2)
                                                    )
                                         ),
                     labels = scales::comma,  
                     breaks = c(ufp_valid_p$uk_trend_p,
                                ufp_valid_p$pt_cm3_upper_bin,
                                ufp_valid_p$pt_cm3_lower_bin
                                )
                     ) +
  scale_x_continuous(breaks = 2001) +
  labs(y = "UFP (pt/cm3)",
       fill = "",
       col = "",
       title = "NOx predictor model. Observed (Kim et al., 2004) and predicted UFP concentration for Beacon Hill"
       )

```



RMSE here is also just the error since there is only one value.

```{r}
# table
ufp_valid_p %>%
  dplyr::summarize(
    N = n(),  
    Years = paste(unique(Year), collapse = ", ")      ,
    RMSE = round(rmse_fn(obs = pt_cm3_upper_bin, pred = uk_trend_p)),
    R2 = r2_mse_based(obs = pt_cm3_upper_bin, pred = uk_trend_p)
  ) %>%
  kable(caption = "UFP model out-of-sample performance") %>%
  kable_styling()

```

* NOx model performs nearly the same

```{r}
#nox
ufp_valid_p %>%
  dplyr::summarize(
    N = n(),  
    Years = paste(unique(Year), collapse = ", ")      ,
    RMSE = round(rmse_fn(obs = pt_cm3_upper_bin, pred = uk_trend_p_nox)),
    R2 = r2_mse_based(obs = pt_cm3_upper_bin, pred = uk_trend_p_nox)
  )%>%
  kable(caption = "NOx predictor UFP model out-of-sample performance") %>%
  kable_styling()


```


# Map predictions to a grid over time

```{r}

pollutant_units <- c("BC (ng/m3)", "UFP (pt/cm3)")
#place to save maps
bc_plots <- list()
ufp_plots <- list()

for (i in seq_along(pollutants)) {
  #i=1
  #select a specific year
  df1 <- grid_uk_trend_predictions %>%
      filter(Pollutant == pollutants[i])
  
  # range of all yrs for any given pollutant
  plot_range <- data.frame(
    min = min(df1$uk_trend_p, na.rm = T),
    max = max(df1$uk_trend_p, na.rm = T)
  )
  
  for(yr in seq_along(mapping_yrs)) {
    #yr=1
    # select a single pollutant
    df2 <- df1 %>% 
    filter(Year == mapping_yrs[yr]) %>% 
      select(Pollutant, latitude, longitude, uk_trend_p)
    
    p_map <- map0 +
        geom_point(data = df2,
                   aes(x = longitude, y = latitude, 
                       col = uk_trend_p),
                   alpha=1) + 
    scale_color_gradient(name=paste0(pollutant_units[i]), 
                         low = "yellow", high = "red", 
                         #plot all maps on same scale
                         limits = c(plot_range$min, plot_range$max)
                         ) +
      
      #plot w/ same dimensions as last plot, which needs this for the scale & N arrow
      geom_sf(data = grid_shp, inherit.aes = FALSE,
          #don't actually show dots
          alpha=0) +

  
    theme_bw() +
    #avoid overlapping x-axis labels
    #scale_x_continuous( n.breaks = 4) +
    # theme(
    #   legend.justification=c(0,1), #legend.justification=c(1,0), 
    #   legend.position=c(0,1),  #legend.position=c(1,0), 
    #   legend.background =  element_blank()
    #   ) +
      
    labs(
      title =mapping_yrs[yr],
      x = "Longitude",
      y = "Latitude",
      shape = ""
      ) 

    if(yr==3) {
      
      p_map <- p_map +
      # add scale & N arrow to top left
      # geom_sf(data = grid_shp, inherit.aes = FALSE,
      #     #don't actually show dots
      #     alpha=0) +
      annotation_scale(data = grid_shp, location = "tr") +
      annotation_north_arrow(location = "tr",
                             #point towards North Pole
                             which_north = "true",
                             pad_y = unit(0.5, "in"),
                             style = north_arrow_fancy_orienteering
                             )  


    # # #markers
    # geom_point(data = markers, aes(x=x, y=y, shape = marker)) +
    # scale_shape_manual(values=c(12, 1, 2)) +
    # #make longer legend text appear at top
    # guides(col = guide_colorbar(order = 2),
    #             shape = guide_legend(order = 1)
    #        ) 
      
      
    }
    
    
    
     #p_map
    
    # save BC/UFP plots 
    if(pollutants[i] == "bc") {
      bc_plots[yr] <- list(p_map)
      names(bc_plots)[yr] <- mapping_yrs[yr]
    }
    
    if(pollutants[i] == "ufp") {
      ufp_plots[yr] <- list(p_map)
      names(ufp_plots)[yr] <- mapping_yrs[yr]
    }

  }
  
}
 
```

```{r}
#bc 
bc_plots %>%
  ggarrange(plotlist = .,
          nrow = 1,
          common.legend = T, legend = "right"
          ) %>%
  annotate_figure(top = "Predicted BC Exposure Surfaces" ) 

#ufp 
ufp_plots %>%
  ggarrange(plotlist = .,
          nrow = 1,
          common.legend = T, legend = "right") %>%
  annotate_figure(top = "Predicted UFP Exposure Surfaces" ) 


```

Prediction differences

* TRAP concentration differences vary over space. This indicates that TRAP concentrations are predicted to have dropped more drastically at some locations (e.g., downtown Seattle and highways) than other locations over time.
  - the general trend in decreasing concentrations over time is a result of using the trend adjustment
  - the spatial differences in concentration differences are a result of using TVCs in our models

```{r}
diff_plots <- list()

for (i in seq_along(pollutants)) {
  #i=1
  #select a specific year
  df1 <- grid_uk_trend_predictions %>%
      filter(Pollutant == pollutants[i],
             Year %in% c(first_p_yr, last_p_yr)) %>%
    select(Pollutant, site_id, latitude, longitude, Year, uk_trend_p) %>%
    #calculate prediction differences
    spread(Year, uk_trend_p) %>%
    mutate(diff_p = `2019`-`1995`)
    
    p_map <- map0 +
        geom_point(data = df1,
                   aes(x = longitude, y = latitude, 
                       col = diff_p),
                   alpha=1) + 
    scale_color_gradient(name=paste0(pollutant_units[i]), 
                         #low = "yellow", mid = "white", high = "red",  
                         ) +
      
           # add scale & N arrow to top left
      geom_sf(data = grid_shp, inherit.aes = FALSE,
          #don't actually show dots
          alpha=0) +
      annotation_scale(data = grid_shp, location = "tr") +
      annotation_north_arrow(location = "tr",
                             #point towards North Pole
                             which_north = "true",
                             pad_y = unit(0.5, "in"),
                             style = north_arrow_fancy_orienteering
                             ) +
        
        
    # # #markers
    # geom_point(data = markers, aes(x=x, y=y, shape = marker)) +
    # scale_shape_manual(values=c(12, 1, 2)) +
    # #make longer legend text appear at top
    # guides(col = guide_colorbar(order = 2),
    #             shape = guide_legend(order = 1)
    #       ) +
  
    theme_bw() +
    theme(
      legend.justification=c(0,1), #legend.justification=c(1,0), 
      legend.position=c(0,1),  #legend.position=c(1,0), 
      legend.background =  element_blank()
      ) +
      
    labs(
      title =toupper(pollutants[i]),
      x = "Longitude",
      y = "Latitude"
      ) 

    #p_map
    
    diff_plots[i] <- list(p_map)
    names(diff_plots)[i] <- pollutants[i]

  }
  
 
diff_plots %>%
  ggarrange(plotlist = .,
          nrow = 1) %>%
  annotate_figure(top = paste0("Differences in predicted BC and UFP exposure surfaces (", last_p_yr, "-", first_p_yr, ")") )

```





# Possible Additional Analyses

??? could do sensitivity analysis similar to A1 w/ & w/o temporally-adjusted predictions

```{r}

```


\newpage
# Code

```{r,ref.label=knitr::all_labels(),echo=TRUE,eval=FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=60), include=F}
```
 