---
title: 'Aim 3: UK PLS models and validation'
author: "Magali Blanco"
date: ' `r Sys.Date()` '
output:
  html_document: 
    number_sections: yes
    toc: yes
editor_options:
  chunk_output_type: console
---


```{r setup, include=FALSE}
 knitr::opts_chunk$set(echo = F, 
                      cache=T, cache.comments = F, 
                      message = F, warning = F, 
                      tidy.opts=list(width.cutoff=60), tidy=TRUE,
                      fig.height = 5, fig.width = 8
                      )  

# Clear workspace of all objects and unload all extra (non-base) packages
rm(list = ls(all = TRUE))
if (!is.null(sessionInfo()$otherPkgs)) {
  res <- suppressWarnings(
    lapply(paste('package:', names(sessionInfo()$otherPkgs), sep=""),
      detach, character.only=TRUE, unload=TRUE, force=TRUE))
}

pacman::p_load(knitr, kableExtra, 
               #descriptive statistics
               Hmisc, EnvStats, 
               # modeling
               pls, geoR, #gstat - alternative for UK
               akima, # interp() - interpolate predictions on map
               ggpubr, tidyverse,
               # 3D mapping
               rayshader,
               
               #parallel  # mclapply() for parallized processing;  detectCores()
               )    
#Himsc: describe(); EnvStats: summaryFull(); ggpubr: ggarrange()

set.seed(1)

options(knitr.kable.NA = '')
source("0.Global_Fns.R")
source("A2.0.1_Var&Fns.R")
source("A3.0.0_Var&Fns.R")

```

### --> why aren't these read from "0.Global_Fns.R"?  

```{r}
# #returns MSE
# mse <- function(obs, pred){
#   mean((obs - pred)^2)
#   }
# 
# rmse <- function(obs, pred){
#   sqrt(mean((obs - pred)^2))
#   }
# 
# #returns MSE-based R2
# r2_mse_based <- function(obs, pred) {
#   mse.est <- mse(obs, pred)
#   r2 <- 1- mse.est/mean((obs - mean(obs))^2)
#   max(0, r2)
#   }

```

# Purpose & Approach

## UK Model

This script builds off of the Universal Kriging work completed for Aim 2 (in A2.3_UK_v4.Rmd) by incorporating time-varying covariates and using a temporal trend adjustment factor to predict historical BC and UFP levels back in time. Our primary prediction models are: 

### --> update notation

We believe that we can predict TRAP concentrations at a particular location and time from that location's geocovariates (which may vary over time), with some error due to some unknowns. Our assumed underlying model is: 

$$ \tag{1} 
Ln(Conc_{s,t}^{}) = \theta_0 + \sum_{f=1}^{3}\theta^{f} Z_{s}^{f} + \theta^{pop} Z_{s,t}^{pop} + \theta^{NDVI} Z_{s,t}^{NDVI} + \theta^{EC} Z_{s,t}^{EC} + \epsilon_{s} $$

Where:

* $Ln(Conc_{s,t}^{})$ denotes the log-tranformed annual-average BC (ng/m3) or UFP (pt/cm3) concentration for a particular location $s$ __within our study area__ and year $t$, __where $t=1995-2019$.__

* $Z$ are dimension-reduced, linear combinations of geocovariate predictors for location $s$ that are fixed over time ($Z_s$), or for a specific time $t$ ($Z_{s,t}$). These are derived from PLS regression, such that: $Z_{s,m} = \sum_{j=1}^p \phi_{j,m}X_{s,j}$, where $p$ are our original geocovariate predictors and $m$ is the PLS component number. Specifically, 

  - $f$ denotes multiple geocovariates that are largely fixed ($f$) in time      
  - $pop$ denotes population density geocovariates    
  - $NDVI$ denotes Normalized Difference Vegetation Index (NDVI) geocovariates
  - $EC$ denotes elemental carbon (EC) emissions geocovariates

* $\theta$ are the model coefficients.

* $\epsilon_{s}$ is the residual term with mean zero and a geostatistical structure assumed to be an exponential function with range $\phi$, partial sill $\sigma$ and nugget $\Theta_{s}$.


Given our data, we estimate the modeling parameters for a UK model with PLS:

$$ \tag{2} 
\hat{Ln(Conc_{s^*,t^*})} = \hat{\theta}_0 + \sum_{f=1}^{3}\hat{\theta}^{f} Z_{s^*}^{f} + \hat{\theta}^{pop} Z_{s^*,t^*}^{pop} + \hat{\theta}^{NDVI} Z_{s^*,t^*}^{NDVI} + \hat{\theta}^{EC} Z_{s^*,t^*}^{EC} $$

where:

* ($\hat{Ln(Conc_{s,t}^{})}$) is the log-tranformed annual-average BC (ng/m3) or UFP (pt/cm3) concentration 

* [ __???__  __do the Zs need a hat?__] The PLS regression loadings ($\sum_{j=1}^p \phi_{j,m}$) are calibrated using TRAP concentrations and geocovariates from our mobile monitoring campaign stops ($s^*$, where $s^*=1,...,309$) for the 2019 year ($t*=2019$): $Z_{s^*,t^*,m} = \sum_{j=1}^p \hat{\phi}_{j,m}X_{s^*,t^*,j}$   

* The model coefficients are estimated ($\hat{\theta}$) using TRAP concentrations and geocovariates from our 2019 mobile monitoring campaign.  


Next, wee estimate TRAP concentrations from 1995-2019 at cohort locations within our study  area using a two-step approach:

A. First, the fitted model is used to estimate TRAP concentrations for specific locations and times. Estimates for particular locations vary over time based on changing geocovariate values ($pop, NDVI, EC$). 

$$ \tag{3} 
\hat{Ln(Conc_{s,t})} = \hat{\theta}_0 + \sum_{f=1}^{3}\hat{\theta}^{f} Z_{s}^{f} + \hat{\theta}^{pop} Z_{s,t}^{pop} + \hat{\theta}^{NDVI} Z_{s,t}^{NDVI} + \hat{\theta}^{EC} Z_{s,t}^{EC} $$

B. Next, we exponentiate and apply a temporal trend to these estimates to account for the fact that TRAP concentrations have been steadily decreasing since the early 1990s:
 
$$\tag{4} 
\hat{Conc}_{s,t} = exp(Ln(Conc_{s,t})) + \alpha_t^{trend} $$

Where: 

* $\alpha_t^{trend}$ is the temporal trend adjustment based on historical observations of EC at the Beacon Hill AQS site.

To calculate $\alpha_t^{trend}$, we first fit the following model:

$$\tag{5}
\hat{EC_{t}} = \hat{\beta}_0 + \sum_{l=1}^{3}\hat{\beta}_{l}b_{l}{t}$$  

Where:

* $\hat{EC_{t}}$ is the predicted, smoothened annual average EC concentration for Beacon Hill for a particular year   

* $\hat{\beta}$ are the estimated model parameters 
* $t$ is the observation year, fit as a cubic spline using $b$ basis functions


$\alpha_t^{trend}$ is calculated as the difference between the predicted EC concentration for any given year $t$ and the predicted EC concentration for 2018 (the reference year), such that:

$$\tag{6a}
\alpha_t^{trend} = \hat{EC_{t}} - \hat{EC}_{2018}   $$

### --> or multiplicative trend:

$$\tag{6b}
\alpha_t^{trend} = \hat{EC_{t}} / \hat{EC}_{2018}   $$





The models separate space and space-time geocovarites in order to:

a) inter-/intra-polate a single linear combinations composed of related geocovariates (e.g., different buffers for the same covariate) rather than many individual covariate buffers each year, and    
b) estimate model coeffiecients for time-varying covariate


## Validation

Furthermore, this script conducts out-of-sample validation to check the model's historical BC (and UFP) predictions at AQS sites. Specifically, we compare model predictions to historical observations at AQS sites:   

* overall
* by site
* stratified by whether or not AQS sites are in study area
* by year/decade 
  

For quality control purposes, we:   

* Check that model predictions are not increasingly bias back in time
* Check whether some sites are more accurately predicted than others 

## Sensitivity Analyses 


```{r}
### --> ? diff ML approaches: lasso, random forests...with stacked ensemble?


cbind(
  Analysis = c("Primary: EC emissions, additive temporal trend adjustment", 
               "NOx Emissions (vs EC)",
               "Ratio temporal trend adjustment (vs additive)",
               "No temporal trend adjustment"
               )
  ) %>%
  kable(., caption = "Description of primary and sensitivity analyses") %>%
  kable_styling()


```

```{r}
# upload datasets 
#mm annual estimates & covariates
mm0 <- readRDS(file.path("Data", "Aim 2", "Mobile Monitoring", "annual_2020-03-17.rda")) %>%
  # drop sensitivity analysis estimaets for Aim 2
  select(site_id, contains("primary")) %>%
  rename_at(vars(contains("primary")), ~gsub("_primary", "", .))

## still has: [1] MS0000 MS0398     #MS0601 replaced MS0398
cov_mm <- readRDS(file.path("Data", "Aim 2", "Geocovariates", "cov_mm_preprocessed.rda")) %>%
  #drop Roosevelt garage & stop w/ 1 obeservation that was replaced by MS0601
  filter(!site_id %in% c("MS0000", "MS0398"))
   
## act geocovariates
### ~cov_act_all in A2 code
cohort <- readRDS(file = file.path("Data", "Aim 2", "Geocovariates", "cov_act_preprocessed.rda"))

mm <- mm0 %>%
  #convert to log
  mutate_at(vars(contains(c("ufp", "bc"))), ~log(.)) %>%
  left_join(cov_mm)  

#AQS observations for validation? Or use year-round estimates from PSCAA?
mm_aqs_2019 <- mm %>%
  filter(str_detect(site_id, "MC"))

#only keep non-AQS site stops
mm <- mm %>%
  filter(str_detect(site_id, "MS"))

# group ACT locations by location
monitoring_ids <- cohort$site_id[grepl("monitoring", cohort$site_location)] 

outside_monitoring_in_study_ids <- cohort$site_id[grepl("study", cohort$site_location)] 
outside_monitoring_in_st_ids <- cohort$site_id[grepl("study|st", cohort$site_location)]

study_ids <- append(monitoring_ids, outside_monitoring_in_study_ids)
st_ids <- append(monitoring_ids, outside_monitoring_in_st_ids)


# ACT cov for primary analysis
cohort <- cohort %>% filter(site_id %in% study_ids)


# grid in study area (from GIS)
grid_in_study <- read.csv(file.path("..", "GIS", "Shapefiles", "Predictions", "grid", "grid_200602.csv")) %>%
  filter(study == TRUE) %>%
  select(location_id = location_i)

grid_in_study <- grid_in_study$location_id

# grid cov
grid <- readRDS(file = file.path("Data", "Aim 2", "Geocovariates", "cov_grid_preprocessed.rda")) %>%
  # only keep points in study area-land (grid isn't large enough for st area)
  filter(site_id %in% grid_in_study)

#save grid for testing
#saveRDS(grid, file = file.path("Output", "Aim 3", "grid.rda"))

# main datasets
# mm 
# cohort 
# grid 

```

```{r}
# CV results from A2
a2_cv_results <- readRDS(file.path("Output", "Aim 2", "Tables", "3. UK", "cv_results.rda")) %>%
label_analysis(var = "Analysis") %>%
    filter(grepl("primary", Analysis, ignore.case = T)) %>%
  select(Pollutant, PLS_Components, Variogram_Distance_Fraction, RMSE, R2) %>%
  mutate(Aim = 2)
 
```

### --> need to add geocovariates for AQS sites not in MM campaign 

```{r}
# trend adjustment
## convert ug/m3 (10^-6) to ng/m3 (10^-9)
conversion_factor <- 1e3 
trend_adjustment <- read_rds(file.path("Data", "Aim 3", "Hx BC at AQS Sites", "trend_adjustment.rda")) %>%
  #mutate(difference = difference*conversion_factor) %>%
  select(Year, ratio_adjustment = ratio)

# Hx BC validation - Hx AQS readings 
bc_validation <- read_rds(file.path("Data", "Aim 3", "Hx BC at AQS Sites", "aqs_avgs_for_validation.rda")) %>%
  mutate(mean_ng_m3 = mean*conversion_factor,
         site_id = NA,
         Pollutant = "bc"
         ) %>%
  select(-c(mean, no_zero_readings))  
 
## add site_id #s for merging later since these sites have slightly different site names
bc_validation$site_id[str_detect(string = bc_validation$Site, pattern = "Weller")] <- "MC0120"
bc_validation$site_id[str_detect(string = bc_validation$Site, pattern = "Beacon")] <- "MC0003"
bc_validation$site_id[str_detect(string = bc_validation$Site, pattern = "Duwamish")] <- "MC0126"
bc_validation$site_id[str_detect(string = bc_validation$Site, pattern = "Kent")] <- "MC0406"
bc_validation$site_id[str_detect(string = bc_validation$Site, pattern = "Allentown")] <- "MC0002"

### --> add site_ids for other sites  when get new datasaet? or merge them in 


# UFP validation - BH 2001
ufp_validation <- readRDS(file.path("Data", "Aim 3", "Hx UFP at BH", "BH_2001_UFP.rda")) %>%
  mutate(site_id = "MC0003",
         Pollutant = "ufp",
         Year = 2001
         )

```

Add geocovariates to validation AQS sites so we can make predictions here later

### --> need geocovariates for AQS sites not in MM campaign 

```{r}
#add geocovariates to validation AQS sites so we can make predictions here later 
aqs <- bc_validation %>%
  select(Site) %>%
  unique() 

aqs$site_id <- NA
aqs$site_id[str_detect(string = aqs$Site, pattern = "Weller")] <- "MC0120"
aqs$site_id[str_detect(string = aqs$Site, pattern = "Beacon")] <- "MC0003"
aqs$site_id[str_detect(string = aqs$Site, pattern = "Duwamish")] <- "MC0126"
aqs$site_id[str_detect(string = aqs$Site, pattern = "Kent")] <- "MC0406"
aqs$site_id[str_detect(string = aqs$Site, pattern = "Allentown")] <- "MC0002"

aqs <- aqs %>%
  #add geocovariates
  left_join(mm_aqs_2019) %>%
  #won't be using these annual avg estimates from the MM campaign
  select(-c(bc, ufp))

```



```{r}
# pop density 2010+ for Seattle-Tacoma-Bellevue, WA Metro Area from annual Census ACS surveys  

# place to save total estimates for all files

pop_folder <- file.path("Data", "Aim 3", "Population", "Census", "ACS-yearly")

# files w/ data...end w/ csv
pop_files <- list.files(pop_folder) %>% 
  str_subset("data_with_overlays.*csv$")


acs <- data.frame(
  # get years from file names
  Year = as.numeric(str_extract(pop_files, "[0-9]{4}")),
  Estimate = NA)


for (i in seq_along(acs$Year)) {
  #i=1
  #pop_file <- paste0("ACSDP1Y", acs$Year[i], ".DP05_data_with_overlays_2020-06-15T124502.csv")
  
                                                                  #total pop estimate
  acs$Estimate[i] <- read.csv(file.path(pop_folder, pop_files[i]))[2, "DP05_0001E"] %>% 
    #convert factor to number
    as.character() %>% as.numeric()

}

#estimate avg annual change (proportion) relative to 2010 for the entire area
pop_2010_2018_prop <-  (acs$Estimate[nrow(acs)] - acs$Estimate[1])/acs$Estimate[1]/(acs$Year[nrow(acs)] - acs$Year[1] )
 
```


```{r}
# acs%>%
#   ggplot(aes(x=Year, y=Estimate)) +
#   geom_point() +
#   labs(title = "1 yr ACS survey total population estimate",
#        subtitle = "for the Seattle-Tacoma_Bellevue metropolitan area"
#        )

```

```{r}
# shapefiles for maps

#background map slightly larger than the study area  
study_area <- readRDS(file.path("Data", "GIS", "study_area_df.rda"))
monitoring_area <- readRDS(file.path("Data", "GIS", "monitoring_area_df.rda")) 
map0 <- map_base(dt=study_area, latitude_name = "lat", longitude_name = "long")
aviation_area <- readRDS(file.path("Data", "GIS", "zoning_df.rda")) 
airports_area <- readRDS(file.path("Data", "GIS", "airports_df.rda")) 

```

###--> check that n_components still the same for BC & UFP

```{r}
# common variables

site_loc_vars <- cov_mm %>% select(site_id:lambert_y) %>% names()

space_vars <- cov_mm %>%
  select(-site_loc_vars,
         -contains(c("pop", "ndvi"#, 
                    #"ec"
                    ))) %>%
  names()

# modeling parameters from Aim 2

n_components <-  min(a2_cv_results$PLS_Components) #3 
variog_dist_frac <- 0.10
  
# n_components_ufp <- a2_cv_results$PLS_Components[str_detect(a2_cv_results$Pollutant, "UFP")]
# n_components_bc <- a2_cv_results$PLS_Components[str_detect(a2_cv_results$Pollutant, "BC")]
variog_dist_frac_ufp <- a2_cv_results$Variogram_Distance_Fraction[str_detect(a2_cv_results$Pollutant, "UFP")]
variog_dist_frac_bc <- a2_cv_results$Variogram_Distance_Fraction[str_detect(a2_cv_results$Pollutant, "BC")]


# years obtained for TVCs
census_data_yrs <- c(1990, 2000, 2010)
last_acs_yr <- max(acs$Year)
### --> check that these years are correct
ndvi_data_yrs <- c(2006) #c(1990:1993, 2006, 2010)
emissions_data_yrs <- c(seq(1990, 2015, by = 5))

#PLS modeling year used
pop_model_yr <- 2010

### --> update
ndvi_model_yr <- 2006
emissions_model_yr <- 2015

# first prediction year
first_p_yr <- 1995
last_p_yr <- 2019

# pollutant names
pollutants <- c("bc", "ufp")

#mapping years
mapping_yrs <- c(seq(first_p_yr, last_p_yr-1, 15), 2019)

```

# Available time-varying covariates (TVCs)

Each of these TVCs has several buffers for each year.

* Decenial US Census surveys (1990, 2000 and 2010) 
* NDVI for 1990-1993,2006 and 2010/2019
* Emissions every 5 years from 1990-2015 and for 2019 from MOVES emission factors for King County (1995-2019) and WADOT AADT (1995-2015) estimates 


# Create dimension-reduced versions of time-varying covariates

Transforming space-varying covariates (for M=3 components), population density (M=1), NDVI (M=1) and EC emissions (M=1) into linear combinations (scores) using PLS regression and 2019 TRAP observations (Y).

$Z_m = \sum_{j=1}^p \phi_{j,m}X_j$, where $p$ are our original predictors. 
 
Fit PLS models using 2019 TRAP (BC, UFP) and:

* space-varying covariates
* population: Census 2010   
* NDVI __2006__ [for now]
* emissions 2019 [not yet]   

```{r}
pop_model_vars <- "pop10_"
ndvi_model_vars <- "ndvi_"

space_models <- fit_pls(dt = mm, x = space_vars, .ncomp = n_components)
pop2010_models <- fit_pls(dt = mm, x = pop_model_vars, .ncomp = 1)
ndvi2006_models <- fit_pls(dt = mm, x = ndvi_model_vars, .ncomp = 1)
#ec2019_models <- fit_pls(dt = mm, x = "ec_", .ncomp = 1)

```

Variability explained by each model.

```{r}
variance_explained <- data.frame()

for(i in seq_along(pollutants)) {
  
  space_var <- explvar(space_models[[pollutants[i]]]) %>% as.vector()
                                                    
  pop_var <- explvar(pop2010_models[[pollutants[i]]]) %>% as.vector() %>% 
    # NAs for 2nd & 3rd components 
    append(rep(NA, n_components-1))
  ndvi_var <- explvar(ndvi2006_models[[pollutants[i]]]) %>% as.vector() %>% 
    append(rep(NA, n_components-1))
   
  # ec_var <- explvar(ec2019_models[[pollutants[i]]]) %>% as.vector() %>% 
  #   append(rep(NA, n_components-1))
  
  var0 <- rbind(space_var,
        pop_var,
        ndvi_var
        ) %>%
    as.data.frame() %>%
    rename_all( ~ names(explvar(space_models$bc))) %>%
    rownames_to_column(var = "Model") %>%
    mutate(
      Model = recode_factor(factor(Model),
                            space_var = "Space",
                            pop_var = "Population Density",
                            ndvi_var = "NDVI"
                            #ec_var = "EC"
                            ),
      Pollutant = toupper(pollutants[i])
      ) %>%
    select(Pollutant, everything())
  
  variance_explained <- rbind(variance_explained, var0)
   
}

variance_explained %>%
    kable(
      caption = paste0("Percent of variability explained by each PLS model for annual average ", toupper(pollutants[i] )),
      digits = 1
    ) %>%
    kable_styling() 

```


**Geocovariate loadings from models fit to 2019 MM observations**

Space models

```{r, fig.height=10}

pls_loadings <- data.frame()

for(i in seq_along(pollutants)) {
  #i=1
  pls_loadings0 <- as.data.frame(space_models[[pollutants[i]]]$loadings[]) %>%
    rownames_to_column(var = "cov") %>%
    # rename variables if buffers
    split_cov_name(cov = "cov") %>%
    #make long format for faceting
    gather(key = "Component", value = "Loading", contains("Comp")) %>%
    mutate(Component = as.numeric(substr(Component, 6, nchar(Component))),
           pollutant = toupper(pollutants[i])
           )   
  
  pls_loadings <- rbind(pls_loadings, pls_loadings0)
  
}

pls_loadings  %>%
  #buffered covariates
  drop_na(buffer) %>%
  ggplot(aes(x = Loading, y = cov)) +
  geom_point(aes(size=buffer, col=buffer), shape=1) +
  scale_size(breaks = c(min(pls_loadings$buffer, na.rm = T),
                        max(pls_loadings$buffer, na.rm = T))) +  
  #non-buffered covariates
  geom_point(data = pls_loadings[is.na(pls_loadings$buffer),],
           aes(shape="")) +
  geom_vline(xintercept=0,
             linetype="solid") +
  facet_grid(pollutant~Component, labeller = "label_both") +
  labs(y = "Geocovariate",
       shape= "non-buffer", 
       size = "buffer size (m)",
       col = "buffer size (m)",
       title = paste0(toupper(pollutants[i]), " PLS geocovariate component loadings for space-varyign covariates")) +
  theme(legend.position = "bottom") 

```

TVC models 

```{r}

########################################################################################################
# returns PLS loading plots for TVCs and each pollutant

loading_plots_tvcs <- function(dt, tvc_name) {
  
  #plot_list <- list()
  pls_loadings <- data.frame()
  
  for(i in seq_along(pollutants)) {
    #i=1
    pls_loadings0 <- as.data.frame(dt[[pollutants[i]]]$loadings[]) %>%
      rownames_to_column(var = "cov") %>%
      # rename variables if buffers
      split_cov_name(cov = "cov") %>%
      #make long format for faceting
      gather(key = "Component", value = "Loading", contains("Comp")) %>%
      mutate(Component = as.numeric(substr(Component, 6, nchar(Component))),
             Pollutant = toupper(pollutants[i])
             )   
  
  pls_loadings <- rbind(pls_loadings, pls_loadings0)
  
  }
  
  plot <- pls_loadings  %>%
    mutate(buffer = factor(buffer)) %>%
    ggplot(aes(x = Loading, y = cov, size=buffer, col=buffer)) +
    geom_point(shape=1) +
    geom_vline(xintercept=0,
               linetype="solid") +
    facet_grid(Pollutant~Component, labeller = "label_both") +
    labs(y = "Geocovariate",
         title = paste0(tvc_name, " component loadings for PLS model")
         ) +
    theme(legend.position = "bottom") 
    
  #   plot_list[i] <- list(plot)
  #   names(plot_list)[i] <- pollutants[i]
  # }
  
  return(plot)
  
  }

########################################################################################################

```

```{r}
loading_plots_tvcs(dt = pop2010_models, tvc_name = paste0("population density (", pop_model_yr, ")"))
  
loading_plots_tvcs(dt = ndvi2006_models, tvc_name = paste0("NDVI (", ndvi_model_yr, ")"))  

# loading_plots_tvcs(dt = ec2015_models, paste0("EC (", emisssions_model_yr, ")"))  
 
```

```{r}

### --> ? don't need mm dataset since don't want to predict here? should predict at all AQS sites instead?


#list of different datasets where we want PLS scores
dt_list <- list(mm = mm, 
                cohort = cohort, 
                grid = grid,
                aqs = aqs)

```

**Use 2019 mobile monitoring fitted models to estimate PLS scores for other times and/or locations.** 

space-varying covariates

```{r}

# place to save PLS scores 
bc_scores_space <- list()
ufp_scores_space <- list()

for(i in seq_along(dt_list)) {
  #i=1
  #get scores for each dataset in list using BC PLS model
  bc_scores_space[i] <- list(get_scores(dt = dt_list[[i]], 
                              rename_vars = FALSE, 
                              pls_model = space_models$bc, 
                              rename_components = paste0("space", c(1:n_components)))
                             )
  
  ufp_scores_space[i] <- list(get_scores(dt = dt_list[[i]], 
                              rename_vars = FALSE, 
                              pls_model = space_models$ufp, 
                              rename_components = paste0("space", c(1:n_components)))
                             )
  
  names(bc_scores_space)[i] <- names(dt_list)[i]
  names(ufp_scores_space)[i] <- names(dt_list)[i]

}


```

Population density

```{r}
 
####################################################################################################

# for POP scores. repeates get_scores() for different datasests (mm, cohort, grid), pollutants (BC, UFP) and population years (1990, 2000, 2010)

get_scores_pop_models <- function( 
  #diff datasets
  .dt,
  # diff pollutant models
  .pls_model#,
  # variables used to build models (e.g., for renaming )
  #.pop_model_vars = pop_model_vars
  ) {
  
  # scores for Census 2010
  pop2010_scores <- get_scores(dt = .dt, 
                               # no need to change var names here
                               rename_vars = FALSE,
                               pls_model = .pls_model,
                               rename_components = "pop2010") 
  
  # scores for Census 2000
  pop2000_scores <- get_scores(dt = .dt, 
                               #var names to change to match model variables (for estimating/predicting scores)
                               rename_vars = TRUE, 
                               change_var_name = "pop_",
                               pls_model = .pls_model, 
                               rename_components = "pop2000") 

  # scores for Census 1990 
  pop1990_scores <- get_scores(dt = .dt, 
                               rename_vars = TRUE, 
                               change_var_name = "pop90_",
                               pls_model = .pls_model, 
                               rename_components = "pop1990")
  
  #combine scores for diff years
  pop_scores <- cbind(pop1990_scores, pop2000_scores, pop2010_scores) 
  
  return(pop_scores)
  
  }

####################################################################################################

```

```{r}

# place to save PLS scores 
bc_scores_pop <- list()
ufp_scores_pop <- list()

for (i in seq_along(dt_list)) {
  #i=1
  bc_scores_pop[i] <- list(get_scores_pop_models(.dt = dt_list[[i]], .pls_model = pop2010_models$bc) )
  ufp_scores_pop[i] <- list(get_scores_pop_models(.dt = dt_list[[i]], .pls_model = pop2010_models$ufp) )
  
  names(bc_scores_pop)[i] <- names(dt_list)[i]
  names(ufp_scores_pop)[i] <- names(dt_list)[i]
}


```

NDVI

### --> update once get new variables 

```{r}
####################################################################################################

# for NDVI scores. repeates get_scores() for different datasests (mm, cohort, grid), pollutants (BC, UFP) and NDVI years (1990-93, 2006, 2010)

get_scores_ndvi_models <- function( 
  #diff datasets
  .dt,
  # diff pollutant models
  .pls_model
  ) {
  
  # scores for NDVI 2006
  ndvi2006_scores <- get_scores(dt = .dt, 
                               # no need to change var names here
                               rename_vars = FALSE,
                               pls_model = .pls_model,
                               rename_components = "ndvi2006") 
  
  # # scores for NDVI 1990-1993
  # ndvi1990_scores <- get_scores(dt = .dt, 
  #                              #var names to change to match model variables (for estimating/predicting scores)
  #                              rename_vars = TRUE, 
  #                              
  #                              ### --> update 
  #                              change_var_name = "ndvi1990_",
  #                              
  #                              pls_model = .pls_model, 
  #                              rename_components = "ndvi1990") 
  # 
  # # scores for Census 1990 
  # ndvi2010_scores <- get_scores(dt = .dt, 
  #                              rename_vars = TRUE, 
  #                               
  #                              ### --> update 
  #                              change_var_name = "ndvi2010_",
  #                              
  #                              pls_model = .pls_model, 
  #                              rename_components = "ndvi2010")
  
  #combine scores for diff years
  ndvi_scores <- cbind(
    #ndvi1990_scores, 
    ndvi2006_scores#, 
    #ndvi2010_scores
    )  
  
  return(ndvi_scores)
  
  }

####################################################################################################

```

```{r}
 # place to save PLS scores 
bc_scores_ndvi <- list()
ufp_scores_ndvi <- list()

for (i in seq_along(dt_list)) {
  #i=1
  bc_scores_ndvi[i] <- list(get_scores_ndvi_models(.dt = dt_list[[i]], .pls_model =ndvi2006_models$bc) )
  ufp_scores_ndvi[i] <- list(get_scores_ndvi_models(.dt = dt_list[[i]], .pls_model = ndvi2006_models$ufp) )
  
  names(bc_scores_ndvi)[i] <- names(dt_list)[i]
  names(ufp_scores_ndvi)[i] <- names(dt_list)[i]
}


 
```
 

emissions

```{r}

```



**Combine all PLS scores**

Datasets of linear combinations of geocovariates (space, pop, ndvi and emissions)

### --> add emissions when have covariates

```{r}

# BC
bc_scores_mm0 <- cbind(mm[site_loc_vars],
                   bc_scores_space$mm,
                   bc_scores_pop$mm,
                   
                   bc_scores_ndvi$mm #, 
                   # bc_scores_ec$mm
                   ) %>%
  mutate(Pollutant = "bc")

bc_scores_cohort0 <- cbind(cohort[site_loc_vars],
                   bc_scores_space$cohort,
                   bc_scores_pop$cohort,
                   
                   bc_scores_ndvi$cohort #, 
                   # bc_scores_ec$cohort
                   ) %>% 
  mutate(Pollutant = "bc")

bc_scores_grid0 <- cbind(grid[site_loc_vars],
                   bc_scores_space$grid,
                   bc_scores_pop$grid,
                   
                   bc_scores_ndvi$grid #, 
                   # bc_scores_ec$grid
                   )%>% 
  mutate(Pollutant = "bc")

bc_scores_aqs0 <- cbind(aqs[site_loc_vars],
                   bc_scores_space$aqs,
                   bc_scores_pop$aqs,

                   bc_scores_ndvi$aqs #,
                   # bc_scores_ec$aqs
                   )%>%
  mutate(Pollutant = "bc")

 
# UFP
ufp_scores_mm0 <- cbind(mm[site_loc_vars],
                   ufp_scores_space$mm,
                   ufp_scores_pop$mm,
                   
                   ufp_scores_ndvi$mm #, 
                   # ufp_scores_ec$mm
                   ) %>% 
  mutate(Pollutant = "ufp")

ufp_scores_cohort0 <- cbind(cohort[site_loc_vars],
                   ufp_scores_space$cohort,
                   ufp_scores_pop$cohort,
                   
                   ufp_scores_ndvi$cohort #, 
                   # ufp_scores_ec$cohort
                   ) %>% 
  mutate(Pollutant = "ufp")

ufp_scores_grid0 <- cbind(grid[site_loc_vars],
                   ufp_scores_space$grid,
                   ufp_scores_pop$grid,
                   
                   ufp_scores_ndvi$grid #, 
                   # ufp_scores_ec$grid
                   ) %>% 
  mutate(Pollutant = "ufp")

ufp_scores_aqs0 <- cbind(aqs[site_loc_vars],
                   ufp_scores_space$aqs,
                   ufp_scores_pop$aqs,

                   ufp_scores_ndvi$aqs #,
                   # ufp_scores_ec$aqs
                   ) %>%
  mutate(Pollutant = "ufp")




# combine before doing interpollation

mm_scores0 <- rbind(bc_scores_mm0, ufp_scores_mm0)
cohort_scores0 <- rbind(bc_scores_cohort0, ufp_scores_cohort0)
grid_scores0 <- rbind(bc_scores_grid0, ufp_scores_grid0)
aqs_scores0 <- rbind(bc_scores_aqs0, ufp_scores_aqs0)

```




Plots of TVCs over time.

* Population PLS scores generally increase over time (increasing population density?)

  - the shape of the density curves is very similar. Using a simple trend could generally capture changes over time?   
  - The largest difference could be in 2010 where we see a few locations with very high values.

* NDVI ______    
* EC emissions ______

```{r}
cohort_scores0 %>%
  gather("TVC", "value", c(paste0("pop", census_data_yrs), paste0("ndvi", ndvi_data_yrs)) ) %>%
  separate(col = TVC, 
           into = c("TVC", "Year"), 
           sep = "(?<=[A-Za-z])(?=[0-9])" 
           ) %>%
  mutate(TVC = toupper(TVC)) %>%
  
  #View()
  
  group_by(Pollutant, TVC, Year) %>%
  distribution.table(var.string = "value") %>%
  kable(caption = "Distribution of PLS scores for the cohort, by pollutant and TVC") %>%
  kable_styling()
```

```{r}

cohort_scores0 %>%
  #            --> update when get new emission covariates
  gather("tvc", "value", starts_with(c("pop", "ndvi", "ec"))) %>%
  mutate(
    Year = substr(tvc, nchar(tvc)-3, nchar(tvc)),
    tvc = substr(tvc, 1, nchar(tvc)-4),
    
    #rename TVCs - for plotting
    tvc = recode_factor(factor(tvc),
                        "pop" = "Population Density",
                        "ndvi" = "NDVI"
                        )
  ) %>%
  #View()
  
  ggplot(aes(x=value, fill=Year)) + 
  geom_density(alpha=0.3) + 
  facet_grid(Pollutant~tvc, scales="free") + 
  labs(
    title = "PLS scores for TVCs at cohort locations",
    x = "PLS score"
    #subtitle = ""  
  ) +
  scale_fill_brewer(palette = "Spectral")  


```

* the best fit line is not parallel to the 1-1 line. In general, higher density areas had a greater increase in population density over time than lower density areas.
* Some sites deviate from the general trend more than others.    


TVCs will characterize temporal site changes better than our trend adjustment at sites where the change in population density between 1990 and 2010 differs from what would be our trend adjustment (i.e., a parallel line above the 1-1 line)

###--> ? see strange pattern. Amanda says it is due to transformation issues in Tacoma area.

```{r}
 cohort_scores0 %>%
  gather(key = "Census", value = "Value", contains("pop"), -contains("90")) %>%
  mutate(
    Census = str_extract(Census, "\\d{4}"),
    Pollutant = toupper(Pollutant)
  ) %>%
  ggplot(aes(x=pop1990, y=Value, col=Census)) + 
  geom_point(alpha=0.02) + 
  #geom_smooth(se=F) + 
  geom_smooth(method = "lm", aes(linetype="lm") ) +  #
  geom_abline(aes(slope = 1, intercept = 0, linetype= "1-1")) +
  facet_grid(Census~Pollutant) + 
  labs(
    title = "Cohort population density PLS scores over time",
    x = "1990 Census",
    y = "PLS Score",
    linetype = ""
  )


```

Locations with high residuals.

```{r}
#list to store model results
pop_m1 <- list()

for(i in seq_along(pollutants)) {
  
  #save model results
  pop_m1[i] <- cohort_scores0 %>%
  #filter(Pollutant == pollutants[1]) %>%
  lm(pop2010~pop1990, data = ., subset = Pollutant == pollutants[i]) %>%
  list()
  
  # rename saved models
  names(pop_m1)[i] <- pollutants[i]

}

```

```{r}
high_quant <- 0.99

```

```{r}
#use model to predict & estimate residuals
cohort_scores0 %>%
  mutate(
    #lm_prediction = ifelse(Pollutant == "bc", predict(pop_m1$bc), predict(pop_m1$ufp)),
    lm_residual = ifelse(Pollutant == "bc", residuals(pop_m1$bc), residuals(pop_m1$ufp))
  ) %>%
  
  #View()
  
  gather(key = "Census", value = "Value", contains("pop"), -contains("90")) %>%
  mutate(
    Census = str_extract(Census, "\\d{4}"),
    Pollutant = toupper(Pollutant),
    #indicator for high residual
    high_residual = lm_residual < quantile(lm_residual, 1-high_quant) | 
           lm_residual > quantile(lm_residual, high_quant)
  ) %>%
  
  
  ggplot(aes(x=pop1990, y=Value, col=lm_residual)) + 
  geom_point(aes(shape=high_residual),
    #alpha=0.5, 
    ) + 
  #geom_smooth(se=F) + 
  geom_smooth(method = "lm", aes(linetype="lm") ) +  #
  geom_abline(aes(slope = 1, intercept = 0, linetype= "1-1")) +
  facet_grid(Census~Pollutant) + 
  labs(
    title = "Cohort population density PLS scores over time",
    subtitle = paste0("high residual = < ",1-high_quant, " or > ", high_quant, " quantile"),
    x = "1990 Census",
    y = "PLS Score",
    linetype = ""
  ) + 
  scale_color_gradient2()
  

```

Map of places with large residuals (<`r 1-high_quant` or > `r high_quant` quantile).

In the BC model, places with higher than expected population densities in 2010 concentrations based on 1990 concentrations were locations:

* downtown   
* in Everrett east of Paine Field airport 
* in/near Mill Creek 

```{r}

# --> only do this for BC since maps look very similar to as UFP
cohort_scores_high_residual <- cohort_scores0 %>%
  filter(Pollutant == "bc") %>%
  mutate(
    #lm_prediction = ifelse(Pollutant == "bc", predict(pop_m1$bc), predict(pop_m1$ufp)),
    lm_residual = ifelse(Pollutant == "bc", residuals(pop_m1$bc), residuals(pop_m1$ufp)),
    lm_residual = as.double(lm_residual)
    
  ) %>%
  filter(lm_residual < quantile(lm_residual, 1-high_quant) | 
           lm_residual > quantile(lm_residual, high_quant)
         ) 

map0 +
  geom_polygon(data = study_area,
                aes(x = long, y = lat, group = group, fill = "Study Area"),
                alpha = 0.1) +
  geom_polygon(data = monitoring_area,
                aes(x = long, y = lat, group = group, fill = "Monitoring Area"),
                alpha = 0.1) +
  geom_point(data = cohort_scores_high_residual,
                 aes(x = longitude, y = latitude, col = lm_residual),
                 #alpha=0.3
                 ) + 
  scale_color_gradient2(name = "lm residual",
                        ) +
  labs(
    title = "Cohort sites with large residuals (using the BC model)",
    fill = "",
    col = ""
    ) 

```



## Inter- and intra-polation of PLS scores over time

Use linear inter- and intra-pollation to estimate PLS scores for otherwise unavailable years: 

* Population density: 

  - at the individual level, using decenial US Census block group information: interpolate 1991-1999 (using 1990 and 2000 linear fit) and 2001-2009 (using 2000 and 2010 linear fit)    
  - at the metropolitan area level, using annual US Census ACS survey (since 2020 decenail Census survey is not yet available): inter- and intral-polate 2011-2019 (using 2010-2018 linear fit).    
    - Note: unlike adjusting at the individual level, this approach assumes that all places increased in population density the same degree

* NDVI: use the same NDVI for 1990-1993 (NDVI estimated using images from 1990-1993), 1994-2005 (using 1990-1993 and 2006 linear fit), 2007-2009 (using 2006-2010 linear fit), and ??? __the same NDVI for 2010-2019 (since we are unsure of the temporal trend post 2010)__  

* emissions: use linear interpolation from 5 yr estimates to estimate missing years (e.g., use 1990-1995 fit to estimate 1991-1994).
 

- Population density


```{r}
# returns pop interpollated values for unavailable years 

interpolate_pop <- function(dt) {
  
  dt <- dt %>%
    mutate(
      # calculate yearly change
      pop1990_2000_slope = (pop2000 - pop1990)/10,
      pop2000_2010_slope = (pop2010 - pop2000)/10
    ) #%>% cbind(interpolated_df0)
  
  # use 1990-2000 individual-level fit to interpollate 
  for(i in 1:9){
    #i=1
    yr <- 1990+i
    
    var <- paste0("pop", yr)
    dt[var] <- dt$pop1990 + dt$pop1990_2000_slope*i
  }
  
  # use 2000-2010 individual-level fit to interpollate  
  for(i in c(1:9)){
    #i=1
    yr <- 2000+i
    
    var <- paste0("pop", yr)
    dt[var] <- dt$pop2000 + dt$pop2000_2010_slope*i
  }
  
  # use 2000-2018 metro area-level fit to inter- and intra-pollate 
  for(i in c(1:9)){
    #i=1
    yr <- 2010+i
    
    var <- paste0("pop", yr)
    # add the same percent annual increase as the Sea-tac-bell area
                            # convert from proportion change/yr to linear combination units. since the proportion slope is positive, make sure the linear combination units being added also are
    dt[var] <- dt$pop2010 + (abs(dt$pop2010)* pop_2010_2018_prop*i)
  }
  
  return(dt)
  
}
```


```{r}
# BC
bc_mm_scores <- interpolate_pop(dt = bc_scores_mm0) %>%
  mutate(Pollutant = "bc",
         # add observed concentration
         value = mm[["bc"]]) %>%
  select(Pollutant, value, everything())
  
bc_cohort_scores <- interpolate_pop(dt = bc_scores_cohort0)%>%
  mutate(Pollutant = "bc",
         #no obsrved concentration, but need this column for modeling later
         value = NA) %>%
  select(Pollutant, value, everything())

bc_grid_scores <- interpolate_pop(dt = bc_scores_grid0)%>%
  mutate(Pollutant = "bc",
         value = NA) %>%
  select(Pollutant, value, everything())

bc_aqs_scores <- interpolate_pop(dt = bc_scores_aqs0)%>%
  mutate(Pollutant = "bc",
         value = NA) %>%
  select(Pollutant, value, everything())

# UFP
ufp_mm_scores <- interpolate_pop(dt = ufp_scores_mm0) %>%
  # add observed pollutant concentration
  mutate(Pollutant = "ufp",
         value = mm[["ufp"]]) %>%
  select(Pollutant,value, everything())

ufp_cohort_scores <- interpolate_pop(dt = ufp_scores_cohort0)%>%
  mutate(Pollutant = "ufp",
         value = NA) %>%
  select(Pollutant, value, everything())

ufp_grid_scores <- interpolate_pop(dt = ufp_scores_grid0) %>%
  mutate(Pollutant = "ufp",
         value = NA) %>%
  select(Pollutant, value, everything())

ufp_aqs_scores <- interpolate_pop(dt = ufp_scores_aqs0) %>%
  mutate(Pollutant = "ufp",
         value = NA) %>%
  select(Pollutant, value, everything())

## combine pollutants
mm_scores <- rbind(bc_mm_scores, ufp_mm_scores)
cohort_scores <- rbind(bc_cohort_scores, ufp_cohort_scores)
grid_scores <- rbind(bc_grid_scores, ufp_grid_scores)
aqs_scores <- rbind(bc_aqs_scores, ufp_aqs_scores) %>%
  #drop aqs sites w/o geocovariats. no aqs sites have "value" estimates
  drop_na(-value) 

```


- NDVI

```{r}
# --> make fn for interpolation 
# paste0("ndvi", c(1994:2005, 2007:2009, 2011:2019))


```

- Emissions

```{r}
# --> make fn for interpolation 
# paste0("ec", c(1991:1994, 1996:1999, 2001:2004, 2006:2009, 2011:2014, 2016:2019)) 



```

Plots show a steady change in TVC values over time. 

for **population density**, values generally increase from 1990-2000. Values are more dispersed in 2010 than 2000, hence why we see increases in value over time.

### --> fn to automatically name pollutants in tables; figures w/ units

PLS distribution:

-table 

```{r}
#           sep = "(?<=[A-Za-z])(?=[0-9])"  

cohort_scores %>%
  gather("TVC", "value", c(paste0("pop", c(census_data_yrs, last_acs_yr)), paste0("ndvi", ndvi_data_yrs)) ) %>%
  separate(col = TVC, 
           into = c("TVC", "Year"), 
           sep = "(?<=[A-Za-z])(?=[0-9])" 
           ) %>%
  mutate(TVC = toupper(TVC)) %>%
  
  #View()
  
  group_by(Pollutant, TVC, Year) %>%
  distribution.table(var.string = "value") %>%
  kable(caption = "Distribution of PLS scores for the cohort, by pollutant and TVC") %>%
  kable_styling()
  
```

-plot 

```{r}
cohort_scores %>%
  gather("tvc", "value", contains(c("pop", "ndvi")), -contains("slope")) %>%
  separate(col = tvc, 
           into = c("tvc", "Year"), 
           #separate text from numbers: position following letters; 
           sep = "(?<=[A-Za-z])(?=[0-9])" ) %>%
  mutate(
    Year = as.numeric(Year),
    estimate_type = ifelse((Year %in% census_data_yrs & grepl("pop", tvc) ) |
      (Year %in% ndvi_data_yrs & grepl("ndvi", tvc)), # | (Year %in% emissions_data_yrs & grepl("ec", tvc))
      "observed", "interpolated")
  ) %>%

  #View()
  
ggplot(aes(x=Year, y=value, )) + 
  geom_boxplot(aes(fill=estimate_type, group=Year), alpha=0.8) + 
  geom_smooth(se=F) +
  facet_grid(Pollutant~tvc, scales="free") + 
  labs(
    title = "Distribution of observed and interpolated TVC PLS scores for the cohort"
  ) + 
  coord_flip()
  
  
```

```{r}
site_sample_n <- 150

cohort_scores %>%
  gather("Year", "Value", paste0("pop", c(census_data_yrs, last_acs_yr))
         
         #contains("pop")
         
         ) %>%
  mutate(
    Year = as.numeric(str_extract(Year, "\\d{4}"))
  ) %>%
  
  #View()
  #only plot a subsample of sites
  filter(site_id %in% sample(site_id, size = site_sample_n, replace = F)) %>%
  #View()
  
  ggplot(aes(x=Year, y= Value, group=site_id)) + 
  geom_point(alpha=0.2) + 
  geom_line(alpha=0.2) + 
  
  facet_wrap(~Pollutant) + 
  scale_x_continuous(breaks = c(census_data_yrs, last_acs_yr)) +
  labs(
    title = paste0("Population density PLS scores over time for a sample of cohort sites ", "(n = ",site_sample_n, ")"),
    y = "PLS Score"
  )  

```


Map TVC Surfaces

```{r}
# map visualization ideas
# https://gist.github.com/jebyrnes/f3f626aa24f565d78003f05aab4d0372 

```

```{r, fig.height=8}
# FN returns list of mapped PLS scores for particular TVC variables & years

# dt <- grid_scores
# yrs <- mapping_yrs
# vars <- paste0("pop",  yrs)

map_scores <- function(
  #data
  dt,
  # years to plot
  yrs,
  #PLS variables to plot
  vars, 
  legend_title = ""
  ) {
  
  vars <- paste0(vars,  yrs)
  
  #if no label is given, automatically add this label
  if(legend_title == "") {
    legend_title <- "PLS\nScore"
  }
  
  
  plot_range <- data.frame(
    min = min(dt[vars]),
    max = max(dt[vars])
  )
  
  plots_list <- list()
  
  for(i in seq_along(yrs)) {
    #i=1
  
    df <- dt %>%  
    #select that yrs variable
    rename(new_var = vars[i])  
    
    p <- map0 + 
      geom_point(data = df, aes(x=longitude, y=latitude, col = new_var),
                 alpha=0.5
                 ) +
      scale_color_gradient(low = "yellow", high = "red",
                          #make all legends have the same range
                          limits = c(plot_range$min, plot_range$max)) +
    labs(
       title = paste0(yrs[i]),  
      col = legend_title #paste0("Population\nDensity") #"\n ", "\n ", "\n "
      ) 
   
    plots_list[i] <- list(p)
  
  }
  
  return(plots_list)

}

```

PLS score maps.

Note: PLS scores are composed of both small and large population buffers, thus these are indicative of population nearby. 

Not plotting these maps for smaller buffers alone since these are only available for specific years (e.g., 1990, 2000, 2010 population Census)

Higher PLS scores are indicative of higher population densities (population density is positively associated with TRAP).

```{r}
# Pop
map_scores(dt = grid_scores, yrs = mapping_yrs, vars = "pop") %>%
  ggarrange(plotlist = .,
          nrow = 1,
          common.legend = T, legend = "right") %>%
  annotate_figure(top = "Population density" ) 

```

Higher NDVI PLS scores are indicative of lower raw NDVI values (NDVI is negatively associated with TRAP). We see the highest NDVI PLS scores (lowest raw NDVI values) near seattle, and lower PLS scores east of Seattle and I-5.

```{r}

# NDVI
map_scores(dt = grid_scores, yrs = 2006, vars = c("ndvi")) %>%
  ggarrange(plotlist = .,
          nrow = 1,
          common.legend = T, legend = "right"
          ) %>%
  annotate_figure(top = "NDVI" ) 


# --> add EC

```

PLS score differences maps.

* most areas have seen a similar change in population density over time.   
* Some areas have seen greater changes in population density than others. The Seattle area, for example, has seen larger increases in population density than other surrounding places. These are the places that will benefit from using time-varying covariates rather than just using a trend adjustment.   


```{r}
grid_scores %>%
  mutate(pop_diff1995_2019 = pop2019 - pop1995) %>%
  map_scores(dt = ., yrs = c("1995_2019"), vars = "pop_diff", 
             legend_title = paste0("PLS Score\nDifference")
             ) %>%
  ggarrange(plotlist = .,
          nrow = 1,
          common.legend = T, legend = "right") %>%
  annotate_figure(top = "Change in population density over time") 


## --> repeat for NDIV, EC

```

# UK Models

**Correlation between UFP and BC**

Since we will be using BC emission for UFP model. 

```{r}
high_quant <- 0.97
```

```{r}
model1 <- "ufp~bc"
ufp_bc <- lm(as.formula(model1), data = mm)
round_digits <- 2
lm_fit <- paste0("y = ", round(ufp_bc$coefficients[1], round_digits), " + ", round(ufp_bc$coefficients[2], round_digits), "X" )
lm_r2 <-  round(summary(ufp_bc)$r.squared, round_digits)
rmse <- rmse(obs = exp(mm$ufp), pred = exp(predict(ufp_bc))) %>% round()

fit.info <- paste0(lm_fit,
                   "\nR2 = ", lm_r2,  
                   "\nRMSE = ", rmse,
                   "\nNo. Pairs = ", nrow(mm))


```

```{r, results="asis"}
# show equation in Rmd render

## --> error installing/using package
#install.packages("texPreview", dependencies = T)
#library(texPreview) #%>% texPreview::preview()

equatiomatic::extract_eq(ufp_bc) #%>% texPreview::preview()

#equatiomatic::extract_eq(ufp_bc, use_coefs=T)

 
```

Basic plot

* UFP and BC appear to be moderately positively correlated. This suggests taht BC may serve as a decent surrogate for UFP at many locations, though less well at locations furthest away from the best fit line.


```{r}
mm %>%
  ggplot(aes(x=bc, y=ufp)) + 
  geom_point(alpha=0.5) + 
  geom_smooth(method = "lm", se=F, aes(linetype = "lm")) + 
  labs(
    x= "ln BC (ln ng/m3)",
    y = "ln UFP (ln pt/cm3)",
    title = "Comparison of annual average BC and UFP in mobile monitoring campaign",
    caption = paste0("R2 and RMSE are based on OLS for ", model1, "model. RMSE is calculated on the native scale."),
    linetype = ""
  ) +
  #annotate("text", -Inf, Inf, label = fit.info, hjust = 0, vjust = 1) + 
  annotate("text", -Inf, Inf, label = fit.info, hjust = 0, vjust = 1) + 
  
  scale_color_gradient2()
```

Plot highlighting residuals.

```{r}
mm %>%
  mutate(
    lm_residual = residuals(ufp_bc),
    high_residual = lm_residual <= quantile(lm_residual, 1-high_quant) |
           lm_residual >= quantile(lm_residual, high_quant)
  ) %>%
  
  #View()
  
  ggplot(aes(x=bc, y=ufp)) + 
  geom_point(alpha=0.7, aes(col=lm_residual, shape=high_residual)) + 
  #geom_smooth(se=F) + 
  geom_smooth(method = "lm", se=F, aes(linetype = "lm")) + 
  labs(
    x= "ln BC (ln ng/m3)",
    y = "ln UFP (ln pt/cm3)",
    title = "Comparison of annual average BC and UFP in mobile monitoring campaign",
    subtitle = paste0("high residual: value <=", 1-high_quant, " or > ", high_quant, " quantile"),
    caption = paste0("R2 and RMSE are based on OLS for ", model1, "model. RMSE is calculated on the native scale."),
    shape = "high residual",
    col = "lm residual",
    linetype = ""
  ) +
  annotate("text", -Inf, Inf, label = fit.info, hjust = 0, vjust = 1) + 
  scale_color_gradient2()
  

```

Map of where high residuals are found. 

* When we map the high residuals, we see that many of these are near airports.   
* When we map the low residuals (higher BC levels than expected), most are near rialroad tracks

```{r}
ufp_bc_lm_high_residuals <- mm %>%
  mutate(
    lm_residual = residuals(ufp_bc)
  ) %>%
  filter(lm_residual <= quantile(lm_residual, 1-high_quant) |
           lm_residual >= quantile(lm_residual, high_quant)
           )

```

```{r, fig.height=8}
map0 +
  geom_polygon(data = study_area,
                aes(x = long, y = lat, group = group, fill = "Study Area"),
                alpha = 0.1) +
  geom_polygon(data = monitoring_area,
                aes(x = long, y = lat, group = group, fill = "Monitoring Area"),
                alpha = 0.1) +
  #airport area
  geom_polygon(data = airports_area, #aviation_area, #airports_area
                aes(x = long, y = lat, group = group, 
                    fill = "Airport"
                    #fill = "Aviation zones"
                    ),
                #alpha = 0.3
               ) +
      geom_point(data = ufp_bc_lm_high_residuals,
                 aes(x = longitude, y = latitude, col = lm_residual),
                 #alpha=0.3
                 ) + 
  scale_color_gradient2(name = "lm residual") +
  labs(
    title = "Cohort sites with large UFP~BC model residuals",
    fill = "",
    col = ""
    ) 

```


```{r}
# # datasets for modeling
# bc_mm_scores  #mm_scores  
# bc_cohort_scores 
# bc_grid_scores 
# bc_aqs_scores
# 
# # UFP
# ufp_mm_scores 
# ufp_cohort_scores 
# ufp_grid_scores 
# ufp_aqs_scores
```

## CV RMSE and R2 for the 2019 surface

### --> ? parallel code? 

```{r}

uk_predictors. <- c(paste0("space", c(1:n_components)), "pop", "ndvi"#, "ec"
                                                  )

# FN returns CV predictions from fitting a UK model. This fn is similar to pls_uk_cv_predictions() in A2.0.1_Var&Fns.R, but it does not fit PLS since we have already determined these parameters in Aim 2. It can be used to predict within the same dataset (10 FCV - generates training/test sets) or to predict at new locations (uses MM stops IDs to find the training dataset).

# #performing CV 
# #dt = mm_scores %>% filter(grepl("BC", Pollutant))
# #perform_cv = TRUE
# 
# # predict at other locations
# dt = mm_cohort_scores %>% filter(grepl("BC", Pollutant))
# perform_cv = FALSE
# 
# y_name <- "value"
# uk_predictors = c(paste0("space", c(1:3)), "pop2019", "ndvi2006")
# dist_fract. = 0.1
 
uk_predictions <- function(dt,
                              y_name = "value",
                              #whether to perform CV within the same dataset. If false, identifies MM stops based on id and makes those the training set 
                              perform_cv = TRUE,
                              uk_predictors = uk_predictors.
                                                            ) {  
  
  dt <- dt %>% rename(y_name = y_name)  
  
  # variogram maximum distance fraction to model, from Aim 2
  dist_fract. <- ifelse(grepl("bc", unique(dt$Pollutant)), 
                        a2_cv_results$Variogram_Distance_Fraction[str_detect(a2_cv_results$Pollutant, "BC")],
                        a2_cv_results$Variogram_Distance_Fraction[str_detect(a2_cv_results$Pollutant, "UFP")]
                        
                        )
  
  
  if(perform_cv == TRUE) {
    # 10 FCV
    k <- 10
    
    dt <- dt %>%
    #create training/test set within a dataset
    mutate(set = sample(c(1:k), size = nrow(.), replace = T),
           # to save predictions
           uk_prediction = NA)
  }
  
  if(perform_cv == FALSE) {
    # only need to build a model w/ training data once
    k <-1
  }
  
  for(f in seq_len(k)) {
    #f=1
    
    ################################ create training/test sets ################################
    if(perform_cv == TRUE) { 
      #create test/training sets if conducting CV
      train_grp <- dt$set != f
    }
    
    if(perform_cv == FALSE) { 
      #ID non-aqs site MM stops as the training data
      train_grp <- grepl("MS", dt$site_id) #"MS|MC"
    }
    
    dt_train <- dt %>% filter(train_grp)  
    dt_test <- dt %>% filter(!train_grp) %>%
      #drop here, otherwise rows will be dropped when using as.geodata() if there are NAs
      select(-y_name)
    
    ################################ geodatasets ################################
    geo_train <- as.geodata(dt_train, 
                            coords.col = c("lambert_x", "lambert_y"), 
                            data.col = "y_name", 
                            covar.col = uk_predictors)
    geo_test <- as.geodata(dt_test, 
                           coords.col = c("lambert_x", "lambert_y"), 
                           covar.col = uk_predictors)
    
    ##trend
    cov_trend <-  as.formula(paste0("~ ", paste0(uk_predictors,  collapse = " + " )))
    
    max.dist <- summary(geo_train)$distances.summary[["max"]]
    
    # --> ? select this variogram parameter through CV??
    max.plot.dist <- max.dist*dist_fract. #[dist_fract_index] 
    
    ############################ model residuals ###################################### 
    ##Empirical Variogram
    brk_pt <- 1000
    by1_pt <- 300
    by2_pt <- 1000
    
    variog_train <- variog(geo_train,
                           #plotting breakpoints 
                           uvec=c(seq(0, brk_pt, by = by1_pt), seq((brk_pt + by2_pt), max.plot.dist, by= by2_pt)),
                           #UK
                           trend = cov_trend, 
                           messages = F)
    
    #use geoR to try to estimate intitial range & sill values. using WLS and an exponential fit
    wls_ests_train <- variofit(variog_train, cov.model = "exp", 
                               messages = F)
    
    # --> ? select this variogram parameter through CV??
    #don't need initial values above since estimates seem to be the same w/ or w/o ini = wls_ests_train (based on small sample)?
    resid_model_train <- variofit(vario = variog_train, 
                                  ini = wls_ests_train, 
                                  cov.model = "exp",
                                  weights = "npairs",#wls
                                  ) 
    
    
    # if fitting a model to the entire MM dataset (i.e., not performing CV), save residual model parameters
    if(perform_cv == FALSE) {
      
      # partial sill: sigma sq # range: phi, # nugget: tau sq
      resid_model.s <- summary(resid_model_train)
      
      ## residual model parameters
      residual_model_table <- data.frame(
        Partial_Sill = resid_model.s$estimated.pars[["sigmasq"]],
        Range_m = resid_model.s$estimated.pars[["phi"]],
        Nugget = resid_model.s$estimated.pars[["tausq"]])
      } 
    
    #trend
    train_trend <- trend.spatial(trend = cov_trend, geo_train)
    test_trend <- trend.spatial(trend = cov_trend, geo_test)
    
    ############################# Use UK to predict #############################
    kc_cv <- krige.conv(geo_train,
                        # where you want to predict
                        locations = geo_test$coords,
                        krige = krige.control(type = "ok",
                                                          # range, nugget, partial sill
                                              obj.model = resid_model_train, 
                                              trend.d = train_trend,
                                              trend.l = test_trend))
    
    #save CV predictions
    dt$uk_prediction[!train_grp] <- kc_cv$predict
    
  }

  
  ############################ return results ############################
  if(perform_cv == TRUE) {
    result <- list(data=dt)
    }
  
  if(perform_cv == FALSE) {
    result <- list(data=dt,
                   # include additional information on residual model
                   residual_model = resid_model_train,
                   residual_model_table = residual_model_table,
                   variogram = variog_train
                   )
  }
  
  return(result)
  
  }

```

```{r, results="hide"}
# UK model will be built using MM stops & most recent TVCs
mm_scores_2019 <- mm_scores %>%
  select(Pollutant:lambert_y,
         contains("space"),
         #select TVCs for modeling
         pop = pop2019,
         ndvi = ndvi2006,
         #ec = ec2019
         )

cv_results <- data.frame(
  Pollutant = pollutants,
  RMSE = NA,
  R2 = NA
)

# not sure why have to repeate this here. otherwise, doesn't recognize rmse()
source("0.Global_Fns.R")


for (i in seq_along(pollutants)) {
  #i=1
  mm_cv <- mm_scores_2019 %>% 
    filter(grepl(pollutants[i], Pollutant)) %>%
    uk_predictions(dt = ., 
                   y_name = "value",  
                   uk_predictors = uk_predictors.,
                   perform_cv = TRUE)
  
  cv_results$RMSE[cv_results$Pollutant == pollutants[i]] <- rmse(obs = exp(mm_cv$data$y_name), pred = exp(mm_cv$data$uk_prediction)) %>% round()
  cv_results$R2[cv_results$Pollutant == pollutants[i]] <- r2_mse_based(obs = exp(mm_cv$data$y_name), pred = exp(mm_cv$data$uk_prediction)) %>% round(2)
  
  }

```


```{r}
a2_cv_results1 <- a2_cv_results %>%
  mutate(
    Pollutant = ifelse(grepl("bc", Pollutant, ignore.case = T), "bc", "ufp")
  ) %>%
  select(Pollutant, A2_RMSE = RMSE, A2_R2 = R2)

cv_results %>%
  left_join(a2_cv_results1) %>%
  mutate(Pollutant = toupper(Pollutant)) %>%
  kable(caption = "Cross-validated RMSE and MSE-based R2 estimates based on the 1-1 line. Using 2019 surfaces. Estimates are compared to those obtained in the UK model from Aim 2") %>%
  kable_styling()
```

## UK model fitting & prediction at cohort & grid locations  

```{r}
# FN returns UK predictions for new locations (e.g., cohort, grid) and times after fitting a UK model to MM observations in 2019 

# dt_for_prediction = aqs_scores
# dt_mm2019 = mm_scores_2019
# #only need to predict when AQS sites start measuring BC (2003+)
# prediction_yrs = sort(unique(bc_validation$Year))
# pollutants. = pollutants
# uk_predictors.. = uk_predictors.

predict_over_time <- function(
  dt_for_prediction,  
  dt_mm2019 = mm_scores_2019,
  prediction_yrs = c(seq(first_p_yr, last_acs_yr, by=5), 2019),  
  pollutants. = pollutants,
  uk_predictors.. = uk_predictors.
  ) {
  
  dt <- dt_for_prediction %>%
    select(Pollutant, site_id, latitude, longitude) %>%
    #for merging later since AQS sites are strings
    mutate(site_id = as.character(site_id))
    
    
  for(y in seq_along(prediction_yrs)) {
  #y=1   
  
  dt0 <- dt_for_prediction %>%
    #select & rename TVCs for prediction
    select(Pollutant:lambert_y,
           contains("space"),
           #rename TVCs
           pop = paste0("pop", prediction_yrs[y]),
           
           ### --> update these when get new covarites
           ndvi = paste0("ndvi", "2006"),
           
           #ec = ec2019
           
           ) %>%
    #attach modeling data 
    rbind(dt_mm2019)
    
  
  predictions_df <- data.frame()
  
  for(i in seq_along(pollutants.)) {
    #i=1
    
    # list w/ UK results
    dt_list <- dt0 %>% 
      filter(grepl(pollutants.[i], Pollutant)) %>%
      uk_predictions(dt = ., y_name = "value", perform_cv = FALSE, 
                     uk_predictors = uk_predictors..)
     
    # pull the df out of the list
    predictions_df0 <- dt_list$data %>% 
      select(Pollutant, site_id, uk_prediction) %>%
      #drop MM stops, which don't have predictions & to match dt_for_prediction rows
      filter(!str_detect(site_id, "MS")) %>%
      mutate(#site_id = as.numeric(site_id),
             #convert prediction back to the native scale
             uk_prediction = exp(uk_prediction)
             )
    
    predictions_df <- rbind(predictions_df, predictions_df0)
    
    # include the residual table & variogram from the last models fit to each pollutant. They should all be very similar since all models are fit to the same MM data alone, but they predict at different locations/times
    if(pollutants.[i] == "bc" & y==length(prediction_yrs)) {
      
      bc_residual_model <- dt_list$residual_model
      bc_residual_model_table <- dt_list$residual_model_table %>%
        mutate(Pollutant = "BC")
      
      bc_variogram  <- dt_list$variogram 
      
    }
    
    if(pollutants.[i] == "ufp" & y==length(prediction_yrs)) {
      
      ufp_residual_model <- dt_list$residual_model
      ufp_residual_model_table <- dt_list$residual_model_table %>%
        mutate(Pollutant = "UFP")
      
      ufp_variogram <- dt_list$variogram
      
    }
     
  }
  
  names(predictions_df)[names(predictions_df) == "uk_prediction"] <- paste0("uk_", prediction_yrs[y])
  
  #save predictions 
  dt <- dt %>% left_join(predictions_df)
  
  }

  # combine residual model parameters for pollutants
  residual_model_table <- rbind(bc_residual_model_table,
                                   ufp_residual_model_table) %>%
    select(Pollutant, everything())
  
  result <- list(
    #predictions for all location-years
    dt = dt,
    
    #UK residual model parameters
    bc_residual_model = bc_residual_model,
    ufp_residual_model = ufp_residual_model,
    residual_model_table = residual_model_table,
    #variograms for each pollutant
    bc_variogram = bc_variogram,
    ufp_variogram = ufp_variogram
    )
  
  #return(dt)
  return(result)

}

```

#### --> UPDATE aqs sites

```{r, results="hide"}
# use UK to make perdictions

cohort_predictions <- predict_over_time(dt_for_prediction = cohort_scores,
                                        dt_mm2019 = mm_scores_2019,
                                        prediction_yrs = mapping_yrs 
                                        )

grid_predictions <- predict_over_time(dt_for_prediction = grid_scores,
                                        dt_mm2019 = mm_scores_2019,
                                      prediction_yrs = mapping_yrs 
                                        )
#for validation
aqs_predictions <- predict_over_time(dt_for_prediction = aqs_scores, 
                    dt_mm2019 = mm_scores_2019,
                    #only need to predict when AQS sites start measuring UFP, BC (2001, 2003+)
                    prediction_yrs = sort(unique(c(ufp_validation$Year, bc_validation$Year)))
                    )

```

Variogram plots

```{r}
# BC
plot(cohort_predictions$bc_variogram,
     main = paste0("Binned empirical and modeled variogram for BC model"),
     xlab = "Distance (m)")

lines(cohort_predictions$bc_variogram, lty=1)
lines(cohort_predictions$bc_residual_model, lty=2, col=2)
legend("bottomright",
       legend = c("Empirical", paste0("Residual Model")),
       lty=c(1:2), col = c(1:2))

```

### --> ? improve fit? 

```{r}
# UFP
plot(cohort_predictions$ufp_variogram,
     main = paste0("Binned empirical and modeled variogram for UFP model"),
     xlab = "Distance (m)")

lines(cohort_predictions$ufp_variogram, lty=1)
lines(cohort_predictions$ufp_residual_model, lty=2, col=2)
legend("bottomright",
       legend = c("Empirical", paste0("Residual Model")),
       lty=c(1:2), col = c(1:2))

```

Residual model parameters 

```{r}
cohort_predictions$residual_model_table  %>%
  kable(digits = 3, 
        col.names = c("Pollutant", "Partial Sill", "Range (m)", "Nugget"),
        caption = "UK residual model parameters for each pollutant"
        ) %>%
  kable_styling()

```


# Temporal trend adjustment

Conducting a multiplicative (rather than an additive) trend adjustment since EC, BC and UFP scales and/or units are different. 

### --> update other Rmd so get 2019 trend as well

```{r}
#trend_adjustment
#trend_yrs <- unique(trend_adjustment$Year)

#cohort
cohort_uk_trend_predictions <- cohort_predictions$dt %>%
  #rename_at(vars(contains("uk_")), ~str_replace(., "uk_", "uk_trend_")) %>%
  gather(Year, uk_p, contains("uk")) %>%
  mutate(Year = as.numeric(str_extract(Year, "\\d{4}"))) %>%
  #merge w/ trend adjustment
  left_join(trend_adjustment) %>%
  mutate(
    #adjust UK predictions using trend
    uk_trend_p = uk_p*ratio_adjustment
  )


#grid
grid_uk_trend_predictions <- grid_predictions$dt %>%
  gather(Year, uk_p, contains("uk")) %>%
  mutate(Year = as.numeric(str_extract(Year, "\\d{4}"))) %>%
  #merge w/ trend adjustment
  left_join(trend_adjustment) %>%
  mutate(
    #adjust UK predictions using trend
    uk_trend_p = uk_p*ratio_adjustment
  )

#aqs sites
aqs_uk_trend_predictions <- aqs_predictions$dt %>%
  gather(Year, uk_p, contains("uk")) %>%
  mutate(Year = as.numeric(str_extract(Year, "\\d{4}"))) %>%
  #merge w/ trend adjustment
  left_join(trend_adjustment) %>%
  mutate(
    #adjust UK predictions using trend
    uk_trend_p = uk_p*ratio_adjustment
  )
```

####--> relabel pollutants: capitalize, add units

Plots comparing predictions before and after the trend adjustment.

-density plot

TRAP predictions are very similar across years unless a trend adjustment is applied. 

The trend adjustment makes TRAP predictions more variable back in time.

```{r}
cohort_uk_trend_predictions %>%
  gather("trend", "prediction", contains("uk")) %>%
  mutate(
    Trend = ifelse(str_detect(trend, "trend"), "trend-adjusted", "non trend-adjusted")
  ) %>%
  #View()
  
  ggplot(aes(x=prediction, fill=Year, group=Year)) + 
  geom_density(alpha=0.3) + 
  facet_wrap(Trend~Pollutant, scales="free",
             #labeller = "label_both"
             ) +
  labs(
    title = "UFP and BC predictions before and after applying a trend adjustment"
  )
```

scatterplot.

for any given year, the sites most impacted by the multiplicataive trend adjustment are those with higher concentrations (these are further away from the 1-1 line).

```{r}
cohort_uk_trend_predictions %>%
  # relabel pollutants
  
  ggplot(aes(x=uk_p, y=uk_trend_p, col=Year)) + 
  geom_point(alpha=0.1) + 
  geom_abline(aes(slope = 1, intercept = 0, linetype = "1-1")) +
  facet_wrap(~Pollutant, scales="free") + 
  labs(
    title = "UFP and BC predictions before and after applying a trend adjustment",
    x = "Non-Adjusted",
    y = "Adjusted",
    linetype = ""
    
  )

```

-table

```{r}
cohort_uk_trend_predictions %>%
  drop_na(uk_trend_p) %>%
  gather("trend", "prediction", contains("uk")) %>%
  mutate(
    trend = ifelse(str_detect(trend, "trend"), "adjusted", "non-adjusted")
  ) %>%
  
  mutate(Year = as.factor(Year)) %>%
  group_by(Pollutant, Year, trend) %>%
  
  #--> relabel pollutants
  
  
  distribution.table(var.string = "prediction") %>%
  kable(caption = "Trend-adjusted pollutant predictions over time at cohort locations") %>%
  kable_styling()


```


# Validation - out of sample  

### --> ? don't include BH since we used EC trend? Fit is the best for BH

```{r}
#combine observation & prediction datasets for AQS sites
bc_valid_p <- bc_validation %>%
  #only keep predictions for site-years w/ AQS estimates
  left_join(aqs_uk_trend_predictions) %>%
  #drop sites w/o geocovariates...or UK predictions
  drop_na()

ufp_valid_p <- ufp_validation %>%
  left_join(aqs_uk_trend_predictions)

```

Site-years available for validation

```{r}
bc_valid_p %>%
  ggplot(aes(x=Year, y=Site, fill=Site)) + 
  stat_bin2d() + 
  labs(
    title = "Site-years used to validate BC model"
  )

```

### --> calc RMSE, R2.   e.g., Molter 2010 Table 2: Year, N AQS sites, RMSE, R2

-overall 

```{r}
#calculate RMSE and R2 (MSE-based) around the 1-1 line
bc_valid_overall <- bc_valid_p %>%
  dplyr::summarize(
    N = n(), #all site-years
    Years = paste(unique(Year), collapse = ", ")      ,
    Sites = paste(unique(Site), collapse = ", ")      ,
    RMSE = round(rmse(obs = mean_ng_m3, pred = uk_trend_p)),
    R2 = r2_mse_based(obs = mean_ng_m3, pred = uk_trend_p)
  ) 

bc_valid_overall %>%
  kable(#digits = 2, 
        caption = "Out-of-sample validation for all AQS sites and years with BC observations. RMSE and R2 (MSE-based) are calculated around the 1-1 line.", 
          ) %>%
  kable_styling()


```

-by site

* The BC model performed better (lower RMSE) at some sites than others

```{r}
bc_valid_by_site <- bc_valid_p %>%
  group_by(Site) %>%
  dplyr::summarize(
    N = n(),  
    Years = paste(unique(Year), collapse = ", ")      ,
    RMSE = round(rmse(obs = mean_ng_m3, pred = uk_trend_p)),
    R2 = r2_mse_based(obs = mean_ng_m3, pred = uk_trend_p)
  ) 

bc_valid_by_site %>%
  kable(digits = 2, 
        caption = "Out-of-sample validation results for the BC model by AQS site. N is the number of years avaialble for that site. RMSE and R2 (MSE-based) are calculated around the 1-1 line.", 
          ) %>%
  kable_styling()

```

```{r}
scale_val <- 600

bc_valid_by_site %>%
  mutate(R2 = R2*scale_val,
         #sorten site names
         Site = str_remove(Site, "Seattle ")
         ) %>%
  gather(key = Measure, value = value, RMSE, R2) %>%
  ggplot(aes(x=Site, y=value, shape=Measure, col=N )) + 
  geom_point() +
  geom_hline(aes(yintercept = bc_valid_overall$RMSE, linetype="RMSE")) +
  geom_hline(aes(yintercept = bc_valid_overall$R2*scale_val, linetype="R2")) +
  
  scale_y_continuous(sec.axis = sec_axis(~./scale_val, name = "R2")) + 
  labs(y = "RMSE (ng/m3)",
       title = "Out-of-sample validation results for the BC model by AQS site",
       linetype = "Overall",
       shape="Site",
       col = "No. Years"
       )
  
## fix overalpping labels
#scale_x_discrete(guide = guide_axis(n.dodge=2))+

```

-by year

* the BC model generally performs better (lower RMSE) during more recent years 

```{r}
bc_valid_by_yr <- bc_valid_p %>%
  group_by(Year) %>%
  dplyr::summarize(
    N = n(),  
    Sites = paste(unique(Site), collapse = ", ")      ,
    RMSE = round(rmse(obs = mean_ng_m3, pred = uk_trend_p)),
    R2 = r2_mse_based(obs = mean_ng_m3, pred = uk_trend_p)
  ) 

bc_valid_by_yr %>%
  kable(digits = 2, 
        caption = "Out-of-sample validation by year. N is the number of sites avaialble for that year. RMSE and R2 (MSE-based) are calculated around the 1-1 line.", 
          ) %>%
  kable_styling()
```

```{r}
scale_val <- 1e3

bc_valid_by_yr %>%
  mutate(R2 = R2*scale_val) %>%
  gather(key = Measure, value = value, RMSE, R2) %>%
  ggplot(aes(x=Year, y=value, shape=Measure, col=N)) + 
  geom_point() + 
  geom_smooth(se=F) +
  geom_hline(aes(yintercept = bc_valid_overall$RMSE, linetype="RMSE")) +
  geom_hline(aes(yintercept = bc_valid_overall$R2*scale_val, linetype="R2")) +
  scale_y_continuous(sec.axis = sec_axis(~./scale_val, name = "R2")) + 
  labs(y = "RMSE (ng/m3)",
       title = "Out-of-sample validation results for the BC model by year",
       linetype = "Overall",
       shape="Site",
       col = "No. Sites"
       )
  

```

## --> ? 

Scatter plots 

-overall

```{r}
# overall
bc_valid_p %>%
  colo.plot(data.wide = ., x.variable = "mean_ng_m3", x.label = "Observed",
          y.variable = "uk_trend_p", y.label = "Predicted",
          col.by = "Site", 
          alpha_value = 1, 
          mytitle = paste0("Comparison of ", toupper(bc_valid_p$Pollutant)[1], " Concentrations (ng/m3)")
          )

```

```{r}
# # by site
# validation_site_names <- unique(bc_valid_p$Site)
# plot_list <- list()
# 
# for(i in seq_along(validation_site_names)) {
#   #i=5
#   p <- bc_valid_p %>%
#   filter(Site == validation_site_names[i]) %>%
#     colo.plot(data.wide = ., x.variable = "mean_ng_m3", x.label = "Observed",
#           y.variable = "uk_trend_p", y.label = "Predicted",
#           col.by = "Year", 
#           alpha_value = 1, 
#           mysubtitle = validation_site_names[i]
#           )
#   
#   plot_list[i] <- list(p)
#   names(plot_list)[i] <- validation_site_names[i]
# 
# }
# 
# ggarrange(plotlist = plot_list) %>%
#   annotate_figure(top = "Comparison of observed and predicted BC concentrations (ng/m3)" ) 

```

-by site

The BC model produces underpredicts observed BC concentrations

```{r}
#calculate plot range
plot_range <- bc_valid_p %>%
  dplyr::summarize(
    min_val = min(c(mean_ng_m3, uk_trend_p)),
    max_val = max(c(mean_ng_m3, uk_trend_p))
    )
  
bc_valid_p %>%
  ggplot(aes(x = mean_ng_m3, y = uk_trend_p, 
             col=Year,
             shape=Site,
             )) + 
  geom_point() + 
  geom_smooth(se=F, span=1) +
  geom_abline(aes(slope = 1, intercept = 0, linetype="1-1")) + 
  #make square plot
  lims(x=c(plot_range$min_val, plot_range$max_val),
       y=c(plot_range$min_val, plot_range$max_val)) +
  labs(x= "Observed", 
       y= "Predicted",
       title = "Scatterplot of observed vs predicted BC concentration (ng/m3)"
       )
  
```




# Map predictions to a grid over time

Actual predictions 

```{r}

pollutant_units <- c("BC (ng/m3)", "UFP (pt/cm3)")
#place to save maps
bc_plots <- list()
ufp_plots <- list()

for (i in seq_along(pollutants)) {
  #i=1
  #select a specific year
  df1 <- grid_uk_trend_predictions %>%
      filter(Pollutant == pollutants[i])
  
  # range of all yrs for any given pollutant
  plot_range <- data.frame(
    min = min(df1$uk_trend_p, na.rm = T),
    max = max(df1$uk_trend_p, na.rm = T)
  )
  
  for(yr in seq_along(mapping_yrs)) {
    #yr=1
    # select a single pollutant
    df2 <- df1 %>% 
    filter(Year == mapping_yrs[yr]) %>% 
      select(Pollutant, latitude, longitude, uk_trend_p)
    
    p_map <- map0 +
        geom_point(data = df2,
                   aes(x = longitude, y = latitude, 
                       col = uk_trend_p),
                   alpha=1) + 
    scale_color_gradient(name=paste0(pollutant_units[i]), 
                         low = "yellow", high = "red", 
                         #plot all maps on same scale
                         limits = c(plot_range$min, plot_range$max)
                         ) +
    labs(
      title =mapping_yrs[yr]
      ) 

    # save BC/UFP plots 
    if(pollutants[i] == "bc") {
      bc_plots[yr] <- list(p_map)
      names(bc_plots)[yr] <- mapping_yrs[yr]
    }
    
    if(pollutants[i] == "ufp") {
      ufp_plots[yr] <- list(p_map)
      names(ufp_plots)[yr] <- mapping_yrs[yr]
    }

  }
  
}
 
```

```{r}
#bc 
bc_plots %>%
  ggarrange(plotlist = .,
          nrow = 1,
          common.legend = T, legend = "right") %>%
  annotate_figure(top = "Predicted BC Exposure Surfaces" ) 

#ufp 
ufp_plots %>%
  ggarrange(plotlist = .,
          nrow = 1,
          common.legend = T, legend = "right") %>%
  annotate_figure(top = "Predicted UFP Exposure Surfaces" ) 


```

Prediction differences

* TRAP concentration differences vary over space. This indicates that TRAP concentrations are predicted to have dropped more drastically at some locations (e.g., downtown Seattle and highways) than other locations over time.
  - the general trend in decreasing concentrations over time is a result of using the trend adjustment
  - the spatial differences in concentration differences are a result of using TVCs in our models

```{r}
diff_plots <- list()

for (i in seq_along(pollutants)) {
  #i=1
  #select a specific year
  df1 <- grid_uk_trend_predictions %>%
      filter(Pollutant == pollutants[i],
             Year %in% c(first_p_yr, last_prediction_yr)) %>%
    select(Pollutant, site_id, latitude, longitude, Year, uk_trend_p) %>%
    #calculate prediction differences
    spread(Year, uk_trend_p) %>%
    mutate(diff_p = `2019`-`1995`)
    
    p_map <- map0 +
        geom_point(data = df1,
                   aes(x = longitude, y = latitude, 
                       col = diff_p),
                   alpha=1) + 
    scale_color_gradient(name=paste0(pollutant_units[i]), 
                         #low = "yellow", mid = "white", high = "red",  
                         ) +
    labs(
      title =toupper(pollutants[i])
      ) 

    diff_plots[i] <- list(p_map)
    names(diff_plots)[i] <- pollutants[i]

  }
  
 
diff_plots %>%
  ggarrange(plotlist = .,
          nrow = 1) %>%
  annotate_figure(top = paste0("Differences in predicted BC and UFP exposure surfaces (", last_prediction_yr, "-", first_p_yr, ")") )

```



\newpage
# Code

```{r,ref.label=knitr::all_labels(),echo=TRUE,eval=FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=60), include=T}
```
 