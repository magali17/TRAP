---
title: "Aim 2: UK"
author: "Magali Blanco"
date: ' `r Sys.Date()` '
output:
  html_document:
    number_sections: yes
    df_print: paged
    toc: yes
    toc_depth: '3'
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: 3
editor_options:
  chunk_output_type: console
---

```{r}
# --> to do
# map predictions (mm & ACT) 
# relabel loadings plot

```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, 
                      cache=T, cache.comments = F, 
                      message = F, warning = F, 
                      tidy.opts=list(width.cutoff=60), tidy=TRUE,
                      fig.height = 8, fig.width = 8
                      )  

# Clear workspace of all objects and unload all extra (non-base) packages
rm(list = ls(all = TRUE))
if (!is.null(sessionInfo()$otherPkgs)) {
  res <- suppressWarnings(
    lapply(paste('package:', names(sessionInfo()$otherPkgs), sep=""),
      detach, character.only=TRUE, unload=TRUE, force=TRUE))
}

pacman::p_load(tidyverse, ggpubr, #dplyr, 
               knitr, kableExtra, 
               #descriptive statistics
               Hmisc, EnvStats, 
                  #qwraps2, #mean_sd, median_iqr
               # modeling
               pls, geoR #gstat - alternative for UK
               )    
#Himsc: describe(); EnvStats: summaryFull(); ggpubr: ggarrange()

options(knitr.kable.NA = '')
set.seed(1)
source("A2.0.1_Var&Fns.R")

images_path <- file.path(images_path0, "3. UK")
tables_path <- file.path(tables_path0, "3. UK")

#read in data
          #readRDS(file.path("Data", "Aim 2", "Mobile Monitoring", "mm.w_2019-12-22.rda"))
annual0 <- readRDS(file.path("Data", "Aim 2", "Mobile Monitoring", "annual_ufp_and_cov.rda")) 
          
site_locations <- annual0 %>%
  select(site_id, contains("lambert")) #latitude, longitude

cov_names <- annual0 %>%
  select(elev_elevation:log_m_to_a23) %>%
  names()

## --> needs to be updated & geocovariates cleaned up
geo_act <- read.csv(file.path("Data", "Aim 2", "ACT", "ACT_selected.csv"))  

```

# Approach    

1. use 10 folds to split up the data (annual averages and geocovariates for ~ 308 locations) into a training (90%) and validation (10%) dataset 
  
2. Run PLS on the training set to calculate the scores for various components (start with 1 component).

3. Fit UK models to the training data with the selected number of PLS components and variogram settings (e.g., max distance)

4. Repeat steps 2-3 nine more times to calculate cross-validated RMSE and R2

5. Repeat steps 2-4 with an increasing numbber of PLS components (e.g., 1-10) and for different variogram settings

6. Fit a final model with the selected parameters (number of PLS components, residual model) and all of the data.

7. Estimate the out-of-sample RMSE and R2 for AQS sites using the final UK model.

8. Predict at ACT participant locations.


# PLS to create summary features of geocovariates. 

Create training,  test and AQS set.

```{r, results="hide"}
# 1. fit PLS using training set to ID # features to use
annual_all <- annual0 %>%
  select(site_id,
         log_ufp,
         cov_names) %>%
  drop_na() %>%
  #create test, train, aqs data set
  mutate(set = ifelse(grepl("MC", site_id), "aqs",
                 sample(c(1:10), size = sum(grepl("MS", site_id)), replace = T)),
    cv_prediction = NA
         )

annual_aqs <- annual_all %>%
  #only keep aqs sites
  filter(set == "aqs")  

annual_train_test <- annual_all %>%
  #drop aqs sites
  filter(set  != "aqs")  

```

Determine what the best plotting distance is when a different number of PLS components are used.

plots of different variogram distances. Using all data.

```{r, results="hide"}
pdf("variogram plots.pdf")

use_n_scores <- 10

dist_fractions <- c(0.05,  seq(0.1, 0.5, by=0.1))  
#dist_fractions <- seq(0.01, 0.12, by=0.01)  

par(mfrow = c(3, 2))

for(i in seq_len(use_n_scores)) {
  #i=1
  score_n_names <- paste0("Comp", 1:i)
  
  #fit PLS. calling it "train", but using entire dataset
  pls_train_test <- plsr(log_ufp ~.,
               data=annual_train_test[,c("log_ufp", cov_names)], 
               ncomp = i,
               scale=T
               )
  
  # extract scores for UK
  scores_train_test <- scores(pls_train_test)[,c(1:i)] %>%
    as.data.frame()
  
  # take out spaces in names
  names(scores_train_test) <- score_n_names
  
  # dataset w/ UFP measurements, geocovariates, location
  pls_df_train_test <- cbind(
    annual_train_test[c("site_id", "log_ufp")],
    scores_train_test
  ) %>%
    left_join(site_locations, by = "site_id") 

  ################################ UK ################################
   geo_train_test <- as.geodata(pls_df_train_test, 
                        coords.col = c("lambert_x", "lambert_y"), 
                        data.col = "log_ufp", 
                        covar.col = score_n_names)

  ##trend
  cov_trend <-  as.formula(paste0("~ ", paste0(score_n_names, collapse = " + " )))
  
  #plotting distances
  max.dist <- summary(geo_train_test)$distances.summary[["max"]]
  ## plot nearby locations more finely - improves variogram model fit?
  brk_pt <- 1000
  by1_pt <- 300
  by2_pt <- 1000
  
  ############################ model residuals ######################################
  for(j in seq_along(dist_fractions)) {
    #j=1
    max.plot.dist <- max.dist*dist_fractions[j]

  
  ##Empirical Variogram
  variog_train_test <- variog(geo_train_test,
                         #create equally spaced bins for all distances plotted
                         uvec=c(seq(0, brk_pt, by = by1_pt), seq((brk_pt + by2_pt), max.plot.dist, by= by2_pt)),
                        #UK
                        trend = cov_trend, 
                        messages = F  
                        )

  #plot(variog_train)
  
  wls_ests_train_test <- variofit(variog_train_test, cov.model = "exp",
                                              messages = F)
  
  resid_model_train_test <- variofit(vario = variog_train_test, 
                      ini = wls_ests_train_test, 
                      cov.model = "exp",
                      #weights = "equal") #ols
                      weights = "npairs",
                      messages = F
                      ) #wls
  
   plot(variog_train_test, 
       main = paste0("Comp: ", i, ", Dist fract: ", dist_fractions[j]), 
       cex.main=0.8
         )
  lines(variog_train_test)
  lines(resid_model_train_test, col=2)

  }
  
}

dev.off()

```

10 FCV to evalute best number of PLS components.

### --> ? run CV serveral times w/ diff spatial blocks to produce a range (rather than a #) of model fit estimates? May be more imp for A3 where I will be extrapolating predictions

### --> ? error: phi too high when make distnace bins too small? 

```{r, results="hide"}
set.seed(1)
################# variables for later ###############
# CV folds
k <- 10

# max PLS components to use in UK
use_n_scores <- 10  

dist_fract <-  c(0.05, seq(0.1, 0.5, by=0.1))
#df to save CV RMSE
cv.eval <- data.frame(
  expand.grid(pls_comp = c(1:use_n_scores),
            dist_fract = dist_fract
            ),
  RMSE = NA,
  R2 = NA
  )
#cv.eval <- cv.eval %>% left_join(variog_dist_to_plot)

################## CV loops ##################################
#estimate CV RMSE and r2 for diff number of PLS component combinations 
for(i in seq_len(use_n_scores)) {
  #i=1
  score_n_names <- paste0("Comp", 1:i)
  
  for(j in seq_along(dist_fract)) {
  #j=1
  #10FCV for each PLS component combination
  for(f in seq_len(k)) {
  #f=1
  train_grp <- annual_train_test$set != f
  
  annual_train <- annual_train_test %>%
    filter(train_grp)  
  
  annual_test <- annual_train_test %>%
    filter(!train_grp)   
  
  #fit PLS to training data
  pls_train <- plsr(log_ufp ~.,
               data=annual_train[,c("log_ufp", cov_names)], 
               ncomp = i,
               scale=T
                  )
  
  #pls_train$scores
  
  # extract scores for UK
  scores_train <- scores(pls_train)[,c(1:i)] %>%
    as.data.frame()
  scores_test <- predict(object = pls_train,
                         newdata = annual_test,
                         ncomp = 1:i,
                         type = "score"
                         ) %>%
    as.data.frame()
  
  # take out spaces in names
  names(scores_train) <- score_n_names
  names(scores_test) <- score_n_names
  
  # dataset w/ UFP measurements, geocovariates, location
  pls_df_train <- cbind(
    annual_train[c("site_id", "log_ufp")],
    scores_train
  ) %>%
    left_join(site_locations,by = "site_id") 
  
  pls_df_test <- cbind(
    annual_test[c("site_id", "log_ufp")],
    scores_test
  ) %>%
    left_join(site_locations,by = "site_id")

  ################################ UK ################################
   geo_train <- as.geodata(pls_df_train, 
                        coords.col = c("lambert_x", "lambert_y"), 
                        data.col = "log_ufp", 
                        covar.col = score_n_names)
  geo_test <- as.geodata(pls_df_test, 
                        coords.col = c("lambert_x", "lambert_y"), 
                        data.col = "log_ufp", 
                        covar.col = score_n_names)
  
  ##trend
  cov_trend <-  as.formula(paste0("~ ", paste0(score_n_names, collapse = " + " )))
  
  max.dist <- summary(geo_train)$distances.summary[["max"]]
 
  max.plot.dist <- max.dist*dist_fract[j] 

  ############################ model residuals ###################################### 
  ##Empirical Variogram
  brk_pt <- 1000
  by1_pt <- 300
  by2_pt <- 1000
  
  variog_train <- variog(geo_train,
                         #uvec=c(seq(0, brk_pt, by = by1_pt), seq((brk_pt + by2_pt), max.plot.dist, by= by2_pt)),
                         #uvec=seq(0, max.plot.dist, by = by2_pt),
                         uvec=c(seq(0, brk_pt, by = by1_pt), seq((brk_pt + by2_pt), max.plot.dist, by= by2_pt)),


                        #UK
                        trend = cov_trend, 
                        messages = F  
                        )

  #use geoR try to estimate intitial range & sill values. using WLS and an exponential fit
  wls_ests_train <- variofit(variog_train, cov.model = "exp", 
                             messages = F)
   
  # REML does slightly better in terms of RMSE and R2 when using fewer components (< ~10), otherwise does about the same. BUT variogram Looks worse than the rest. OLS seems to do better visually.
  
  # resid_model_train <- likfit(geodata = geo_train,
  #                      ini=wls_ests_train,
  #                      cov.model = "exp",
  #                      trend = cov_trend,
  #                      lik.method="REML") # "ML"
  
  resid_model_train <- variofit(vario = variog_train, 
                      ini = wls_ests_train, 
                      cov.model = "exp",
                      #weights = "equal", #ols
                      weights = "npairs",#wls
                      messages = F
                      ) 

  # plot(variog_train)
  # lines(variog_train)
  # lines(resid_model_train, col=2)
  
  #trend
    train_trend <- trend.spatial(trend = cov_trend, geo_train)
    test_trend <- trend.spatial(trend = cov_trend, geo_test)
    
    ############################# Predict #############################
    
    kc_cv <- krige.conv(coords = geo_train$coords,
                        data = geo_train$data,
                        locations = geo_test$coords,
                        krige = krige.control(type = "ok",
                                            obj.model = resid_model_train, 
                                              trend.d = train_trend,
                                              trend.l = test_trend
                                              ), 
                        )
    
    #save CV predictions temporarily  
    annual_train_test$cv_prediction[!train_grp] <- kc_cv$predict
  
  }
   cv.eval$RMSE[cv.eval$pls_comp== i & cv.eval$dist_fract==dist_fract[j]] <- rmse(obs = annual_train_test$log_ufp, pred = annual_train_test$cv_prediction)
  cv.eval$R2[cv.eval$pls_comp== i & cv.eval$dist_fract==dist_fract[j]] <-  r2_mse_based(obs = annual_train_test$log_ufp , pred = annual_train_test$cv_prediction) 
    
    }

}

```


```{r}
######################### select CV parameters #########################
  min_rmse <- min(cv.eval$RMSE)
  cv_r2 <- cv.eval$R2[cv.eval$RMSE == min_rmse] 
  cv_comp <- cv.eval$pls_comp[cv.eval$RMSE == min_rmse]
  cv_dist_fract <- cv.eval$dist_fract[cv.eval$RMSE == min_rmse]
  
  cv_comp_names <- names(scores_train)[1:cv_comp]
 
  #trends
  cv_cov_trend <-  as.formula(paste0("~ ", paste0(cv_comp_names, collapse = " + " )))

######################## Plot CV RMSE & R2 ################################## 
   
fit_info <- paste0("Min RMSE: ", round(min_rmse, 3),
                   "\nR2: ", round(cv_r2, 2),
                   "\nPLS Components: ", cv_comp,
                   "\nFraction of maximum distance plotted in variogram: ", cv_dist_fract
                   )

cv.eval %>%
  gather("variable", "value", RMSE:R2) %>%
  ggplot(aes(x=pls_comp, y=value, 
             col=factor(dist_fract)
             )) + 
  geom_point(alpha=0.7) + 
  geom_line(alpha=0.7) +
  geom_vline(xintercept = cv_comp, 
             linetype = "dashed",
             alpha=0.5) +
  facet_wrap(~variable, 
             scales = "free", 
             ncol=1
             ) +
  scale_x_continuous(breaks= scales::pretty_breaks(n = cv_comp)) +
  labs(
    x = "PLS Components",
    col = "Fract of max dist\nplotted in variogram",
    title = "10FCV Model Performance",
    subtitle = fit_info,
    caption = "MSE-based R2 = max(0, 1 – MSE / Var(Y))"
  ) 

```

# Fit final UK model to all stop data    
* exclude AQS co-location sites

```{r}
############################# PLS #############################
# fit model to all data
pls_train_test <- plsr(log_ufp ~.,
             data=annual_train_test[,c("log_ufp", cov_names)], 
             ncomp = cv_comp,
             scale=T
                )

# extract scores for UK
scores_train_test <- scores(pls_train_test)[,c(1:cv_comp)] %>%
  as.data.frame()

# take out spaces in names
names(scores_train_test) <- gsub(" ", "", names(scores_train_test))

```

Plot of PLS loadings. 

### --> relabel "_stdev" into buffers. crate buffers for "_below", "_above", and "_at_elev" even though exact definitions are a little diff? 

```{r, fig.height=15}
pls.loadings <- pls_train_test$loadings[] %>%
  as.data.frame() %>%
  rownames_to_column(var = "cov")

pls.loadings <- pls.loadings %>%
  # rename variables if buffers
  split_cov_name(cov = "cov")

my.alpha=0.3

pls.loadings.l  <- pls.loadings %>%
  #make long format for faceting
  gather(key = "Component", value = "Loading", contains("Comp")) %>%
  mutate(Component = as.numeric(substr(Component, 6, nchar(Component))))  
   
#first 4 components 
pls.loadings.l  %>%
  #drop these in first points
  drop_na(buffer) %>%
  ggplot(aes(x = Loading, y = cov)) +
  geom_point(aes(size=buffer),
             shape=1,
             alpha=my.alpha) +
  scale_size(breaks = c(min(pls.loadings$buffer, na.rm = T),
                        max(pls.loadings$buffer, na.rm = T)
                        )
             ) + #500, 5000, 10000,

  geom_point(data = pls.loadings.l[is.na(pls.loadings.l$buffer),],
           alpha=my.alpha,
           aes(shape="")) +
  geom_vline(xintercept=0,
             linetype="solid",
             alpha=my.alpha) +
    facet_wrap(~Component,
               labeller = "label_both",
               ncol = 2
               ) +
  labs(#x = paste0("Loading"),
       y = "Geocovariate",
       shape= "non-buffer", #"proximity,\nelevation",
       title = "PLS Geocovariate Component Loadings") +
  theme(legend.position = "bottom")


```

Geodataset.

```{r}

############################# UK #############################
# add site_id, ufp & lat/long
pls_df_train_test <- cbind(
  annual_train_test[c("site_id", "log_ufp")],
  scores_train_test
) %>%
  left_join(site_locations) 

# create geodata for test set
geo_train_test <- as.geodata(pls_df_train_test , 
                        coords.col = c("lambert_x", "lambert_y"), 
                        data.col = "log_ufp", 
                        covar.col = cv_comp_names)
# trend
trend_train_test <- trend.spatial(trend = cv_cov_trend,
                           geodata = geo_train_test)

# residual model

max.dist <- summary(geo_train_test)$distances.summary[["max"]]

```

Residual model.   

```{r, results="hide"}
max.plot.dist <- max.dist*cv_dist_fract

##Empirical Variogram
variog_train_test <- variog(geo_train_test,
                       #uvec=seq(0, max.plot.dist, by = 1000),
                       max.dist=max.plot.dist,
                      #UK
                      trend = cv_cov_trend 
                      )
#plot(variog_train_test)

#estimate range & sill values. using WLS and an exponential fit
wls_ests <- variofit(vario = variog_train_test, cov.model = "exp")

# resid_model <- likfit(geodata = geo_train_test, 
#                            ini=wls_ests, 
#                            trend = cv_cov_trend,
#                            cov.model = "exp",
#                            #lik.method="ML") #ML
#                            lik.method="REML") # REML

# ols: equal; wls: npairs
resid_model <- variofit(vario = variog_train_test, 
                      ini=wls_ests, 
                      cov.model = "exp",
                      #weights="equal") #ols
                      weights = "npairs") #wls

# partial sill: sigma sq
# range: phi, 
# nugget: tau sq
resid_model.s <- summary(resid_model)
model_partial_sill <- resid_model.s$estimated.pars[["sigmasq"]]
model_range <- resid_model.s$estimated.pars[["phi"]]
model_nugget <- resid_model.s$estimated.pars[["tausq"]]

```

```{r}
#par(mfrow = c(1, 1))
plot(variog_train_test,
     main = paste0("Binned empirical and modeled variogram\nfor all stop data (excldues AQS sites). Using ", cv_comp, " PLS components"),
     xlab = "Distance (m)",
     #xlim=c(0,10000)
     )
lines(variog_train_test, lty=1)
lines(resid_model, lty=2, col=2)
legend("bottomright",
       legend = c("Empirical", paste0("Residual Model")),
       lty=c(1:2), col = c(1:2),
       #cex = 0.7
       )

```

Residual model parameters.

```{r}
data.frame(
  method = c("OLS"),
  Partial_Sill = c(model_partial_sill),
  Range_m = c(model_range),
  Nugget = c(model_nugget)
)%>% 
  kable(caption = "Residual model parameters for final model",  
        col.names = c("Method", "Partial Sill", "Range (m)", "Nugget"),
        digits = 4
        ) %>%
  kable_styling()

```

# Out-of-sample RMSE and R2 for AQS sites 

```{r,results="hide"}
#AQS test set
#estimate scores for test set
scores_test_aqs <- predict(object = pls_train_test,
                       newdata = annual_aqs,
                       ncomp = 1:cv_comp, 
                       type = "score"
                       ) %>%
  as.data.frame()

#take out spaces 
names(scores_test_aqs) <- names(scores_train_test) 

#add site_id, ufp & lat/long
pls_df_test_aqs <- cbind(
  annual_aqs[c("site_id", "log_ufp")],
  scores_test_aqs
) %>%
  left_join(site_locations) 

#create geodata for test set
geo_test_aqs <- as.geodata(pls_df_test_aqs , 
                        coords.col = c("lambert_x", "lambert_y"), 
                        data.col = "log_ufp", 
                        covar.col = cv_comp_names)

trend_test_aqs <- trend.spatial(trend = cv_cov_trend,
                           geodata = geo_test_aqs)

uk_test_aqs <- krige.conv(coords = geo_train_test$coords,
                            data = geo_train_test$data,
                            locations = geo_test_aqs$coords,
                            krige=krige.control(type = "ok",
                                                obj.model = resid_model,
                                                trend.d= trend_train_test,
                                                trend.l= trend_test_aqs))

#save UK predictions
pls_df_test_aqs$pred  <- uk_test_aqs$predict

#calculate out-of-sample RMSE and R2
rmse_test_aqs <- rmse(obs =  pls_df_test_aqs$log_ufp, pred = pls_df_test_aqs$pred)
r2_test_aqs <- r2_mse_based(obs =  pls_df_test_aqs$log_ufp, pred = pls_df_test_aqs$pred)

```

Results table.

```{r}
data.frame(
  N = c(length(geo_test_aqs$data)),
  RMSE = c(rmse_test_aqs),
  R2 = c(r2_test_aqs)
) %>%
  kable(caption = paste0("Out-of sample RMSE and R2 for AQS co-location sites"),
        digits = 2) %>%
  kable_styling()

```
 
plot of Out-of-sample predictions vs measured UFP concentrations.

```{r}

pls_df_test_aqs %>%
  colo.plot(x.variable = "log_ufp", x.label = "Measure log UFP (log pt/cm3)",
            y.variable = "pred", y.label = "Predicted log UFP (log pt/cm3)", 
            rmse.digits = 2, 
            mytitle = paste0("Out-of-sample predictions vs measured UFP",
                                              "\nat AQS co-location stops"),
            mysubtitle = paste0("UK model wtih ", cv_comp, " PLS components"),
            mycaption = "MSE-based R2: max(0, 1 – MSE / Var(Y))"
            )

```






Predict at participant homes

### --> see 556 lab 9 code for R?
### --> need new dataset w/ same variable names. rename all variables to match training data?   

```{r, eval=F}
# geo_act %>%
#   select_at(vars(contains("por")), ~ paste0(., "t"),
#             vars(ends_with("air")), ~ paste0(., "p")
#             ) %>%
#   names()

 
# #rename differently labeled variables in this dataset 
# t <- geo_act %>%
#   rename_at(vars(contains("por")), ~ paste0(., "t"),
#             #vars(ends_with("air")), ~ paste0(., "p")
#             )  %>%
#   rename(m_to_l_airp = m_to_l_air)



#estimate scores for test set
scores_act <- predict(object = pls_train_test,
                       newdata = geo_act,
                       ncomp = 1:cv_comp, 
                       type = "score"
                       ) %>%
  as.data.frame()


#take out spaces 
names(scores_act) <- names(scores_train_test) 

#add site ientifier & lat/long
pls_df_act <- cbind(
  # --> add cov names
  geo_act[c("location_i",      )],
  scores_act
)  

#create geodataset
geo_act <- as.geodata(pls_df_act, 
                      coords.col = c("lambert_x", "lambert_y"), 
                      #data.col = "log_ufp", 
                      covar.col = names(scores_act))

#trend
trend_act <- trend.spatial(trend = cov_trend,
                           geodata = geo_act)
#uk
uk_act <- krige.conv(coords = geo_train_test$coords,
                            data = geo_train_test$data,
                            locations = geo_act$coords,
                            krige=krige.control(type = "ok",
                                                obj.model = resid_model,
                                                trend.d= trend_train_test,
                                                trend.l= trend_act))
#save UK predictions
geo_act$pred  <- uk_act$predict
 
```

prediction boxplots overall and by important TRAP indicators

```{r}

```


Map predictions

### --> see 2.4.5

```{r}

```

# for Aim 3: How much does the regression vs kriging contribute to your esimate?
Paul Sampson suggestion: decompose the 2019 UK model to see how much of the prediction is generated by regression vs observed interpolations (kriging). If regression is most responsible, less worried about issues w/ smoothing back in time.

*kriging* + error = Yhat – xB 

```{r}

```


# QC  

Amanda: "Compare distribution of PLS scores in monitoring vs cohort. See if combination of variables is unusual. Check predicted PLS values are not super large – suggests extrapolation. If have strange points, can look to see where they are on a map (e.g., a point is in middle of lake; or strange geographic area)."

```{r}

```

### --> ? variogram for something w/ more spatial structure, e.g., PM2.5
